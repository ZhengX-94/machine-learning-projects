{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1 download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.twenty_newsgroups module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build a semantic contennt recommendation system with AWS SageMaker\n",
    "\n",
    "Data source: \n",
    "    20newsgroups dataset contained in scikit-learn package\n",
    "\n",
    "Usage: \n",
    "    The model automatically extract semantic information from the news data, and \n",
    "    identify similar news articles from the corpus and provide content recommendations\n",
    "    for similar news items based on the ones that are selected\n",
    "    \n",
    "    The model is trained and deployed in the Amazon SageMaker Notebook instance,\n",
    "    using built-in algorithms for Neural Topic Model(NTM) and K-Nearest Neighbour(K-NN)\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')['data']\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')['data']\n",
    "NUM_TOPICS = 30\n",
    "NUM_NEIGHBORS = 10\n",
    "BUCKET = 'sagemaker-content-recommender'\n",
    "PREFIX = '20newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newsgroups_train)):\n",
    "    newsgroups_train[i] = strip_newsgroup_header(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_quoting(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_footer(newsgroups_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data to numeric format by assigning a token to each word\n",
    "# Use WordNetLemmatizer as a lemmatizer from NLTK pacakage\n",
    "# Use CountVectorizer to perform token counting\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000\n",
      "Done. Time elapsed: 42.63s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "newidx = np.random.permutation(idx) # this will be the labels fed into the KNN model for training\n",
    "# Need to store these permutations:\n",
    "\n",
    "vectors = vectors[newidx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 2000) (2263, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Convert data into training and validation data\n",
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "val_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "\n",
    "print(train_vectors.shape,val_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://sagemaker-content-recommender/20newsgroups/train\n",
      "Validation set location s3://sagemaker-content-recommender/20newsgroups/val\n",
      "Trained model will be saved at s3://sagemaker-content-recommender/20newsgroups/output\n"
     ]
    }
   ],
   "source": [
    "# define training and validation path\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part0.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part1.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part2.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part3.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part4.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part5.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part6.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/train/train_part7.pbr\n",
      "Uploaded data to s3://sagemaker-content-recommender/20newsgroups/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "# convert the raw vectors into RecordIO format, and use n_parts to breaks dataset into shards for destributed training\n",
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP2 Train and deploy the topic model\n",
    "\n",
    "#### create and run the training job\n",
    "The built in algorithms are stored as docker conatiners in Amazon ECR, need to specify container and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# Use estimator to specify the infrastructure for model training\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=2, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)\n",
    "# set the hyperparamter\n",
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ShardedBys3Key to spread data to different workers, each worker receives a different portion of the full data set, this method is more efficient that FullyReplicated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 21:11:57 Starting - Starting the training job...\n",
      "2020-06-28 21:12:02 Starting - Launching requested ML instances.........\n",
      "2020-06-28 21:13:28 Starting - Preparing the instances for training...............\n",
      "2020-06-28 21:16:26 Downloading - Downloading input data...\n",
      "2020-06-28 21:16:59 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:14 INFO 140483679958848] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:14 INFO 140483679958848] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'30', u'epochs': u'100', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:14 INFO 140483679958848] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'100', u'mini_batch_size': u'128', u'num_topics': u'30', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:14 INFO 140483679958848] nvidia-smi took: 0.0252010822296 secs to identify 0 gpus\u001b[0m\n",
      "\n",
      "2020-06-28 21:17:18 Training - Training image download completed. Training in progress.\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/41afb0c2-ef69-441f-935d-2abf983068a3', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/41afb0c2-ef69-441f-935d-2abf983068a3', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-103-137.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/39c96b97-c8b0-4118-bab5-14c30c76dd8c', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/41afb0c2-ef69-441f-935d-2abf983068a3', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/41afb0c2-ef69-441f-935d-2abf983068a3', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.103.137', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-103-137.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/39c96b97-c8b0-4118-bab5-14c30c76dd8c', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/41afb0c2-ef69-441f-935d-2abf983068a3', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/41afb0c2-ef69-441f-935d-2abf983068a3', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-103-137.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/39c96b97-c8b0-4118-bab5-14c30c76dd8c', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/41afb0c2-ef69-441f-935d-2abf983068a3', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/41afb0c2-ef69-441f-935d-2abf983068a3', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.103.137', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-103-137.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/39c96b97-c8b0-4118-bab5-14c30c76dd8c', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/41afb0c2-ef69-441f-935d-2abf983068a3', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/41afb0c2-ef69-441f-935d-2abf983068a3', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.103.137', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-103-137.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/39c96b97-c8b0-4118-bab5-14c30c76dd8c', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 36 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 37 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:19.412] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Initializing\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] None\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] vocab.txt\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Vocab file is not provided\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:19 INFO 140483679958848] Create Store: dist_async\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'30', u'epochs': u'100', u'feature_dim': u'2000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'2000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'100', u'mini_batch_size': u'128', u'num_topics': u'30', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] nvidia-smi took: 0.0252628326416 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Launching parameter server for role server\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e10aa1dd-a06f-4ddb-bfe4-68a0f4bfac78', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/e10aa1dd-a06f-4ddb-bfe4-68a0f4bfac78', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-116-100.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c1edeb6a-9928-4b8e-bab6-5540a68e6bfb', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e10aa1dd-a06f-4ddb-bfe4-68a0f4bfac78', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/e10aa1dd-a06f-4ddb-bfe4-68a0f4bfac78', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.103.137', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-116-100.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c1edeb6a-9928-4b8e-bab6-5540a68e6bfb', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e10aa1dd-a06f-4ddb-bfe4-68a0f4bfac78', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/e10aa1dd-a06f-4ddb-bfe4-68a0f4bfac78', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.103.137', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-06-28-21-11-57-701', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-116-100.ca-central-1.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c1edeb6a-9928-4b8e-bab6-5540a68e6bfb', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/ntm-2020-06-28-21-11-57-701', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35mProcess 37 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Using default worker.\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:22.304] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Initializing\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] None\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] vocab.txt\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Vocab file is not provided\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1593379042.736627, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1593379042.736592}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:22.736] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 3332, \"num_examples\": 1, \"num_bytes\": 30204}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:22 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:22 INFO 140483679958848] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1593379042.732967, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1593379042.732934}\n",
      "\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:22.733] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 441, \"num_examples\": 1, \"num_bytes\": 29768}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:22 INFO 140435207169856] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:23.875] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1137, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] # Finished training epoch 1 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] Loss (name: value) total: 6.95806062222\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] Loss (name: value) kld: 0.0171862471729\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] Loss (name: value) recons: 6.94087436464\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] Loss (name: value) logppx: 6.95806062222\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.95806062222\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1593379043.88073, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1593379042.736993}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3954.94667573 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:23 INFO 140483679958848] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:23.949] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] # Finished training epoch 1 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] Loss (name: value) total: 6.95840146144\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] Loss (name: value) kld: 0.0182116072069\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] Loss (name: value) recons: 6.94018987152\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] Loss (name: value) logppx: 6.95840146144\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=6.95840146144\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Total Records Seen\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1593379043.955947, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1593379042.733281}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3702.09540908 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:23 INFO 140435207169856] # Starting training for epoch 2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:17:25.199] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1318, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] # Finished training epoch 2 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] Loss (name: value) total: 6.86405001084\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] Loss (name: value) kld: 0.00216994716579\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] Loss (name: value) recons: 6.86188005077\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] Loss (name: value) logppx: 6.86405001084\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.86405001084\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] Timing: train: 1.32s, val: 0.00s, epoch: 1.32s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 9048, \"sum\": 9048.0, \"min\": 9048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1593379045.204727, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1593379043.881006}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3417.09164219 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:25 INFO 140483679958848] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:25.337] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 1380, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] # Finished training epoch 2 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] Loss (name: value) total: 6.88239257865\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] Loss (name: value) kld: 0.00275127909036\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] Loss (name: value) recons: 6.8796412415\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] Loss (name: value) logppx: 6.88239257865\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=6.88239257865\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] Timing: train: 1.38s, val: 0.00s, epoch: 1.39s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}, \"Total Records Seen\": {\"count\": 1, \"max\": 9054, \"sum\": 9054.0, \"min\": 9054}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1593379045.343649, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1593379043.956262}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3262.59214811 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:25 INFO 140435207169856] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:26.586] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1381, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] # Finished training epoch 3 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] Loss (name: value) total: 6.86209166712\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] Loss (name: value) kld: 0.00508964814233\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] Loss (name: value) recons: 6.85700203313\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] Loss (name: value) logppx: 6.86209166712\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.86209166712\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] Timing: train: 1.38s, val: 0.00s, epoch: 1.39s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}, \"Total Records Seen\": {\"count\": 1, \"max\": 13572, \"sum\": 13572.0, \"min\": 13572}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1593379046.590545, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1593379045.205183}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3265.22595946 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:26 INFO 140483679958848] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:26.919] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1575, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] # Finished training epoch 3 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] Loss (name: value) total: 6.87554550171\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] Loss (name: value) kld: 0.00525169752655\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] Loss (name: value) recons: 6.87029382255\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] Loss (name: value) logppx: 6.87554550171\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=6.87554550171\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] Timing: train: 1.58s, val: 0.00s, epoch: 1.58s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}, \"Total Records Seen\": {\"count\": 1, \"max\": 13581, \"sum\": 13581.0, \"min\": 13581}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1593379046.924193, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1593379045.343955}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=2864.51916014 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:26 INFO 140435207169856] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:27.769] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1178, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] # Finished training epoch 4 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] Loss (name: value) total: 6.85691378514\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] Loss (name: value) kld: 0.00655304278674\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] Loss (name: value) recons: 6.85036071142\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] Loss (name: value) logppx: 6.85691378514\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.85691378514\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 18096, \"sum\": 18096.0, \"min\": 18096}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1593379047.773059, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1593379046.590811}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3826.14572883 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:27 INFO 140483679958848] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:28.086] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 1161, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] # Finished training epoch 4 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] Loss (name: value) total: 6.8714317547\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] Loss (name: value) kld: 0.00663957654615\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] Loss (name: value) recons: 6.86479218801\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] Loss (name: value) logppx: 6.8714317547\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=6.8714317547\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] Timing: train: 1.16s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}, \"Total Records Seen\": {\"count\": 1, \"max\": 18108, \"sum\": 18108.0, \"min\": 18108}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1593379048.091812, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1593379046.92442}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3876.27795609 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:28 INFO 140435207169856] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:28.956] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1182, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] # Finished training epoch 5 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] Loss (name: value) total: 6.85554668638\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] Loss (name: value) kld: 0.00863529503759\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] Loss (name: value) recons: 6.846911384\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] Loss (name: value) logppx: 6.85554668638\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.85554668638\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 22620, \"sum\": 22620.0, \"min\": 22620}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1593379048.962056, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1593379047.773328}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3805.27517479 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:28 INFO 140483679958848] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:29.328] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1233, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] # Finished training epoch 5 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] Loss (name: value) total: 6.87002332343\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] Loss (name: value) kld: 0.00934743282333\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] Loss (name: value) recons: 6.86067586475\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] Loss (name: value) logppx: 6.87002332343\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=6.87002332343\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 22635, \"sum\": 22635.0, \"min\": 22635}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1593379049.336367, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1593379048.092667}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3639.51825201 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:29 INFO 140435207169856] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:30.164] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1202, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] # Finished training epoch 6 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Loss (name: value) total: 6.85475693146\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Loss (name: value) kld: 0.0111083808588\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Loss (name: value) recons: 6.84364851316\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Loss (name: value) logppx: 6.85475693146\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.85475693146\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] patience losses:[6.958060622215271, 6.864050010840098, 6.862091667122311, 6.85691378513972, 6.855546686384413] min patience loss:6.85554668638 current loss:6.85475693146 absolute loss difference:0.000789754920535\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 27144, \"sum\": 27144.0, \"min\": 27144}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1593379050.170949, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1593379048.962398}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3742.60086485 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:30 INFO 140483679958848] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:30.578] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 1241, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] # Finished training epoch 6 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] Loss (name: value) total: 6.86674662431\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] Loss (name: value) kld: 0.00931362426167\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] Loss (name: value) recons: 6.85743304094\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] Loss (name: value) logppx: 6.86674662431\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=6.86674662431\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] patience losses:[6.958401461442311, 6.8823925786548195, 6.875545501708984, 6.8714317546950445, 6.8700233234299555] min patience loss:6.87002332343 current loss:6.86674662431 absolute loss difference:0.00327669911914\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] Timing: train: 1.24s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 216, \"sum\": 216.0, \"min\": 216}, \"Total Records Seen\": {\"count\": 1, \"max\": 27162, \"sum\": 27162.0, \"min\": 27162}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1593379050.583502, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1593379049.3369}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3630.42872892 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:30 INFO 140435207169856] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:31.406] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1234, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] # Finished training epoch 7 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] Loss (name: value) total: 6.85136501657\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] Loss (name: value) kld: 0.0104132388693\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] Loss (name: value) recons: 6.84095174074\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] Loss (name: value) logppx: 6.85136501657\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.85136501657\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] patience losses:[6.864050010840098, 6.862091667122311, 6.85691378513972, 6.855546686384413, 6.854756931463878] min patience loss:6.85475693146 current loss:6.85136501657 absolute loss difference:0.0033919148975\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 252, \"sum\": 252.0, \"min\": 252}, \"Total Records Seen\": {\"count\": 1, \"max\": 31668, \"sum\": 31668.0, \"min\": 31668}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1593379051.410737, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1593379050.17138}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3649.8439362 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:31 INFO 140483679958848] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:31.772] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1186, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] # Finished training epoch 7 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Loss (name: value) total: 6.86720285813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Loss (name: value) kld: 0.0112020343853\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Loss (name: value) recons: 6.85600084066\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Loss (name: value) logppx: 6.86720285813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] #quality_metric: host=algo-2, epoch=7, train total_loss <loss>=6.86720285813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] patience losses:[6.8823925786548195, 6.875545501708984, 6.8714317546950445, 6.8700233234299555, 6.866746624310811] min patience loss:6.86674662431 current loss:6.86720285813 absolute loss difference:0.000456233819326\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 252, \"sum\": 252.0, \"min\": 252}, \"Total Records Seen\": {\"count\": 1, \"max\": 31689, \"sum\": 31689.0, \"min\": 31689}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1593379051.774002, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1593379050.584133}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3804.03230167 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:31 INFO 140435207169856] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:32.567] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1156, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] # Finished training epoch 8 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] Loss (name: value) total: 6.84819789728\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] Loss (name: value) kld: 0.0125427418518\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] Loss (name: value) recons: 6.83565513293\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] Loss (name: value) logppx: 6.84819789728\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.84819789728\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] patience losses:[6.862091667122311, 6.85691378513972, 6.855546686384413, 6.854756931463878, 6.851365016566382] min patience loss:6.85136501657 current loss:6.84819789728 absolute loss difference:0.00316711929109\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 36192, \"sum\": 36192.0, \"min\": 36192}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1593379052.57131, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1593379051.411021}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3898.50373639 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:32 INFO 140483679958848] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:32.953] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 1178, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] # Finished training epoch 8 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] Loss (name: value) total: 6.86355614662\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] Loss (name: value) kld: 0.012304724608\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] Loss (name: value) recons: 6.85125140349\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] Loss (name: value) logppx: 6.86355614662\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] #quality_metric: host=algo-2, epoch=8, train total_loss <loss>=6.86355614662\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] patience losses:[6.875545501708984, 6.8714317546950445, 6.8700233234299555, 6.866746624310811, 6.867202858130137] min patience loss:6.86674662431 current loss:6.86355614662 absolute loss difference:0.00319047768911\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 288, \"sum\": 288.0, \"min\": 288}, \"Total Records Seen\": {\"count\": 1, \"max\": 36216, \"sum\": 36216.0, \"min\": 36216}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1593379052.959958, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1593379051.774268}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3816.73936093 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:32 INFO 140435207169856] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:33.748] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1176, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] # Finished training epoch 9 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Loss (name: value) total: 6.84770460924\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Loss (name: value) kld: 0.0132252063162\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Loss (name: value) recons: 6.8344793982\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Loss (name: value) logppx: 6.84770460924\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.84770460924\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] patience losses:[6.85691378513972, 6.855546686384413, 6.854756931463878, 6.851365016566382, 6.848197897275289] min patience loss:6.84819789728 current loss:6.84770460924 absolute loss difference:0.000493288040161\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 324, \"sum\": 324.0, \"min\": 324}, \"Total Records Seen\": {\"count\": 1, \"max\": 40716, \"sum\": 40716.0, \"min\": 40716}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1593379053.753097, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1593379052.571606}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3828.57752408 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:33 INFO 140483679958848] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:34.152] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 1191, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] # Finished training epoch 9 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Loss (name: value) total: 6.86494008038\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Loss (name: value) kld: 0.013576090116\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Loss (name: value) recons: 6.85136398342\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Loss (name: value) logppx: 6.86494008038\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] #quality_metric: host=algo-2, epoch=9, train total_loss <loss>=6.86494008038\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] patience losses:[6.8714317546950445, 6.8700233234299555, 6.866746624310811, 6.867202858130137, 6.863556146621704] min patience loss:6.86355614662 current loss:6.86494008038 absolute loss difference:0.00138393375609\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 324, \"sum\": 324.0, \"min\": 324}, \"Total Records Seen\": {\"count\": 1, \"max\": 40743, \"sum\": 40743.0, \"min\": 40743}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1593379054.153592, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1593379052.960612}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3793.88147416 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:34 INFO 140435207169856] # Starting training for epoch 10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:17:34.984] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1230, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] # Finished training epoch 10 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] Loss (name: value) total: 6.84635975626\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] Loss (name: value) kld: 0.0147634430064\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] Loss (name: value) recons: 6.83159632815\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] Loss (name: value) logppx: 6.84635975626\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.84635975626\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] patience losses:[6.855546686384413, 6.854756931463878, 6.851365016566382, 6.848197897275289, 6.8477046092351275] min patience loss:6.84770460924 current loss:6.84635975626 absolute loss difference:0.00134485297733\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] Timing: train: 1.23s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 45240, \"sum\": 45240.0, \"min\": 45240}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1593379054.990802, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1593379053.753386}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3655.54150203 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:34 INFO 140483679958848] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:36.136] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1145, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] # Finished training epoch 11 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] Loss (name: value) total: 6.83771532774\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] Loss (name: value) kld: 0.0151269783188\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] Loss (name: value) recons: 6.82258837091\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] Loss (name: value) logppx: 6.83771532774\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.83771532774\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] patience losses:[6.854756931463878, 6.851365016566382, 6.848197897275289, 6.8477046092351275, 6.846359756257799] min patience loss:6.84635975626 current loss:6.83771532774 absolute loss difference:0.00864442851808\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 396, \"sum\": 396.0, \"min\": 396}, \"Total Records Seen\": {\"count\": 1, \"max\": 49764, \"sum\": 49764.0, \"min\": 49764}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1593379056.141016, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1593379054.991158}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3933.82382096 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:36 INFO 140483679958848] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:35.429] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 1274, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] # Finished training epoch 10 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] Loss (name: value) total: 6.86083916823\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] Loss (name: value) kld: 0.0156137227378\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] Loss (name: value) recons: 6.84522545338\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] Loss (name: value) logppx: 6.86083916823\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] #quality_metric: host=algo-2, epoch=10, train total_loss <loss>=6.86083916823\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] patience losses:[6.8700233234299555, 6.866746624310811, 6.867202858130137, 6.863556146621704, 6.8649400803777905] min patience loss:6.86355614662 current loss:6.86083916823 absolute loss difference:0.00271697839101\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 360, \"sum\": 360.0, \"min\": 360}, \"Total Records Seen\": {\"count\": 1, \"max\": 45270, \"sum\": 45270.0, \"min\": 45270}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1593379055.43462, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1593379054.153841}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3534.14822059 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:35 INFO 140435207169856] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:37.317] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1176, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] # Finished training epoch 12 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Loss (name: value) total: 6.84121467008\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Loss (name: value) kld: 0.0204287184687\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Loss (name: value) recons: 6.8207859132\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Loss (name: value) logppx: 6.84121467008\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.84121467008\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] patience losses:[6.851365016566382, 6.848197897275289, 6.8477046092351275, 6.846359756257799, 6.837715327739716] min patience loss:6.83771532774 current loss:6.84121467008 absolute loss difference:0.0034993423356\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 54288, \"sum\": 54288.0, \"min\": 54288}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1593379057.319523, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1593379056.141317}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3839.01597048 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:37 INFO 140483679958848] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:36.633] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 1198, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] # Finished training epoch 11 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] Loss (name: value) total: 6.8562239276\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] Loss (name: value) kld: 0.0157370183281\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] Loss (name: value) recons: 6.84048693048\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] Loss (name: value) logppx: 6.8562239276\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] #quality_metric: host=algo-2, epoch=11, train total_loss <loss>=6.8562239276\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] patience losses:[6.866746624310811, 6.867202858130137, 6.863556146621704, 6.8649400803777905, 6.860839168230693] min patience loss:6.86083916823 current loss:6.8562239276 absolute loss difference:0.00461524062687\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] #progress_metric: host=algo-2, completed 11 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 396, \"sum\": 396.0, \"min\": 396}, \"Total Records Seen\": {\"count\": 1, \"max\": 49797, \"sum\": 49797.0, \"min\": 49797}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1593379056.639457, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1593379055.434925}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3757.07120732 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:36 INFO 140435207169856] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:37.847] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] # Finished training epoch 12 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] Loss (name: value) total: 6.8531117373\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] Loss (name: value) kld: 0.0193459077014\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] Loss (name: value) recons: 6.83376583126\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] Loss (name: value) logppx: 6.8531117373\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] #quality_metric: host=algo-2, epoch=12, train total_loss <loss>=6.8531117373\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] patience losses:[6.867202858130137, 6.863556146621704, 6.8649400803777905, 6.860839168230693, 6.8562239276038275] min patience loss:6.8562239276 current loss:6.8531117373 absolute loss difference:0.00311219029956\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 432, \"sum\": 432.0, \"min\": 432}, \"Total Records Seen\": {\"count\": 1, \"max\": 54324, \"sum\": 54324.0, \"min\": 54324}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1593379057.852208, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1593379056.640155}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3734.40247717 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:37 INFO 140435207169856] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:38.471] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1151, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] # Finished training epoch 13 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] Loss (name: value) total: 6.8364337484\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] Loss (name: value) kld: 0.0203494926811\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] Loss (name: value) recons: 6.81608428558\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] Loss (name: value) logppx: 6.8364337484\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.8364337484\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] patience losses:[6.848197897275289, 6.8477046092351275, 6.846359756257799, 6.837715327739716, 6.841214670075311] min patience loss:6.83771532774 current loss:6.8364337484 absolute loss difference:0.00128157933553\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 468, \"sum\": 468.0, \"min\": 468}, \"Total Records Seen\": {\"count\": 1, \"max\": 58812, \"sum\": 58812.0, \"min\": 58812}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1593379058.475494, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1593379057.319924}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3914.52775215 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:38 INFO 140483679958848] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:39.070] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 1217, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] # Finished training epoch 13 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] Loss (name: value) total: 6.85159156058\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] Loss (name: value) kld: 0.0206427296184\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] Loss (name: value) recons: 6.83094880978\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] Loss (name: value) logppx: 6.85159156058\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] #quality_metric: host=algo-2, epoch=13, train total_loss <loss>=6.85159156058\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] patience losses:[6.863556146621704, 6.8649400803777905, 6.860839168230693, 6.8562239276038275, 6.853111737304264] min patience loss:6.8531117373 current loss:6.85159156058 absolute loss difference:0.00152017672857\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] #progress_metric: host=algo-2, completed 13 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 468, \"sum\": 468.0, \"min\": 468}, \"Total Records Seen\": {\"count\": 1, \"max\": 58851, \"sum\": 58851.0, \"min\": 58851}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1593379059.076229, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1593379057.852478}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3696.15576962 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:39 INFO 140435207169856] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:39.651] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1175, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] # Finished training epoch 14 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] Loss (name: value) total: 6.83078042666\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] Loss (name: value) kld: 0.0258975144114\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] Loss (name: value) recons: 6.80488293701\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] Loss (name: value) logppx: 6.83078042666\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.83078042666\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] patience losses:[6.8477046092351275, 6.846359756257799, 6.837715327739716, 6.841214670075311, 6.836433748404185] min patience loss:6.8364337484 current loss:6.83078042666 absolute loss difference:0.00565332174301\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 63336, \"sum\": 63336.0, \"min\": 63336}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1593379059.656799, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1593379058.475735}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3829.95459488 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:39 INFO 140483679958848] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:40.342] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 1264, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] # Finished training epoch 14 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] Loss (name: value) total: 6.83719612492\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] Loss (name: value) kld: 0.0302360487015\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] Loss (name: value) recons: 6.80696005954\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] Loss (name: value) logppx: 6.83719612492\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] #quality_metric: host=algo-2, epoch=14, train total_loss <loss>=6.83719612492\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] patience losses:[6.8649400803777905, 6.860839168230693, 6.8562239276038275, 6.853111737304264, 6.851591560575697] min patience loss:6.85159156058 current loss:6.83719612492 absolute loss difference:0.0143954356511\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] Timing: train: 1.27s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 504, \"sum\": 504.0, \"min\": 504}, \"Total Records Seen\": {\"count\": 1, \"max\": 63378, \"sum\": 63378.0, \"min\": 63378}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1593379060.346166, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1593379059.077696}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3568.46989592 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:40 INFO 140435207169856] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:40.896] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1238, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] # Finished training epoch 15 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] Loss (name: value) total: 6.81363583936\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] Loss (name: value) kld: 0.0312281968072\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] Loss (name: value) recons: 6.78240764803\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] Loss (name: value) logppx: 6.81363583936\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.81363583936\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] patience losses:[6.846359756257799, 6.837715327739716, 6.841214670075311, 6.836433748404185, 6.830780426661174] min patience loss:6.83078042666 current loss:6.81363583936 absolute loss difference:0.0171445873049\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 540, \"sum\": 540.0, \"min\": 540}, \"Total Records Seen\": {\"count\": 1, \"max\": 67860, \"sum\": 67860.0, \"min\": 67860}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1593379060.901851, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1593379059.657246}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3632.9004294 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:40 INFO 140483679958848] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:42.048] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1144, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] # Finished training epoch 16 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] Loss (name: value) total: 6.7989399963\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] Loss (name: value) kld: 0.0428650539575\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] Loss (name: value) recons: 6.75607495838\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] Loss (name: value) logppx: 6.7989399963\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.7989399963\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] patience losses:[6.837715327739716, 6.841214670075311, 6.836433748404185, 6.830780426661174, 6.813635839356317] min patience loss:6.81363583936 current loss:6.7989399963 absolute loss difference:0.0146958430608\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 72384, \"sum\": 72384.0, \"min\": 72384}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1593379062.054289, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1593379060.902848}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3928.46485287 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:42 INFO 140483679958848] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:41.521] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 1175, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] # Finished training epoch 15 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] Loss (name: value) total: 6.8209687935\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] Loss (name: value) kld: 0.0397707634709\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] Loss (name: value) recons: 6.78119797839\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] Loss (name: value) logppx: 6.8209687935\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] #quality_metric: host=algo-2, epoch=15, train total_loss <loss>=6.8209687935\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] patience losses:[6.860839168230693, 6.8562239276038275, 6.853111737304264, 6.851591560575697, 6.837196124924554] min patience loss:6.83719612492 current loss:6.8209687935 absolute loss difference:0.0162273314264\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 540, \"sum\": 540.0, \"min\": 540}, \"Total Records Seen\": {\"count\": 1, \"max\": 67905, \"sum\": 67905.0, \"min\": 67905}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1593379061.526653, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1593379060.346422}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3835.23470445 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:41 INFO 140435207169856] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:43.205] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1150, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] # Finished training epoch 17 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] Loss (name: value) total: 6.78255942795\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] Loss (name: value) kld: 0.0503253019932\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] Loss (name: value) recons: 6.7322341005\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] Loss (name: value) logppx: 6.78255942795\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.78255942795\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] patience losses:[6.841214670075311, 6.836433748404185, 6.830780426661174, 6.813635839356317, 6.7989399962955055] min patience loss:6.7989399963 current loss:6.78255942795 absolute loss difference:0.0163805683454\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 612, \"sum\": 612.0, \"min\": 612}, \"Total Records Seen\": {\"count\": 1, \"max\": 76908, \"sum\": 76908.0, \"min\": 76908}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1593379063.212125, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1593379062.054586}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3907.7076747 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:43 INFO 140483679958848] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:42.743] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] # Finished training epoch 16 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] Loss (name: value) total: 6.79970246553\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] Loss (name: value) kld: 0.0490397933043\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] Loss (name: value) recons: 6.75066268444\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] Loss (name: value) logppx: 6.79970246553\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] #quality_metric: host=algo-2, epoch=16, train total_loss <loss>=6.79970246553\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] patience losses:[6.8562239276038275, 6.853111737304264, 6.851591560575697, 6.837196124924554, 6.820968793498145] min patience loss:6.8209687935 current loss:6.79970246553 absolute loss difference:0.0212663279639\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 576, \"sum\": 576.0, \"min\": 576}, \"Total Records Seen\": {\"count\": 1, \"max\": 72432, \"sum\": 72432.0, \"min\": 72432}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1593379062.748357, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1593379061.52692}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3705.0854068 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:42 INFO 140435207169856] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:43.900] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 1151, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] # Finished training epoch 17 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] Loss (name: value) total: 6.79238400194\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] Loss (name: value) kld: 0.0544598883328\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] Loss (name: value) recons: 6.73792410559\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] Loss (name: value) logppx: 6.79238400194\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] #quality_metric: host=algo-2, epoch=17, train total_loss <loss>=6.79238400194\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] patience losses:[6.853111737304264, 6.851591560575697, 6.837196124924554, 6.820968793498145, 6.79970246553421] min patience loss:6.79970246553 current loss:6.79238400194 absolute loss difference:0.00731846359041\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] #progress_metric: host=algo-2, completed 17 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 612, \"sum\": 612.0, \"min\": 612}, \"Total Records Seen\": {\"count\": 1, \"max\": 76959, \"sum\": 76959.0, \"min\": 76959}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1593379063.905958, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1593379062.749146}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3912.76229939 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:43 INFO 140435207169856] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:45.094] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1188, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] # Finished training epoch 18 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] Loss (name: value) total: 6.77459846603\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] Loss (name: value) kld: 0.0571022697517\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] Loss (name: value) recons: 6.71749620967\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] Loss (name: value) logppx: 6.77459846603\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] #quality_metric: host=algo-2, epoch=18, train total_loss <loss>=6.77459846603\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] patience losses:[6.851591560575697, 6.837196124924554, 6.820968793498145, 6.79970246553421, 6.7923840019438] min patience loss:6.79238400194 current loss:6.77459846603 absolute loss difference:0.0177855359183\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 81486, \"sum\": 81486.0, \"min\": 81486}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1593379065.098383, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1593379063.906242}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3796.88268718 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:45 INFO 140435207169856] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:46.229] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1130, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] # Finished training epoch 19 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] Loss (name: value) total: 6.76219695807\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] Loss (name: value) kld: 0.0597441253356\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] Loss (name: value) recons: 6.7024527457\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] Loss (name: value) logppx: 6.76219695807\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] #quality_metric: host=algo-2, epoch=19, train total_loss <loss>=6.76219695807\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] patience losses:[6.837196124924554, 6.820968793498145, 6.79970246553421, 6.7923840019438, 6.774598466025458] min patience loss:6.77459846603 current loss:6.76219695807 absolute loss difference:0.0124015079604\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] Timing: train: 1.13s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] #progress_metric: host=algo-2, completed 19 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 684, \"sum\": 684.0, \"min\": 684}, \"Total Records Seen\": {\"count\": 1, \"max\": 86013, \"sum\": 86013.0, \"min\": 86013}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1593379066.234552, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1593379065.098682}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3984.98615426 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:46 INFO 140435207169856] # Starting training for epoch 20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:17:44.374] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 1161, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] # Finished training epoch 18 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] Loss (name: value) total: 6.76773350769\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] Loss (name: value) kld: 0.054659607458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] Loss (name: value) recons: 6.71307391591\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] Loss (name: value) logppx: 6.76773350769\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.76773350769\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] patience losses:[6.836433748404185, 6.830780426661174, 6.813635839356317, 6.7989399962955055, 6.782559427950117] min patience loss:6.78255942795 current loss:6.76773350769 absolute loss difference:0.0148259202639\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 648, \"sum\": 648.0, \"min\": 648}, \"Total Records Seen\": {\"count\": 1, \"max\": 81432, \"sum\": 81432.0, \"min\": 81432}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1593379064.378075, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1593379063.212432}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3880.65066716 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:44 INFO 140483679958848] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:45.543] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 1164, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] # Finished training epoch 19 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] Loss (name: value) total: 6.76488460435\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] Loss (name: value) kld: 0.0590460133842\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] Loss (name: value) recons: 6.70583858755\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] Loss (name: value) logppx: 6.76488460435\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.76488460435\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] patience losses:[6.830780426661174, 6.813635839356317, 6.7989399962955055, 6.782559427950117, 6.7677335076861915] min patience loss:6.76773350769 current loss:6.76488460435 absolute loss difference:0.00284890333811\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 684, \"sum\": 684.0, \"min\": 684}, \"Total Records Seen\": {\"count\": 1, \"max\": 85956, \"sum\": 85956.0, \"min\": 85956}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1593379065.548888, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1593379064.37836}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3864.40085104 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:45 INFO 140483679958848] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:46.760] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] # Finished training epoch 20 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] Loss (name: value) total: 6.75190052059\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] Loss (name: value) kld: 0.0607651020918\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] Loss (name: value) recons: 6.69113545285\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] Loss (name: value) logppx: 6.75190052059\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.75190052059\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] patience losses:[6.813635839356317, 6.7989399962955055, 6.782559427950117, 6.7677335076861915, 6.764884604348077] min patience loss:6.76488460435 current loss:6.75190052059 absolute loss difference:0.0129840837585\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 90480, \"sum\": 90480.0, \"min\": 90480}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1593379066.766276, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1593379065.549178}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3716.58701647 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:46 INFO 140483679958848] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:47.959] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] # Finished training epoch 21 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] Loss (name: value) total: 6.7505675488\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] Loss (name: value) kld: 0.0637134225108\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] Loss (name: value) recons: 6.68685414394\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] Loss (name: value) logppx: 6.7505675488\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.7505675488\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] patience losses:[6.7989399962955055, 6.782559427950117, 6.7677335076861915, 6.764884604348077, 6.751900520589617] min patience loss:6.75190052059 current loss:6.7505675488 absolute loss difference:0.0013329717848\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 756, \"sum\": 756.0, \"min\": 756}, \"Total Records Seen\": {\"count\": 1, \"max\": 95004, \"sum\": 95004.0, \"min\": 95004}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1593379067.962851, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1593379066.766574}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3781.28397964 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:47 INFO 140483679958848] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:47.473] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 1238, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] # Finished training epoch 20 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Loss (name: value) total: 6.76543457641\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Loss (name: value) kld: 0.0650947523407\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Loss (name: value) recons: 6.7003398339\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Loss (name: value) logppx: 6.76543457641\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] #quality_metric: host=algo-2, epoch=20, train total_loss <loss>=6.76543457641\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] patience losses:[6.820968793498145, 6.79970246553421, 6.7923840019438, 6.774598466025458, 6.762196958065033] min patience loss:6.76219695807 current loss:6.76543457641 absolute loss difference:0.00323761834039\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 720, \"sum\": 720.0, \"min\": 720}, \"Total Records Seen\": {\"count\": 1, \"max\": 90540, \"sum\": 90540.0, \"min\": 90540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1593379067.475879, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1593379066.234878}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3647.38831433 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:47 INFO 140435207169856] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:49.165] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1201, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] # Finished training epoch 22 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] Loss (name: value) total: 6.74517611663\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] Loss (name: value) kld: 0.0664598204506\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] Loss (name: value) recons: 6.67871628205\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] Loss (name: value) logppx: 6.74517611663\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.74517611663\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] patience losses:[6.782559427950117, 6.7677335076861915, 6.764884604348077, 6.751900520589617, 6.750567548804813] min patience loss:6.7505675488 current loss:6.74517611663 absolute loss difference:0.00539143217935\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 99528, \"sum\": 99528.0, \"min\": 99528}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1593379069.169624, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1593379067.963175}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3749.24151754 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:49 INFO 140483679958848] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:48.701] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 1225, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] # Finished training epoch 21 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] Loss (name: value) total: 6.75736080938\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] Loss (name: value) kld: 0.0670136674825\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] Loss (name: value) recons: 6.69034709533\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] Loss (name: value) logppx: 6.75736080938\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] #quality_metric: host=algo-2, epoch=21, train total_loss <loss>=6.75736080938\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] patience losses:[6.79970246553421, 6.7923840019438, 6.774598466025458, 6.762196958065033, 6.765434576405419] min patience loss:6.76219695807 current loss:6.75736080938 absolute loss difference:0.00483614868588\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] #progress_metric: host=algo-2, completed 21 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 756, \"sum\": 756.0, \"min\": 756}, \"Total Records Seen\": {\"count\": 1, \"max\": 95067, \"sum\": 95067.0, \"min\": 95067}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1593379068.7059, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1593379067.476227}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3681.0178892 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:48 INFO 140435207169856] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:49.957] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 1251, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] # Finished training epoch 22 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] Loss (name: value) total: 6.7494195501\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] Loss (name: value) kld: 0.0676733144145\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] Loss (name: value) recons: 6.68174623781\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] Loss (name: value) logppx: 6.7494195501\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] #quality_metric: host=algo-2, epoch=22, train total_loss <loss>=6.7494195501\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] patience losses:[6.7923840019438, 6.774598466025458, 6.762196958065033, 6.765434576405419, 6.757360809379154] min patience loss:6.75736080938 current loss:6.7494195501 absolute loss difference:0.00794125927819\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 792, \"sum\": 792.0, \"min\": 792}, \"Total Records Seen\": {\"count\": 1, \"max\": 99594, \"sum\": 99594.0, \"min\": 99594}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1593379069.964266, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1593379068.706194}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3597.39337295 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:49 INFO 140435207169856] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:50.380] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] # Finished training epoch 23 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] Loss (name: value) total: 6.73746233516\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] Loss (name: value) kld: 0.0676375771355\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] Loss (name: value) recons: 6.66982471281\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] Loss (name: value) logppx: 6.73746233516\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.73746233516\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] patience losses:[6.7677335076861915, 6.764884604348077, 6.751900520589617, 6.750567548804813, 6.745176116625468] min patience loss:6.74517611663 current loss:6.73746233516 absolute loss difference:0.00771378146278\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 828, \"sum\": 828.0, \"min\": 828}, \"Total Records Seen\": {\"count\": 1, \"max\": 104052, \"sum\": 104052.0, \"min\": 104052}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1593379070.386419, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1593379069.169933}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3718.37646956 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:50 INFO 140483679958848] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:51.197] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 1231, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] # Finished training epoch 23 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] Loss (name: value) total: 6.74633594354\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] Loss (name: value) kld: 0.073361937888\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] Loss (name: value) recons: 6.67297396395\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] Loss (name: value) logppx: 6.74633594354\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] #quality_metric: host=algo-2, epoch=23, train total_loss <loss>=6.74633594354\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] patience losses:[6.774598466025458, 6.762196958065033, 6.765434576405419, 6.757360809379154, 6.749419550100963] min patience loss:6.7494195501 current loss:6.74633594354 absolute loss difference:0.00308360656103\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] Timing: train: 1.23s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] #progress_metric: host=algo-2, completed 23 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 828, \"sum\": 828.0, \"min\": 828}, \"Total Records Seen\": {\"count\": 1, \"max\": 104121, \"sum\": 104121.0, \"min\": 104121}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1593379071.202551, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1593379069.964818}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3657.10606016 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:51 INFO 140435207169856] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:51.522] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1135, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] # Finished training epoch 24 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Loss (name: value) total: 6.74232136541\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Loss (name: value) kld: 0.0718400980242\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Loss (name: value) recons: 6.67048121823\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Loss (name: value) logppx: 6.74232136541\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.74232136541\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] patience losses:[6.764884604348077, 6.751900520589617, 6.750567548804813, 6.745176116625468, 6.737462335162693] min patience loss:6.73746233516 current loss:6.74232136541 absolute loss difference:0.00485903024673\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 864, \"sum\": 864.0, \"min\": 864}, \"Total Records Seen\": {\"count\": 1, \"max\": 108576, \"sum\": 108576.0, \"min\": 108576}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1593379071.523526, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1593379070.386709}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3979.06674546 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:51 INFO 140483679958848] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:52.368] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 1165, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] # Finished training epoch 24 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] Loss (name: value) total: 6.74202215672\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] Loss (name: value) kld: 0.0743390068205\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] Loss (name: value) recons: 6.66768315103\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] Loss (name: value) logppx: 6.74202215672\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] #quality_metric: host=algo-2, epoch=24, train total_loss <loss>=6.74202215672\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] patience losses:[6.762196958065033, 6.765434576405419, 6.757360809379154, 6.749419550100963, 6.746335943539937] min patience loss:6.74633594354 current loss:6.74202215672 absolute loss difference:0.00431378682454\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 864, \"sum\": 864.0, \"min\": 864}, \"Total Records Seen\": {\"count\": 1, \"max\": 108648, \"sum\": 108648.0, \"min\": 108648}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1593379072.37354, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1593379071.202788}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3865.25368187 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:52 INFO 140435207169856] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:52.657] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1133, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] # Finished training epoch 25 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] Loss (name: value) total: 6.72986864381\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] Loss (name: value) kld: 0.0730662824483\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] Loss (name: value) recons: 6.65680239598\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] Loss (name: value) logppx: 6.72986864381\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.72986864381\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] patience losses:[6.751900520589617, 6.750567548804813, 6.745176116625468, 6.737462335162693, 6.742321365409428] min patience loss:6.73746233516 current loss:6.72986864381 absolute loss difference:0.00759369134903\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] Timing: train: 1.13s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 900, \"sum\": 900.0, \"min\": 900}, \"Total Records Seen\": {\"count\": 1, \"max\": 113100, \"sum\": 113100.0, \"min\": 113100}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1593379072.662617, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1593379071.523763}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3971.94031235 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:52 INFO 140483679958848] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:53.766] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1102, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] # Finished training epoch 26 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] Loss (name: value) total: 6.72855499718\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] Loss (name: value) kld: 0.0764790623863\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] Loss (name: value) recons: 6.65207589997\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] Loss (name: value) logppx: 6.72855499718\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.72855499718\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] patience losses:[6.750567548804813, 6.745176116625468, 6.737462335162693, 6.742321365409428, 6.7298686438136635] min patience loss:6.72986864381 current loss:6.72855499718 absolute loss difference:0.00131364663442\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] Timing: train: 1.10s, val: 0.00s, epoch: 1.11s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 936, \"sum\": 936.0, \"min\": 936}, \"Total Records Seen\": {\"count\": 1, \"max\": 117624, \"sum\": 117624.0, \"min\": 117624}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1593379073.77087, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1593379072.662898}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4082.64228948 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:53 INFO 140483679958848] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:54.917] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1145, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] # Finished training epoch 27 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] Loss (name: value) total: 6.71630969312\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] Loss (name: value) kld: 0.079660449384\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] Loss (name: value) recons: 6.63664926423\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] Loss (name: value) logppx: 6.71630969312\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.71630969312\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] patience losses:[6.745176116625468, 6.737462335162693, 6.742321365409428, 6.7298686438136635, 6.728554997179243] min patience loss:6.72855499718 current loss:6.71630969312 absolute loss difference:0.0122453040547\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 972, \"sum\": 972.0, \"min\": 972}, \"Total Records Seen\": {\"count\": 1, \"max\": 122148, \"sum\": 122148.0, \"min\": 122148}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1593379074.921479, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1593379073.771124}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3932.00518381 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:54 INFO 140483679958848] # Starting training for epoch 28\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m[2020-06-28 21:17:53.549] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 1175, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] # Finished training epoch 25 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] Loss (name: value) total: 6.73203800122\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] Loss (name: value) kld: 0.0762096454079\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] Loss (name: value) recons: 6.65582833025\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] Loss (name: value) logppx: 6.73203800122\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] #quality_metric: host=algo-2, epoch=25, train total_loss <loss>=6.73203800122\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] patience losses:[6.765434576405419, 6.757360809379154, 6.749419550100963, 6.746335943539937, 6.742022156715393] min patience loss:6.74202215672 current loss:6.73203800122 absolute loss difference:0.00998415549596\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] #progress_metric: host=algo-2, completed 25 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 900, \"sum\": 900.0, \"min\": 900}, \"Total Records Seen\": {\"count\": 1, \"max\": 113175, \"sum\": 113175.0, \"min\": 113175}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1593379073.554669, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1593379072.374225}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3834.52059703 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:53 INFO 140435207169856] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:54.747] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 1191, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] # Finished training epoch 26 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] Loss (name: value) total: 6.72752838665\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] Loss (name: value) kld: 0.0809310326974\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] Loss (name: value) recons: 6.64659733242\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] Loss (name: value) logppx: 6.72752838665\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] #quality_metric: host=algo-2, epoch=26, train total_loss <loss>=6.72752838665\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] patience losses:[6.757360809379154, 6.749419550100963, 6.746335943539937, 6.742022156715393, 6.732038001219432] min patience loss:6.73203800122 current loss:6.72752838665 absolute loss difference:0.00450961457358\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 936, \"sum\": 936.0, \"min\": 936}, \"Total Records Seen\": {\"count\": 1, \"max\": 117702, \"sum\": 117702.0, \"min\": 117702}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1593379074.752593, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1593379073.554963}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3778.76655592 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:54 INFO 140435207169856] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:56.099] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] # Finished training epoch 28 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] Loss (name: value) total: 6.70922652218\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] Loss (name: value) kld: 0.0841824801432\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] Loss (name: value) recons: 6.62504396174\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] Loss (name: value) logppx: 6.70922652218\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.70922652218\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] patience losses:[6.737462335162693, 6.742321365409428, 6.7298686438136635, 6.728554997179243, 6.716309693124559] min patience loss:6.71630969312 current loss:6.70922652218 absolute loss difference:0.00708317094379\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1008, \"sum\": 1008.0, \"min\": 1008}, \"Total Records Seen\": {\"count\": 1, \"max\": 126672, \"sum\": 126672.0, \"min\": 126672}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1593379076.104019, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1593379074.921878}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3826.34555987 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:56 INFO 140483679958848] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:55.992] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 1236, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] # Finished training epoch 27 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] Loss (name: value) total: 6.71778790818\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] Loss (name: value) kld: 0.0834710645075\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] Loss (name: value) recons: 6.63431690137\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] Loss (name: value) logppx: 6.71778790818\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] #quality_metric: host=algo-2, epoch=27, train total_loss <loss>=6.71778790818\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] patience losses:[6.749419550100963, 6.746335943539937, 6.742022156715393, 6.732038001219432, 6.727528386645847] min patience loss:6.72752838665 current loss:6.71778790818 absolute loss difference:0.00974047846264\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] #progress_metric: host=algo-2, completed 27 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 972, \"sum\": 972.0, \"min\": 972}, \"Total Records Seen\": {\"count\": 1, \"max\": 122229, \"sum\": 122229.0, \"min\": 122229}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1593379075.997255, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1593379074.753312}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3638.5411522 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:55 INFO 140435207169856] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:57.313] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] # Finished training epoch 29 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Loss (name: value) total: 6.70957769288\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Loss (name: value) kld: 0.0901045983968\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Loss (name: value) recons: 6.61947310633\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Loss (name: value) logppx: 6.70957769288\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.70957769288\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] patience losses:[6.742321365409428, 6.7298686438136635, 6.728554997179243, 6.716309693124559, 6.709226522180769] min patience loss:6.70922652218 current loss:6.70957769288 absolute loss difference:0.000351170698802\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1044, \"sum\": 1044.0, \"min\": 1044}, \"Total Records Seen\": {\"count\": 1, \"max\": 131196, \"sum\": 131196.0, \"min\": 131196}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1593379077.315089, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1593379076.104351}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3736.08226416 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:57 INFO 140483679958848] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:57.190] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] # Finished training epoch 28 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] Loss (name: value) total: 6.71513473325\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] Loss (name: value) kld: 0.0900662689366\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] Loss (name: value) recons: 6.62506848574\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] Loss (name: value) logppx: 6.71513473325\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] #quality_metric: host=algo-2, epoch=28, train total_loss <loss>=6.71513473325\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] patience losses:[6.746335943539937, 6.742022156715393, 6.732038001219432, 6.727528386645847, 6.717787908183204] min patience loss:6.71778790818 current loss:6.71513473325 absolute loss difference:0.00265317493015\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1008, \"sum\": 1008.0, \"min\": 1008}, \"Total Records Seen\": {\"count\": 1, \"max\": 126756, \"sum\": 126756.0, \"min\": 126756}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1593379077.195623, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1593379075.997604}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3777.46751109 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:57 INFO 140435207169856] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:58.351] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 1154, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] # Finished training epoch 29 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] Loss (name: value) total: 6.71312484476\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] Loss (name: value) kld: 0.0933632884795\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] Loss (name: value) recons: 6.61976153983\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] Loss (name: value) logppx: 6.71312484476\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] #quality_metric: host=algo-2, epoch=29, train total_loss <loss>=6.71312484476\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] patience losses:[6.742022156715393, 6.732038001219432, 6.727528386645847, 6.717787908183204, 6.7151347332530555] min patience loss:6.71513473325 current loss:6.71312484476 absolute loss difference:0.00200988849004\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] Timing: train: 1.16s, val: 0.01s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] #progress_metric: host=algo-2, completed 29 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1044, \"sum\": 1044.0, \"min\": 1044}, \"Total Records Seen\": {\"count\": 1, \"max\": 131283, \"sum\": 131283.0, \"min\": 131283}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1593379078.357617, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1593379077.196377}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3897.55355355 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:58 INFO 140435207169856] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:58.459] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1144, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] # Finished training epoch 30 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] Loss (name: value) total: 6.69815500577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] Loss (name: value) kld: 0.0913345685436\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] Loss (name: value) recons: 6.60682040453\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] Loss (name: value) logppx: 6.69815500577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.69815500577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] patience losses:[6.7298686438136635, 6.728554997179243, 6.716309693124559, 6.709226522180769, 6.709577692879571] min patience loss:6.70922652218 current loss:6.69815500577 absolute loss difference:0.0110715164079\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1080, \"sum\": 1080.0, \"min\": 1080}, \"Total Records Seen\": {\"count\": 1, \"max\": 135720, \"sum\": 135720.0, \"min\": 135720}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1593379078.465322, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1593379077.315355}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3933.51638496 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:58 INFO 140483679958848] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:17:59.579] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1113, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] # Finished training epoch 31 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] Loss (name: value) total: 6.69023212459\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] Loss (name: value) kld: 0.0946440235194\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] Loss (name: value) recons: 6.5955881079\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] Loss (name: value) logppx: 6.69023212459\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.69023212459\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] patience losses:[6.728554997179243, 6.716309693124559, 6.709226522180769, 6.709577692879571, 6.698155005772908] min patience loss:6.69815500577 current loss:6.69023212459 absolute loss difference:0.00792288117939\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] Timing: train: 1.12s, val: 0.00s, epoch: 1.12s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1116, \"sum\": 1116.0, \"min\": 1116}, \"Total Records Seen\": {\"count\": 1, \"max\": 140244, \"sum\": 140244.0, \"min\": 140244}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1593379079.58461, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1593379078.465675}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4042.6173824 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:17:59 INFO 140483679958848] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:17:59.548] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 1189, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] # Finished training epoch 30 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] Loss (name: value) total: 6.69897713926\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] Loss (name: value) kld: 0.0959763180258\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] Loss (name: value) recons: 6.60300074683\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] Loss (name: value) logppx: 6.69897713926\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] #quality_metric: host=algo-2, epoch=30, train total_loss <loss>=6.69897713926\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] patience losses:[6.732038001219432, 6.727528386645847, 6.717787908183204, 6.7151347332530555, 6.713124844763014] min patience loss:6.71312484476 current loss:6.69897713926 absolute loss difference:0.014147705502\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1080, \"sum\": 1080.0, \"min\": 1080}, \"Total Records Seen\": {\"count\": 1, \"max\": 135810, \"sum\": 135810.0, \"min\": 135810}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1593379079.554804, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1593379078.358084}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3781.3296754 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:17:59 INFO 140435207169856] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:00.794] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] # Finished training epoch 32 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Loss (name: value) total: 6.69047853682\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Loss (name: value) kld: 0.097857610633\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Loss (name: value) recons: 6.59262088935\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Loss (name: value) logppx: 6.69047853682\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.69047853682\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] patience losses:[6.716309693124559, 6.709226522180769, 6.709577692879571, 6.698155005772908, 6.690232124593523] min patience loss:6.69023212459 current loss:6.69047853682 absolute loss difference:0.000246412224239\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1152, \"sum\": 1152.0, \"min\": 1152}, \"Total Records Seen\": {\"count\": 1, \"max\": 144768, \"sum\": 144768.0, \"min\": 144768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1593379080.796007, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1593379079.584865}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3734.84831049 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:00 INFO 140483679958848] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:00.777] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] # Finished training epoch 31 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] Loss (name: value) total: 6.69376887878\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] Loss (name: value) kld: 0.0988000758096\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] Loss (name: value) recons: 6.59496876929\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] Loss (name: value) logppx: 6.69376887878\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] #quality_metric: host=algo-2, epoch=31, train total_loss <loss>=6.69376887878\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] patience losses:[6.727528386645847, 6.717787908183204, 6.7151347332530555, 6.713124844763014, 6.698977139261034] min patience loss:6.69897713926 current loss:6.69376887878 absolute loss difference:0.00520826048321\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] #progress_metric: host=algo-2, completed 31 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1116, \"sum\": 1116.0, \"min\": 1116}, \"Total Records Seen\": {\"count\": 1, \"max\": 140337, \"sum\": 140337.0, \"min\": 140337}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1593379080.782096, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1593379079.555508}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3690.29827304 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:00 INFO 140435207169856] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:01.933] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1137, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] # Finished training epoch 33 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] Loss (name: value) total: 6.68290050162\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] Loss (name: value) kld: 0.100006366252\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] Loss (name: value) recons: 6.5828941332\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] Loss (name: value) logppx: 6.68290050162\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.68290050162\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] patience losses:[6.709226522180769, 6.709577692879571, 6.698155005772908, 6.690232124593523, 6.690478536817762] min patience loss:6.69023212459 current loss:6.68290050162 absolute loss difference:0.00733162297143\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1188, \"sum\": 1188.0, \"min\": 1188}, \"Total Records Seen\": {\"count\": 1, \"max\": 149292, \"sum\": 149292.0, \"min\": 149292}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1593379081.939652, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1593379080.796265}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3956.13983467 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:01 INFO 140483679958848] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:01.974] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 1191, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] # Finished training epoch 32 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] Loss (name: value) total: 6.68854957819\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] Loss (name: value) kld: 0.0999963127284\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] Loss (name: value) recons: 6.5885532962\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] Loss (name: value) logppx: 6.68854957819\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] #quality_metric: host=algo-2, epoch=32, train total_loss <loss>=6.68854957819\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] patience losses:[6.717787908183204, 6.7151347332530555, 6.713124844763014, 6.698977139261034, 6.693768878777822] min patience loss:6.69376887878 current loss:6.68854957819 absolute loss difference:0.00521930058797\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1152, \"sum\": 1152.0, \"min\": 1152}, \"Total Records Seen\": {\"count\": 1, \"max\": 144864, \"sum\": 144864.0, \"min\": 144864}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1593379081.979844, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1593379080.782319}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3779.88363662 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:01 INFO 140435207169856] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:03.153] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1212, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] # Finished training epoch 34 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] Loss (name: value) total: 6.67680015167\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] Loss (name: value) kld: 0.100367975628\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] Loss (name: value) recons: 6.57643218173\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] Loss (name: value) logppx: 6.67680015167\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.67680015167\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] patience losses:[6.709577692879571, 6.698155005772908, 6.690232124593523, 6.690478536817762, 6.682900501622094] min patience loss:6.68290050162 current loss:6.67680015167 absolute loss difference:0.00610034995609\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1224, \"sum\": 1224.0, \"min\": 1224}, \"Total Records Seen\": {\"count\": 1, \"max\": 153816, \"sum\": 153816.0, \"min\": 153816}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1593379083.158815, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1593379081.939959}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3711.19558702 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:03 INFO 140483679958848] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:03.183] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 1202, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] # Finished training epoch 33 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] Loss (name: value) total: 6.68491245641\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] Loss (name: value) kld: 0.102926918098\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] Loss (name: value) recons: 6.58198555973\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] Loss (name: value) logppx: 6.68491245641\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] #quality_metric: host=algo-2, epoch=33, train total_loss <loss>=6.68491245641\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] patience losses:[6.7151347332530555, 6.713124844763014, 6.698977139261034, 6.693768878777822, 6.68854957818985] min patience loss:6.68854957819 current loss:6.68491245641 absolute loss difference:0.00363712178336\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] #progress_metric: host=algo-2, completed 33 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1188, \"sum\": 1188.0, \"min\": 1188}, \"Total Records Seen\": {\"count\": 1, \"max\": 149391, \"sum\": 149391.0, \"min\": 149391}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1593379083.188922, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1593379081.980111}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3744.61517381 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:03 INFO 140435207169856] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:04.288] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1128, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] # Finished training epoch 35 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Loss (name: value) total: 6.67669586341\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Loss (name: value) kld: 0.105710499092\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Loss (name: value) recons: 6.57098537683\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Loss (name: value) logppx: 6.67669586341\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.67669586341\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] patience losses:[6.698155005772908, 6.690232124593523, 6.690478536817762, 6.682900501622094, 6.676800151666005] min patience loss:6.67680015167 current loss:6.67669586341 absolute loss difference:0.000104288260141\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] Timing: train: 1.13s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1260, \"sum\": 1260.0, \"min\": 1260}, \"Total Records Seen\": {\"count\": 1, \"max\": 158340, \"sum\": 158340.0, \"min\": 158340}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1593379084.293038, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1593379083.159129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3989.19634824 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:04 INFO 140483679958848] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:04.349] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 1159, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] # Finished training epoch 34 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] Loss (name: value) total: 6.68164194955\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] Loss (name: value) kld: 0.107080195099\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] Loss (name: value) recons: 6.57456175486\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] Loss (name: value) logppx: 6.68164194955\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] #quality_metric: host=algo-2, epoch=34, train total_loss <loss>=6.68164194955\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] patience losses:[6.713124844763014, 6.698977139261034, 6.693768878777822, 6.68854957818985, 6.6849124564064875] min patience loss:6.68491245641 current loss:6.68164194955 absolute loss difference:0.00327050685883\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1224, \"sum\": 1224.0, \"min\": 1224}, \"Total Records Seen\": {\"count\": 1, \"max\": 153918, \"sum\": 153918.0, \"min\": 153918}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1593379084.354096, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1593379083.189181}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3885.68276648 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:04 INFO 140435207169856] # Starting training for epoch 35\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:18:05.500] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1206, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] # Finished training epoch 36 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] Loss (name: value) total: 6.67294475105\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] Loss (name: value) kld: 0.10592392118\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] Loss (name: value) recons: 6.56702084674\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] Loss (name: value) logppx: 6.67294475105\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.67294475105\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] patience losses:[6.690232124593523, 6.690478536817762, 6.682900501622094, 6.676800151666005, 6.676695863405864] min patience loss:6.67669586341 current loss:6.67294475105 absolute loss difference:0.00375111235513\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1296, \"sum\": 1296.0, \"min\": 1296}, \"Total Records Seen\": {\"count\": 1, \"max\": 162864, \"sum\": 162864.0, \"min\": 162864}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1593379085.506319, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1593379084.293298}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3728.94428229 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:05 INFO 140483679958848] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:05.604] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 1250, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] # Finished training epoch 35 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] Loss (name: value) total: 6.67489486933\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] Loss (name: value) kld: 0.106046911329\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] Loss (name: value) recons: 6.56884793441\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] Loss (name: value) logppx: 6.67489486933\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] #quality_metric: host=algo-2, epoch=35, train total_loss <loss>=6.67489486933\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] patience losses:[6.698977139261034, 6.693768878777822, 6.68854957818985, 6.6849124564064875, 6.681641949547662] min patience loss:6.68164194955 current loss:6.67489486933 absolute loss difference:0.00674708022012\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] #progress_metric: host=algo-2, completed 35 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1260, \"sum\": 1260.0, \"min\": 1260}, \"Total Records Seen\": {\"count\": 1, \"max\": 158445, \"sum\": 158445.0, \"min\": 158445}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1593379085.609265, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1593379084.35434}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3606.97637903 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:05 INFO 140435207169856] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:06.700] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1193, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] # Finished training epoch 37 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] Loss (name: value) total: 6.66527932882\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] Loss (name: value) kld: 0.108292918859\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] Loss (name: value) recons: 6.55698637168\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] Loss (name: value) logppx: 6.66527932882\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.66527932882\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] patience losses:[6.690478536817762, 6.682900501622094, 6.676800151666005, 6.676695863405864, 6.672944751050737] min patience loss:6.67294475105 current loss:6.66527932882 absolute loss difference:0.00766542222765\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1332, \"sum\": 1332.0, \"min\": 1332}, \"Total Records Seen\": {\"count\": 1, \"max\": 167388, \"sum\": 167388.0, \"min\": 167388}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1593379086.704317, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1593379085.506697}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3776.89980155 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:06 INFO 140483679958848] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:06.814] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 1200, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] # Finished training epoch 36 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Loss (name: value) total: 6.67651846011\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Loss (name: value) kld: 0.109604286133\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Loss (name: value) recons: 6.56691418754\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Loss (name: value) logppx: 6.67651846011\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] #quality_metric: host=algo-2, epoch=36, train total_loss <loss>=6.67651846011\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] patience losses:[6.693768878777822, 6.68854957818985, 6.6849124564064875, 6.681641949547662, 6.674894869327545] min patience loss:6.67489486933 current loss:6.67651846011 absolute loss difference:0.00162359078725\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1296, \"sum\": 1296.0, \"min\": 1296}, \"Total Records Seen\": {\"count\": 1, \"max\": 162972, \"sum\": 162972.0, \"min\": 162972}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1593379086.817154, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1593379085.609561}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3748.06561656 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:06 INFO 140435207169856] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:07.824] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1119, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] # Finished training epoch 38 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] Loss (name: value) total: 6.66230975919\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] Loss (name: value) kld: 0.108722854509\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] Loss (name: value) recons: 6.55358692673\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] Loss (name: value) logppx: 6.66230975919\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.66230975919\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] patience losses:[6.682900501622094, 6.676800151666005, 6.676695863405864, 6.672944751050737, 6.66527932882309] min patience loss:6.66527932882 current loss:6.66230975919 absolute loss difference:0.00296956963009\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] Timing: train: 1.12s, val: 0.00s, epoch: 1.12s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1368, \"sum\": 1368.0, \"min\": 1368}, \"Total Records Seen\": {\"count\": 1, \"max\": 171912, \"sum\": 171912.0, \"min\": 171912}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1593379087.828876, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1593379086.704596}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4023.26456499 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:07 INFO 140483679958848] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:08.004] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 1185, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] # Finished training epoch 37 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] Loss (name: value) total: 6.67247958316\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] Loss (name: value) kld: 0.110980334485\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] Loss (name: value) recons: 6.56149925126\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] Loss (name: value) logppx: 6.67247958316\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] #quality_metric: host=algo-2, epoch=37, train total_loss <loss>=6.67247958316\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] patience losses:[6.68854957818985, 6.6849124564064875, 6.681641949547662, 6.674894869327545, 6.676518460114797] min patience loss:6.67489486933 current loss:6.67247958316 absolute loss difference:0.00241528617011\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] #progress_metric: host=algo-2, completed 37 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1332, \"sum\": 1332.0, \"min\": 1332}, \"Total Records Seen\": {\"count\": 1, \"max\": 167499, \"sum\": 167499.0, \"min\": 167499}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1593379088.008361, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1593379086.817609}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3801.35232393 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:08 INFO 140435207169856] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:08.988] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1159, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] # Finished training epoch 39 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] Loss (name: value) total: 6.65846174293\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] Loss (name: value) kld: 0.113723527226\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] Loss (name: value) recons: 6.54473823971\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] Loss (name: value) logppx: 6.65846174293\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=6.65846174293\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] patience losses:[6.676800151666005, 6.676695863405864, 6.672944751050737, 6.66527932882309, 6.662309759192997] min patience loss:6.66230975919 current loss:6.65846174293 absolute loss difference:0.00384801626205\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1404, \"sum\": 1404.0, \"min\": 1404}, \"Total Records Seen\": {\"count\": 1, \"max\": 176436, \"sum\": 176436.0, \"min\": 176436}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1593379088.993172, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1593379087.829202}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3886.0804404 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:08 INFO 140483679958848] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:09.230] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] # Finished training epoch 38 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] Loss (name: value) total: 6.66360167662\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] Loss (name: value) kld: 0.113552872505\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] Loss (name: value) recons: 6.55004880826\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] Loss (name: value) logppx: 6.66360167662\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] #quality_metric: host=algo-2, epoch=38, train total_loss <loss>=6.66360167662\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] patience losses:[6.6849124564064875, 6.681641949547662, 6.674894869327545, 6.676518460114797, 6.6724795831574335] min patience loss:6.67247958316 current loss:6.66360167662 absolute loss difference:0.00887790653441\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] #progress_metric: host=algo-2, completed 38 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1368, \"sum\": 1368.0, \"min\": 1368}, \"Total Records Seen\": {\"count\": 1, \"max\": 172026, \"sum\": 172026.0, \"min\": 172026}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1593379089.235707, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1593379088.008604}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3688.72034627 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:09 INFO 140435207169856] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:10.120] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1126, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] # Finished training epoch 40 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] Loss (name: value) total: 6.65521550841\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] Loss (name: value) kld: 0.117946706091\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] Loss (name: value) recons: 6.53726884392\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] Loss (name: value) logppx: 6.65521550841\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=6.65521550841\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] patience losses:[6.676695863405864, 6.672944751050737, 6.66527932882309, 6.662309759192997, 6.6584617429309425] min patience loss:6.65846174293 current loss:6.65521550841 absolute loss difference:0.00324623452293\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] Timing: train: 1.13s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1440, \"sum\": 1440.0, \"min\": 1440}, \"Total Records Seen\": {\"count\": 1, \"max\": 180960, \"sum\": 180960.0, \"min\": 180960}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1593379090.124627, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1593379088.993506}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3999.08770285 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:10 INFO 140483679958848] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:10.382] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 1143, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] # Finished training epoch 39 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] Loss (name: value) total: 6.65503156847\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] Loss (name: value) kld: 0.116230978941\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] Loss (name: value) recons: 6.53880055745\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] Loss (name: value) logppx: 6.65503156847\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] #quality_metric: host=algo-2, epoch=39, train total_loss <loss>=6.65503156847\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] patience losses:[6.681641949547662, 6.674894869327545, 6.676518460114797, 6.6724795831574335, 6.663601676623027] min patience loss:6.66360167662 current loss:6.65503156847 absolute loss difference:0.00857010814879\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] #progress_metric: host=algo-2, completed 39 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1404, \"sum\": 1404.0, \"min\": 1404}, \"Total Records Seen\": {\"count\": 1, \"max\": 176553, \"sum\": 176553.0, \"min\": 176553}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1593379090.387043, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1593379089.236006}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3932.42874202 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:10 INFO 140435207169856] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:11.340] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] # Finished training epoch 41 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] Loss (name: value) total: 6.64754961597\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] Loss (name: value) kld: 0.1189417243\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] Loss (name: value) recons: 6.52860788504\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] Loss (name: value) logppx: 6.64754961597\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=6.64754961597\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] patience losses:[6.672944751050737, 6.66527932882309, 6.662309759192997, 6.6584617429309425, 6.655215508408016] min patience loss:6.65521550841 current loss:6.64754961597 absolute loss difference:0.00766589244207\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1476, \"sum\": 1476.0, \"min\": 1476}, \"Total Records Seen\": {\"count\": 1, \"max\": 185484, \"sum\": 185484.0, \"min\": 185484}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1593379091.343477, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1593379090.124861}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3712.02759436 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:11 INFO 140483679958848] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:11.643] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 1255, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] # Finished training epoch 40 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] Loss (name: value) total: 6.64871207211\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] Loss (name: value) kld: 0.117359622175\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] Loss (name: value) recons: 6.53135248025\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] Loss (name: value) logppx: 6.64871207211\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] #quality_metric: host=algo-2, epoch=40, train total_loss <loss>=6.64871207211\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] patience losses:[6.674894869327545, 6.676518460114797, 6.6724795831574335, 6.663601676623027, 6.655031568474239] min patience loss:6.65503156847 current loss:6.64871207211 absolute loss difference:0.00631949636671\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1440, \"sum\": 1440.0, \"min\": 1440}, \"Total Records Seen\": {\"count\": 1, \"max\": 181080, \"sum\": 181080.0, \"min\": 181080}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1593379091.650084, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1593379090.387348}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3584.70060081 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:11 INFO 140435207169856] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:12.523] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1179, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] # Finished training epoch 42 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] Loss (name: value) total: 6.64312145445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] Loss (name: value) kld: 0.120860351767\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] Loss (name: value) recons: 6.52226112286\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] Loss (name: value) logppx: 6.64312145445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=6.64312145445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] patience losses:[6.66527932882309, 6.662309759192997, 6.6584617429309425, 6.655215508408016, 6.647549615965949] min patience loss:6.64754961597 current loss:6.64312145445 absolute loss difference:0.00442816151513\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1512, \"sum\": 1512.0, \"min\": 1512}, \"Total Records Seen\": {\"count\": 1, \"max\": 190008, \"sum\": 190008.0, \"min\": 190008}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1593379092.527526, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1593379091.343716}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3821.17827343 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:12 INFO 140483679958848] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:12.830] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 1179, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] # Finished training epoch 41 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Loss (name: value) total: 6.65067969428\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Loss (name: value) kld: 0.121524108263\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Loss (name: value) recons: 6.52915555901\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Loss (name: value) logppx: 6.65067969428\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] #quality_metric: host=algo-2, epoch=41, train total_loss <loss>=6.65067969428\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] patience losses:[6.676518460114797, 6.6724795831574335, 6.663601676623027, 6.655031568474239, 6.648712072107527] min patience loss:6.64871207211 current loss:6.65067969428 absolute loss difference:0.00196762217416\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] #progress_metric: host=algo-2, completed 41 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1476, \"sum\": 1476.0, \"min\": 1476}, \"Total Records Seen\": {\"count\": 1, \"max\": 185607, \"sum\": 185607.0, \"min\": 185607}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1593379092.831472, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1593379091.650356}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3832.32572443 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:12 INFO 140435207169856] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:13.639] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1111, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] # Finished training epoch 43 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Loss (name: value) total: 6.64248975118\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Loss (name: value) kld: 0.122743357801\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Loss (name: value) recons: 6.5197464625\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Loss (name: value) logppx: 6.64248975118\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=6.64248975118\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] patience losses:[6.662309759192997, 6.6584617429309425, 6.655215508408016, 6.647549615965949, 6.643121454450819] min patience loss:6.64312145445 current loss:6.64248975118 absolute loss difference:0.000631703270806\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] Timing: train: 1.11s, val: 0.00s, epoch: 1.12s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1548, \"sum\": 1548.0, \"min\": 1548}, \"Total Records Seen\": {\"count\": 1, \"max\": 194532, \"sum\": 194532.0, \"min\": 194532}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1593379093.644516, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1593379092.527753}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4049.64121967 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:13 INFO 140483679958848] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:14.015] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 1183, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] # Finished training epoch 42 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] Loss (name: value) total: 6.64598902729\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] Loss (name: value) kld: 0.123074052752\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] Loss (name: value) recons: 6.52291494608\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] Loss (name: value) logppx: 6.64598902729\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] #quality_metric: host=algo-2, epoch=42, train total_loss <loss>=6.64598902729\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] patience losses:[6.6724795831574335, 6.663601676623027, 6.655031568474239, 6.648712072107527, 6.650679694281684] min patience loss:6.64871207211 current loss:6.64598902729 absolute loss difference:0.0027230448193\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] #progress_metric: host=algo-2, completed 42 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1512, \"sum\": 1512.0, \"min\": 1512}, \"Total Records Seen\": {\"count\": 1, \"max\": 190134, \"sum\": 190134.0, \"min\": 190134}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1593379094.021144, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1593379092.831767}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3804.94705429 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:14 INFO 140435207169856] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:14.786] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1141, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] # Finished training epoch 44 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] Loss (name: value) total: 6.63323944145\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] Loss (name: value) kld: 0.124909119681\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] Loss (name: value) recons: 6.50833038489\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] Loss (name: value) logppx: 6.63323944145\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=6.63323944145\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] patience losses:[6.6584617429309425, 6.655215508408016, 6.647549615965949, 6.643121454450819, 6.642489751180013] min patience loss:6.64248975118 current loss:6.63323944145 absolute loss difference:0.00925030973222\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1584, \"sum\": 1584.0, \"min\": 1584}, \"Total Records Seen\": {\"count\": 1, \"max\": 199056, \"sum\": 199056.0, \"min\": 199056}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1593379094.791772, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1593379093.645165}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3945.0548421 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:14 INFO 140483679958848] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:15.171] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 1149, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] # Finished training epoch 43 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] Loss (name: value) total: 6.64137378004\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] Loss (name: value) kld: 0.127171898364\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] Loss (name: value) recons: 6.51420187288\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] Loss (name: value) logppx: 6.64137378004\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] #quality_metric: host=algo-2, epoch=43, train total_loss <loss>=6.64137378004\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] patience losses:[6.663601676623027, 6.655031568474239, 6.648712072107527, 6.650679694281684, 6.645989027288225] min patience loss:6.64598902729 current loss:6.64137378004 absolute loss difference:0.0046152472496\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] #progress_metric: host=algo-2, completed 43 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1548, \"sum\": 1548.0, \"min\": 1548}, \"Total Records Seen\": {\"count\": 1, \"max\": 194661, \"sum\": 194661.0, \"min\": 194661}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1593379095.175284, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1593379094.021794}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3923.98908585 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:15 INFO 140435207169856] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:16.452] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 1276, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] # Finished training epoch 44 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] Loss (name: value) total: 6.63231838412\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] Loss (name: value) kld: 0.125501136813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] Loss (name: value) recons: 6.506817268\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] Loss (name: value) logppx: 6.63231838412\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] #quality_metric: host=algo-2, epoch=44, train total_loss <loss>=6.63231838412\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] patience losses:[6.655031568474239, 6.648712072107527, 6.650679694281684, 6.645989027288225, 6.641373780038622] min patience loss:6.64137378004 current loss:6.63231838412 absolute loss difference:0.00905539592107\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] Timing: train: 1.28s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] #progress_metric: host=algo-2, completed 44 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1584, \"sum\": 1584.0, \"min\": 1584}, \"Total Records Seen\": {\"count\": 1, \"max\": 199188, \"sum\": 199188.0, \"min\": 199188}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1593379096.456806, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1593379095.175657}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3533.15520967 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:16 INFO 140435207169856] # Starting training for epoch 45\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:18:16.004] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1212, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] # Finished training epoch 45 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] Loss (name: value) total: 6.63044397699\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] Loss (name: value) kld: 0.128240561527\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] Loss (name: value) recons: 6.50220341153\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] Loss (name: value) logppx: 6.63044397699\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=6.63044397699\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] patience losses:[6.655215508408016, 6.647549615965949, 6.643121454450819, 6.642489751180013, 6.633239441447788] min patience loss:6.63323944145 current loss:6.63044397699 absolute loss difference:0.0027954644627\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1620, \"sum\": 1620.0, \"min\": 1620}, \"Total Records Seen\": {\"count\": 1, \"max\": 203580, \"sum\": 203580.0, \"min\": 203580}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1593379096.008216, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1593379094.792029}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3719.36406448 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:16 INFO 140483679958848] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:17.207] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1198, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] # Finished training epoch 46 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] Loss (name: value) total: 6.6271550258\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] Loss (name: value) kld: 0.12995865941\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] Loss (name: value) recons: 6.49719633659\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] Loss (name: value) logppx: 6.6271550258\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=6.6271550258\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] patience losses:[6.647549615965949, 6.643121454450819, 6.642489751180013, 6.633239441447788, 6.630443976985084] min patience loss:6.63044397699 current loss:6.6271550258 absolute loss difference:0.00328895118501\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1656, \"sum\": 1656.0, \"min\": 1656}, \"Total Records Seen\": {\"count\": 1, \"max\": 208104, \"sum\": 208104.0, \"min\": 208104}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1593379097.213861, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1593379096.008503}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3752.80892508 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:17 INFO 140483679958848] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:18.379] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1164, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] # Finished training epoch 47 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] Loss (name: value) total: 6.62420515882\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] Loss (name: value) kld: 0.133432961793\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] Loss (name: value) recons: 6.49077218109\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] Loss (name: value) logppx: 6.62420515882\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=6.62420515882\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] patience losses:[6.643121454450819, 6.642489751180013, 6.633239441447788, 6.630443976985084, 6.627155025800069] min patience loss:6.6271550258 current loss:6.62420515882 absolute loss difference:0.00294986698363\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1692, \"sum\": 1692.0, \"min\": 1692}, \"Total Records Seen\": {\"count\": 1, \"max\": 212628, \"sum\": 212628.0, \"min\": 212628}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1593379098.385458, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1593379097.214131}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3861.79130568 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:18 INFO 140483679958848] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:17.654] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 1197, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] # Finished training epoch 45 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Loss (name: value) total: 6.63376929363\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Loss (name: value) kld: 0.130057273536\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Loss (name: value) recons: 6.50371203158\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Loss (name: value) logppx: 6.63376929363\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] #quality_metric: host=algo-2, epoch=45, train total_loss <loss>=6.63376929363\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] patience losses:[6.648712072107527, 6.650679694281684, 6.645989027288225, 6.641373780038622, 6.63231838411755] min patience loss:6.63231838412 current loss:6.63376929363 absolute loss difference:0.0014509095086\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] #progress_metric: host=algo-2, completed 45 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1620, \"sum\": 1620.0, \"min\": 1620}, \"Total Records Seen\": {\"count\": 1, \"max\": 203715, \"sum\": 203715.0, \"min\": 203715}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1593379097.656128, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1593379096.457099}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3774.88410664 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:17 INFO 140435207169856] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:18.884] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 1227, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] # Finished training epoch 46 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] Loss (name: value) total: 6.62541357676\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] Loss (name: value) kld: 0.134702098142\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] Loss (name: value) recons: 6.4907114572\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] Loss (name: value) logppx: 6.62541357676\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] #quality_metric: host=algo-2, epoch=46, train total_loss <loss>=6.62541357676\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] patience losses:[6.650679694281684, 6.645989027288225, 6.641373780038622, 6.63231838411755, 6.633769293626149] min patience loss:6.63231838412 current loss:6.62541357676 absolute loss difference:0.00690480735567\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] #progress_metric: host=algo-2, completed 46 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1656, \"sum\": 1656.0, \"min\": 1656}, \"Total Records Seen\": {\"count\": 1, \"max\": 208242, \"sum\": 208242.0, \"min\": 208242}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1593379098.890124, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1593379097.656489}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3668.26350676 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:18 INFO 140435207169856] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:19.609] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1222, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] # Finished training epoch 48 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] Loss (name: value) total: 6.62025374174\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] Loss (name: value) kld: 0.134981938327\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] Loss (name: value) recons: 6.48527179162\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] Loss (name: value) logppx: 6.62025374174\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=6.62025374174\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] patience losses:[6.642489751180013, 6.633239441447788, 6.630443976985084, 6.627155025800069, 6.6242051588164435] min patience loss:6.62420515882 current loss:6.62025374174 absolute loss difference:0.00395141707526\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1728, \"sum\": 1728.0, \"min\": 1728}, \"Total Records Seen\": {\"count\": 1, \"max\": 217152, \"sum\": 217152.0, \"min\": 217152}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1593379099.612558, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1593379098.385739}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3687.1920269 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:19 INFO 140483679958848] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:20.072] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] # Finished training epoch 47 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Loss (name: value) total: 6.62826219532\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Loss (name: value) kld: 0.136947281969\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Loss (name: value) recons: 6.49131493436\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Loss (name: value) logppx: 6.62826219532\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] #quality_metric: host=algo-2, epoch=47, train total_loss <loss>=6.62826219532\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] patience losses:[6.645989027288225, 6.641373780038622, 6.63231838411755, 6.633769293626149, 6.625413576761882] min patience loss:6.62541357676 current loss:6.62826219532 absolute loss difference:0.00284861856037\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] #progress_metric: host=algo-2, completed 47 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1692, \"sum\": 1692.0, \"min\": 1692}, \"Total Records Seen\": {\"count\": 1, \"max\": 212769, \"sum\": 212769.0, \"min\": 212769}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1593379100.073599, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1593379098.89092}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3827.29382938 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:20 INFO 140435207169856] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:20.698] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1085, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] # Finished training epoch 49 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] Loss (name: value) total: 6.61725278033\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] Loss (name: value) kld: 0.137916000767\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] Loss (name: value) recons: 6.47933679157\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] Loss (name: value) logppx: 6.61725278033\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=6.61725278033\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] patience losses:[6.633239441447788, 6.630443976985084, 6.627155025800069, 6.6242051588164435, 6.62025374174118] min patience loss:6.62025374174 current loss:6.61725278033 absolute loss difference:0.00300096140967\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] Timing: train: 1.09s, val: 0.00s, epoch: 1.09s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1764, \"sum\": 1764.0, \"min\": 1764}, \"Total Records Seen\": {\"count\": 1, \"max\": 221676, \"sum\": 221676.0, \"min\": 221676}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1593379100.703541, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1593379099.612826}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4147.07283712 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:20 INFO 140483679958848] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:21.279] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 1205, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] # Finished training epoch 48 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] Loss (name: value) total: 6.61700960663\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] Loss (name: value) kld: 0.136965061745\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] Loss (name: value) recons: 6.48004455037\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] Loss (name: value) logppx: 6.61700960663\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] #quality_metric: host=algo-2, epoch=48, train total_loss <loss>=6.61700960663\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] patience losses:[6.641373780038622, 6.63231838411755, 6.633769293626149, 6.625413576761882, 6.6282621953222485] min patience loss:6.62541357676 current loss:6.61700960663 absolute loss difference:0.00840397013558\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] #progress_metric: host=algo-2, completed 48 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1728, \"sum\": 1728.0, \"min\": 1728}, \"Total Records Seen\": {\"count\": 1, \"max\": 217296, \"sum\": 217296.0, \"min\": 217296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1593379101.283902, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1593379100.073847}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3740.68905649 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:21 INFO 140435207169856] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:21.875] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1171, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] # Finished training epoch 50 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] Loss (name: value) total: 6.60356613663\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] Loss (name: value) kld: 0.139781480966\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] Loss (name: value) recons: 6.46378464169\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] Loss (name: value) logppx: 6.60356613663\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=6.60356613663\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] patience losses:[6.630443976985084, 6.627155025800069, 6.6242051588164435, 6.62025374174118, 6.617252780331506] min patience loss:6.61725278033 current loss:6.60356613663 absolute loss difference:0.0136866437064\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Total Records Seen\": {\"count\": 1, \"max\": 226200, \"sum\": 226200.0, \"min\": 226200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1593379101.881132, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1593379100.704212}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3843.5238292 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:21 INFO 140483679958848] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:23.080] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1199, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] # Finished training epoch 51 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Loss (name: value) total: 6.60958518585\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Loss (name: value) kld: 0.143422119319\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Loss (name: value) recons: 6.46616304583\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Loss (name: value) logppx: 6.60958518585\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] #quality_metric: host=algo-1, epoch=51, train total_loss <loss>=6.60958518585\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] patience losses:[6.627155025800069, 6.6242051588164435, 6.62025374174118, 6.617252780331506, 6.603566136625078] min patience loss:6.60356613663 current loss:6.60958518585 absolute loss difference:0.00601904922061\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1836, \"sum\": 1836.0, \"min\": 1836}, \"Total Records Seen\": {\"count\": 1, \"max\": 230724, \"sum\": 230724.0, \"min\": 230724}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 102, \"sum\": 102.0, \"min\": 102}}, \"EndTime\": 1593379103.0822, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 50}, \"StartTime\": 1593379101.881395}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3767.00834637 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:23 INFO 140483679958848] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:22.529] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 1245, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] # Finished training epoch 49 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] Loss (name: value) total: 6.61138682233\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] Loss (name: value) kld: 0.140408032367\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] Loss (name: value) recons: 6.4709788296\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] Loss (name: value) logppx: 6.61138682233\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] #quality_metric: host=algo-2, epoch=49, train total_loss <loss>=6.61138682233\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] patience losses:[6.63231838411755, 6.633769293626149, 6.625413576761882, 6.6282621953222485, 6.617009606626299] min patience loss:6.61700960663 current loss:6.61138682233 absolute loss difference:0.00562278429667\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] #progress_metric: host=algo-2, completed 49 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1764, \"sum\": 1764.0, \"min\": 1764}, \"Total Records Seen\": {\"count\": 1, \"max\": 221823, \"sum\": 221823.0, \"min\": 221823}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1593379102.534378, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1593379101.284202}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3620.63249624 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:22 INFO 140435207169856] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:24.221] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1138, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] # Finished training epoch 52 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] Loss (name: value) total: 6.59841516945\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] Loss (name: value) kld: 0.143054221757\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] Loss (name: value) recons: 6.45536098878\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] Loss (name: value) logppx: 6.59841516945\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] #quality_metric: host=algo-1, epoch=52, train total_loss <loss>=6.59841516945\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] patience losses:[6.6242051588164435, 6.62025374174118, 6.617252780331506, 6.603566136625078, 6.609585185845693] min patience loss:6.60356613663 current loss:6.59841516945 absolute loss difference:0.00515096717411\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1872, \"sum\": 1872.0, \"min\": 1872}, \"Total Records Seen\": {\"count\": 1, \"max\": 235248, \"sum\": 235248.0, \"min\": 235248}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}}, \"EndTime\": 1593379104.226495, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 51}, \"StartTime\": 1593379103.082491}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3953.97173899 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:24 INFO 140483679958848] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:23.707] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 1171, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] # Finished training epoch 50 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] Loss (name: value) total: 6.60474093093\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] Loss (name: value) kld: 0.1421975005\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] Loss (name: value) recons: 6.46254346768\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] Loss (name: value) logppx: 6.60474093093\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] #quality_metric: host=algo-2, epoch=50, train total_loss <loss>=6.60474093093\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] patience losses:[6.633769293626149, 6.625413576761882, 6.6282621953222485, 6.617009606626299, 6.611386822329627] min patience loss:6.61138682233 current loss:6.60474093093 absolute loss difference:0.0066458914015\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] #progress_metric: host=algo-2, completed 50 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1800, \"sum\": 1800.0, \"min\": 1800}, \"Total Records Seen\": {\"count\": 1, \"max\": 226350, \"sum\": 226350.0, \"min\": 226350}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1593379103.712195, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1593379102.534676}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3844.11583195 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:23 INFO 140435207169856] # Starting training for epoch 51\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:25.380] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1153, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] # Finished training epoch 53 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Loss (name: value) total: 6.59910141759\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Loss (name: value) kld: 0.146561732516\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Loss (name: value) recons: 6.45253966914\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Loss (name: value) logppx: 6.59910141759\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] #quality_metric: host=algo-1, epoch=53, train total_loss <loss>=6.59910141759\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] patience losses:[6.62025374174118, 6.617252780331506, 6.603566136625078, 6.609585185845693, 6.598415169450972] min patience loss:6.59841516945 current loss:6.59910141759 absolute loss difference:0.000686248143515\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1908, \"sum\": 1908.0, \"min\": 1908}, \"Total Records Seen\": {\"count\": 1, \"max\": 239772, \"sum\": 239772.0, \"min\": 239772}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 106, \"sum\": 106.0, \"min\": 106}}, \"EndTime\": 1593379105.381631, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 52}, \"StartTime\": 1593379104.226799}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3916.96811191 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:25 INFO 140483679958848] # Starting training for epoch 54\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:24.894] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 1181, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] # Finished training epoch 51 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] Loss (name: value) total: 6.59932563702\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] Loss (name: value) kld: 0.143624808122\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] Loss (name: value) recons: 6.4557008081\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] Loss (name: value) logppx: 6.59932563702\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] #quality_metric: host=algo-2, epoch=51, train total_loss <loss>=6.59932563702\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] patience losses:[6.625413576761882, 6.6282621953222485, 6.617009606626299, 6.611386822329627, 6.604740930928124] min patience loss:6.60474093093 current loss:6.59932563702 absolute loss difference:0.00541529390547\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] #progress_metric: host=algo-2, completed 51 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1836, \"sum\": 1836.0, \"min\": 1836}, \"Total Records Seen\": {\"count\": 1, \"max\": 230877, \"sum\": 230877.0, \"min\": 230877}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 102, \"sum\": 102.0, \"min\": 102}}, \"EndTime\": 1593379104.899041, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 50}, \"StartTime\": 1593379103.712418}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3814.55866058 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:24 INFO 140435207169856] # Starting training for epoch 52\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:26.147] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 155, \"duration\": 1247, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] # Finished training epoch 52 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] Loss (name: value) total: 6.59653192096\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] Loss (name: value) kld: 0.146607337623\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] Loss (name: value) recons: 6.44992457496\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] Loss (name: value) logppx: 6.59653192096\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] #quality_metric: host=algo-2, epoch=52, train total_loss <loss>=6.59653192096\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] patience losses:[6.6282621953222485, 6.617009606626299, 6.611386822329627, 6.604740930928124, 6.5993256370226545] min patience loss:6.59932563702 current loss:6.59653192096 absolute loss difference:0.00279371605979\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] Timing: train: 1.25s, val: 0.00s, epoch: 1.25s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] #progress_metric: host=algo-2, completed 52 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1872, \"sum\": 1872.0, \"min\": 1872}, \"Total Records Seen\": {\"count\": 1, \"max\": 235404, \"sum\": 235404.0, \"min\": 235404}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 104, \"sum\": 104.0, \"min\": 104}}, \"EndTime\": 1593379106.151186, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 51}, \"StartTime\": 1593379104.899342}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3615.84334633 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:26 INFO 140435207169856] # Starting training for epoch 53\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:27.377] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 1221, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] # Finished training epoch 53 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Loss (name: value) total: 6.60355505678\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Loss (name: value) kld: 0.150886095957\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Loss (name: value) recons: 6.45266897811\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Loss (name: value) logppx: 6.60355505678\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] #quality_metric: host=algo-2, epoch=53, train total_loss <loss>=6.60355505678\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] patience losses:[6.617009606626299, 6.611386822329627, 6.604740930928124, 6.5993256370226545, 6.596531920962864] min patience loss:6.59653192096 current loss:6.60355505678 absolute loss difference:0.00702313582102\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] #progress_metric: host=algo-2, completed 53 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1908, \"sum\": 1908.0, \"min\": 1908}, \"Total Records Seen\": {\"count\": 1, \"max\": 239931, \"sum\": 239931.0, \"min\": 239931}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 106, \"sum\": 106.0, \"min\": 106}}, \"EndTime\": 1593379107.378554, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 52}, \"StartTime\": 1593379106.15287}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3692.94815161 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:27 INFO 140435207169856] # Starting training for epoch 54\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:18:26.536] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1154, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] # Finished training epoch 54 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] Loss (name: value) total: 6.5937852727\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] Loss (name: value) kld: 0.148663871301\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] Loss (name: value) recons: 6.44512144062\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] Loss (name: value) logppx: 6.5937852727\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] #quality_metric: host=algo-1, epoch=54, train total_loss <loss>=6.5937852727\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] patience losses:[6.617252780331506, 6.603566136625078, 6.609585185845693, 6.598415169450972, 6.599101417594486] min patience loss:6.59841516945 current loss:6.5937852727 absolute loss difference:0.00462989674674\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1944, \"sum\": 1944.0, \"min\": 1944}, \"Total Records Seen\": {\"count\": 1, \"max\": 244296, \"sum\": 244296.0, \"min\": 244296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}}, \"EndTime\": 1593379106.542477, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 53}, \"StartTime\": 1593379105.381904}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3897.48757705 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:26 INFO 140483679958848] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:27.682] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1139, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] # Finished training epoch 55 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] Loss (name: value) total: 6.58999559614\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] Loss (name: value) kld: 0.150777325862\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] Loss (name: value) recons: 6.43921824296\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] Loss (name: value) logppx: 6.58999559614\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] #quality_metric: host=algo-1, epoch=55, train total_loss <loss>=6.58999559614\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] patience losses:[6.603566136625078, 6.609585185845693, 6.598415169450972, 6.599101417594486, 6.59378527270423] min patience loss:6.5937852727 current loss:6.58999559614 absolute loss difference:0.0037896765603\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1980, \"sum\": 1980.0, \"min\": 1980}, \"Total Records Seen\": {\"count\": 1, \"max\": 248820, \"sum\": 248820.0, \"min\": 248820}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 110, \"sum\": 110.0, \"min\": 110}}, \"EndTime\": 1593379107.68829, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 54}, \"StartTime\": 1593379106.542793}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3948.82236759 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:27 INFO 140483679958848] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:28.848] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1159, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] # Finished training epoch 56 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] Loss (name: value) total: 6.58660575416\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] Loss (name: value) kld: 0.152971083919\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] Loss (name: value) recons: 6.4336346719\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] Loss (name: value) logppx: 6.58660575416\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] #quality_metric: host=algo-1, epoch=56, train total_loss <loss>=6.58660575416\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] patience losses:[6.609585185845693, 6.598415169450972, 6.599101417594486, 6.59378527270423, 6.589995596143934] min patience loss:6.58999559614 current loss:6.58660575416 absolute loss difference:0.0033898419804\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2016, \"sum\": 2016.0, \"min\": 2016}, \"Total Records Seen\": {\"count\": 1, \"max\": 253344, \"sum\": 253344.0, \"min\": 253344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}}, \"EndTime\": 1593379108.854644, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 55}, \"StartTime\": 1593379107.688589}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3879.21628634 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:28 INFO 140483679958848] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:28.573] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 161, \"duration\": 1194, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] # Finished training epoch 54 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] Loss (name: value) total: 6.59336253007\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] Loss (name: value) kld: 0.151454811812\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] Loss (name: value) recons: 6.44190773699\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] Loss (name: value) logppx: 6.59336253007\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] #quality_metric: host=algo-2, epoch=54, train total_loss <loss>=6.59336253007\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] patience losses:[6.611386822329627, 6.604740930928124, 6.5993256370226545, 6.596531920962864, 6.603555056783888] min patience loss:6.59653192096 current loss:6.59336253007 absolute loss difference:0.00316939089033\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] #progress_metric: host=algo-2, completed 54 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1944, \"sum\": 1944.0, \"min\": 1944}, \"Total Records Seen\": {\"count\": 1, \"max\": 244458, \"sum\": 244458.0, \"min\": 244458}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 108, \"sum\": 108.0, \"min\": 108}}, \"EndTime\": 1593379108.578448, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 53}, \"StartTime\": 1593379107.378864}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3773.0973188 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:28 INFO 140435207169856] # Starting training for epoch 55\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:30.058] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] # Finished training epoch 57 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] Loss (name: value) total: 6.58190516631\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] Loss (name: value) kld: 0.155533923871\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] Loss (name: value) recons: 6.42637126976\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] Loss (name: value) logppx: 6.58190516631\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] #quality_metric: host=algo-1, epoch=57, train total_loss <loss>=6.58190516631\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] patience losses:[6.598415169450972, 6.599101417594486, 6.59378527270423, 6.589995596143934, 6.58660575416353] min patience loss:6.58660575416 current loss:6.58190516631 absolute loss difference:0.00470058785544\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2052, \"sum\": 2052.0, \"min\": 2052}, \"Total Records Seen\": {\"count\": 1, \"max\": 257868, \"sum\": 257868.0, \"min\": 257868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 114, \"sum\": 114.0, \"min\": 114}}, \"EndTime\": 1593379110.063053, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 56}, \"StartTime\": 1593379108.854944}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3744.18346173 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:30 INFO 140483679958848] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:29.780] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 1201, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] # Finished training epoch 55 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] Loss (name: value) total: 6.59149774578\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] Loss (name: value) kld: 0.154861551709\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] Loss (name: value) recons: 6.43663621611\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] Loss (name: value) logppx: 6.59149774578\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] #quality_metric: host=algo-2, epoch=55, train total_loss <loss>=6.59149774578\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] patience losses:[6.604740930928124, 6.5993256370226545, 6.596531920962864, 6.603555056783888, 6.59336253007253] min patience loss:6.59336253007 current loss:6.59149774578 absolute loss difference:0.0018647842937\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] #progress_metric: host=algo-2, completed 55 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1980, \"sum\": 1980.0, \"min\": 1980}, \"Total Records Seen\": {\"count\": 1, \"max\": 248985, \"sum\": 248985.0, \"min\": 248985}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 110, \"sum\": 110.0, \"min\": 110}}, \"EndTime\": 1593379109.784745, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 54}, \"StartTime\": 1593379108.578877}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3753.65017832 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:29 INFO 140435207169856] # Starting training for epoch 56\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:31.264] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1200, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] # Finished training epoch 58 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] Loss (name: value) total: 6.57758269707\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] Loss (name: value) kld: 0.158043007987\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] Loss (name: value) recons: 6.41953967677\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] Loss (name: value) logppx: 6.57758269707\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] #quality_metric: host=algo-1, epoch=58, train total_loss <loss>=6.57758269707\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] patience losses:[6.599101417594486, 6.59378527270423, 6.589995596143934, 6.58660575416353, 6.581905166308085] min patience loss:6.58190516631 current loss:6.57758269707 absolute loss difference:0.00432246923447\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2088, \"sum\": 2088.0, \"min\": 2088}, \"Total Records Seen\": {\"count\": 1, \"max\": 262392, \"sum\": 262392.0, \"min\": 262392}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 116, \"sum\": 116.0, \"min\": 116}}, \"EndTime\": 1593379111.268205, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 57}, \"StartTime\": 1593379110.06331}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3754.20703879 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:31 INFO 140483679958848] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:30.994] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 167, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] # Finished training epoch 56 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] Loss (name: value) total: 6.58190120591\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] Loss (name: value) kld: 0.155989270761\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] Loss (name: value) recons: 6.42591189014\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] Loss (name: value) logppx: 6.58190120591\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] #quality_metric: host=algo-2, epoch=56, train total_loss <loss>=6.58190120591\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] patience losses:[6.5993256370226545, 6.596531920962864, 6.603555056783888, 6.59336253007253, 6.591497745778826] min patience loss:6.59149774578 current loss:6.58190120591 absolute loss difference:0.00959653986825\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:30 INFO 140435207169856] #progress_metric: host=algo-2, completed 56 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2016, \"sum\": 2016.0, \"min\": 2016}, \"Total Records Seen\": {\"count\": 1, \"max\": 253512, \"sum\": 253512.0, \"min\": 253512}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 112, \"sum\": 112.0, \"min\": 112}}, \"EndTime\": 1593379110.999774, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 55}, \"StartTime\": 1593379109.785044}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:31 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3726.25376681 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:31 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:31 INFO 140435207169856] # Starting training for epoch 57\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:32.412] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1144, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] # Finished training epoch 59 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] Loss (name: value) total: 6.57647271951\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] Loss (name: value) kld: 0.161632571887\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] Loss (name: value) recons: 6.41484006246\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] Loss (name: value) logppx: 6.57647271951\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] #quality_metric: host=algo-1, epoch=59, train total_loss <loss>=6.57647271951\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:32.262] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 1261, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] # Finished training epoch 57 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] Loss (name: value) total: 6.5804083546\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] Loss (name: value) kld: 0.158780001311\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] Loss (name: value) recons: 6.42162835598\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] Loss (name: value) logppx: 6.5804083546\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] #quality_metric: host=algo-2, epoch=57, train total_loss <loss>=6.5804083546\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] patience losses:[6.596531920962864, 6.603555056783888, 6.59336253007253, 6.591497745778826, 6.581901205910577] min patience loss:6.58190120591 current loss:6.5804083546 absolute loss difference:0.00149285131031\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] Timing: train: 1.26s, val: 0.00s, epoch: 1.27s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] #progress_metric: host=algo-2, completed 57 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2052, \"sum\": 2052.0, \"min\": 2052}, \"Total Records Seen\": {\"count\": 1, \"max\": 258039, \"sum\": 258039.0, \"min\": 258039}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 114, \"sum\": 114.0, \"min\": 114}}, \"EndTime\": 1593379112.267604, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 56}, \"StartTime\": 1593379111.000301}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3571.72751462 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:32 INFO 140435207169856] # Starting training for epoch 58\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] patience losses:[6.59378527270423, 6.589995596143934, 6.58660575416353, 6.581905166308085, 6.577582697073619] min patience loss:6.57758269707 current loss:6.57647271951 absolute loss difference:0.00110997756322\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2124, \"sum\": 2124.0, \"min\": 2124}, \"Total Records Seen\": {\"count\": 1, \"max\": 266916, \"sum\": 266916.0, \"min\": 266916}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}}, \"EndTime\": 1593379112.419504, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 58}, \"StartTime\": 1593379111.268507}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3930.00751951 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:32 INFO 140483679958848] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:33.453] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 173, \"duration\": 1183, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] # Finished training epoch 58 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] Loss (name: value) total: 6.5791991353\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] Loss (name: value) kld: 0.161066748202\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] Loss (name: value) recons: 6.418132378\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] Loss (name: value) logppx: 6.5791991353\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] #quality_metric: host=algo-2, epoch=58, train total_loss <loss>=6.5791991353\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] patience losses:[6.603555056783888, 6.59336253007253, 6.591497745778826, 6.581901205910577, 6.58040835460027] min patience loss:6.5804083546 current loss:6.5791991353 absolute loss difference:0.00120921929677\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] #progress_metric: host=algo-2, completed 58 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2088, \"sum\": 2088.0, \"min\": 2088}, \"Total Records Seen\": {\"count\": 1, \"max\": 262566, \"sum\": 262566.0, \"min\": 262566}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 116, \"sum\": 116.0, \"min\": 116}}, \"EndTime\": 1593379113.459953, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 57}, \"StartTime\": 1593379112.267903}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3796.19417364 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:33 INFO 140435207169856] # Starting training for epoch 59\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:33.572] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1152, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] # Finished training epoch 60 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] Loss (name: value) total: 6.57310136822\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] Loss (name: value) kld: 0.160434340437\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] Loss (name: value) recons: 6.41266701619\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] Loss (name: value) logppx: 6.57310136822\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] #quality_metric: host=algo-1, epoch=60, train total_loss <loss>=6.57310136822\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] patience losses:[6.589995596143934, 6.58660575416353, 6.581905166308085, 6.577582697073619, 6.576472719510396] min patience loss:6.57647271951 current loss:6.57310136822 absolute loss difference:0.00337135129505\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2160, \"sum\": 2160.0, \"min\": 2160}, \"Total Records Seen\": {\"count\": 1, \"max\": 271440, \"sum\": 271440.0, \"min\": 271440}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 120, \"sum\": 120.0, \"min\": 120}}, \"EndTime\": 1593379113.576445, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 59}, \"StartTime\": 1593379112.419824}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3910.8970905 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:33 INFO 140483679958848] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:34.788] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1211, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] # Finished training epoch 61 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] Loss (name: value) total: 6.57052901718\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] Loss (name: value) kld: 0.164096825032\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] Loss (name: value) recons: 6.40643219815\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] Loss (name: value) logppx: 6.57052901718\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] #quality_metric: host=algo-1, epoch=61, train total_loss <loss>=6.57052901718\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] patience losses:[6.58660575416353, 6.581905166308085, 6.577582697073619, 6.576472719510396, 6.573101368215349] min patience loss:6.57310136822 current loss:6.57052901718 absolute loss difference:0.00257235103183\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2196, \"sum\": 2196.0, \"min\": 2196}, \"Total Records Seen\": {\"count\": 1, \"max\": 275964, \"sum\": 275964.0, \"min\": 275964}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 122, \"sum\": 122.0, \"min\": 122}}, \"EndTime\": 1593379114.793104, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 60}, \"StartTime\": 1593379113.576709}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3718.78310564 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:34 INFO 140483679958848] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:35.900] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1106, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] # Finished training epoch 62 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] Loss (name: value) total: 6.56758606434\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] Loss (name: value) kld: 0.164912258585\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] Loss (name: value) recons: 6.40267380741\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] Loss (name: value) logppx: 6.56758606434\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] #quality_metric: host=algo-1, epoch=62, train total_loss <loss>=6.56758606434\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] patience losses:[6.581905166308085, 6.577582697073619, 6.576472719510396, 6.573101368215349, 6.570529017183516] min patience loss:6.57052901718 current loss:6.56758606434 absolute loss difference:0.00294295284483\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] Timing: train: 1.11s, val: 0.00s, epoch: 1.11s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2232, \"sum\": 2232.0, \"min\": 2232}, \"Total Records Seen\": {\"count\": 1, \"max\": 280488, \"sum\": 280488.0, \"min\": 280488}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}}, \"EndTime\": 1593379115.905262, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 61}, \"StartTime\": 1593379114.793361}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4068.18487345 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:35 INFO 140483679958848] # Starting training for epoch 63\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m[2020-06-28 21:18:34.657] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 1193, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] # Finished training epoch 59 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Loss (name: value) total: 6.58031282822\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Loss (name: value) kld: 0.163108282619\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Loss (name: value) recons: 6.41720456547\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Loss (name: value) logppx: 6.58031282822\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] #quality_metric: host=algo-2, epoch=59, train total_loss <loss>=6.58031282822\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] patience losses:[6.59336253007253, 6.591497745778826, 6.581901205910577, 6.58040835460027, 6.579199135303497] min patience loss:6.5791991353 current loss:6.58031282822 absolute loss difference:0.00111369291941\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] #progress_metric: host=algo-2, completed 59 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2124, \"sum\": 2124.0, \"min\": 2124}, \"Total Records Seen\": {\"count\": 1, \"max\": 267093, \"sum\": 267093.0, \"min\": 267093}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 118, \"sum\": 118.0, \"min\": 118}}, \"EndTime\": 1593379114.658802, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 58}, \"StartTime\": 1593379113.46092}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3778.7409874 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:34 INFO 140435207169856] # Starting training for epoch 60\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:35.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 179, \"duration\": 1164, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] # Finished training epoch 60 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] Loss (name: value) total: 6.57674194045\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] Loss (name: value) kld: 0.166169922385\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] Loss (name: value) recons: 6.41057202551\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] Loss (name: value) logppx: 6.57674194045\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] #quality_metric: host=algo-2, epoch=60, train total_loss <loss>=6.57674194045\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] patience losses:[6.591497745778826, 6.581901205910577, 6.58040835460027, 6.579199135303497, 6.580312828222911] min patience loss:6.5791991353 current loss:6.57674194045 absolute loss difference:0.00245719485813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] #progress_metric: host=algo-2, completed 60 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2160, \"sum\": 2160.0, \"min\": 2160}, \"Total Records Seen\": {\"count\": 1, \"max\": 271620, \"sum\": 271620.0, \"min\": 271620}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 120, \"sum\": 120.0, \"min\": 120}}, \"EndTime\": 1593379115.82844, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 59}, \"StartTime\": 1593379114.659032}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3870.68655894 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:35 INFO 140435207169856] # Starting training for epoch 61\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:37.122] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] # Finished training epoch 63 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] Loss (name: value) total: 6.56392908096\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] Loss (name: value) kld: 0.168404798955\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] Loss (name: value) recons: 6.39552427\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] Loss (name: value) logppx: 6.56392908096\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] #quality_metric: host=algo-1, epoch=63, train total_loss <loss>=6.56392908096\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] patience losses:[6.577582697073619, 6.576472719510396, 6.573101368215349, 6.570529017183516, 6.567586064338684] min patience loss:6.56758606434 current loss:6.56392908096 absolute loss difference:0.00365698337555\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2268, \"sum\": 2268.0, \"min\": 2268}, \"Total Records Seen\": {\"count\": 1, \"max\": 285012, \"sum\": 285012.0, \"min\": 285012}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 126, \"sum\": 126.0, \"min\": 126}}, \"EndTime\": 1593379117.128681, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 62}, \"StartTime\": 1593379115.905559}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3698.26338725 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:37 INFO 140483679958848] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:37.083] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 1253, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] # Finished training epoch 61 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] Loss (name: value) total: 6.57130359279\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] Loss (name: value) kld: 0.167864223528\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] Loss (name: value) recons: 6.40343938933\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] Loss (name: value) logppx: 6.57130359279\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] #quality_metric: host=algo-2, epoch=61, train total_loss <loss>=6.57130359279\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] patience losses:[6.581901205910577, 6.58040835460027, 6.579199135303497, 6.580312828222911, 6.57674194044537] min patience loss:6.57674194045 current loss:6.57130359279 absolute loss difference:0.00543834765752\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] #progress_metric: host=algo-2, completed 61 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2196, \"sum\": 2196.0, \"min\": 2196}, \"Total Records Seen\": {\"count\": 1, \"max\": 276147, \"sum\": 276147.0, \"min\": 276147}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 122, \"sum\": 122.0, \"min\": 122}}, \"EndTime\": 1593379117.08948, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 60}, \"StartTime\": 1593379115.82908}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3590.93627755 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:37 INFO 140435207169856] # Starting training for epoch 62\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:38.340] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1208, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] # Finished training epoch 64 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] Loss (name: value) total: 6.56275151836\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] Loss (name: value) kld: 0.171362121486\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] Loss (name: value) recons: 6.39138937659\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] Loss (name: value) logppx: 6.56275151836\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] #quality_metric: host=algo-1, epoch=64, train total_loss <loss>=6.56275151836\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] patience losses:[6.576472719510396, 6.573101368215349, 6.570529017183516, 6.567586064338684, 6.563929080963135] min patience loss:6.56392908096 current loss:6.56275151836 absolute loss difference:0.00117756260766\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2304, \"sum\": 2304.0, \"min\": 2304}, \"Total Records Seen\": {\"count\": 1, \"max\": 289536, \"sum\": 289536.0, \"min\": 289536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}}, \"EndTime\": 1593379118.345868, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 63}, \"StartTime\": 1593379117.12897}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3717.22117576 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:38 INFO 140483679958848] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:38.298] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 185, \"duration\": 1208, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] # Finished training epoch 62 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] Loss (name: value) total: 6.56509913339\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] Loss (name: value) kld: 0.17023980483\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] Loss (name: value) recons: 6.39485930072\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] Loss (name: value) logppx: 6.56509913339\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] #quality_metric: host=algo-2, epoch=62, train total_loss <loss>=6.56509913339\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] patience losses:[6.58040835460027, 6.579199135303497, 6.580312828222911, 6.57674194044537, 6.5713035927878485] min patience loss:6.57130359279 current loss:6.56509913339 absolute loss difference:0.0062044594023\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] #progress_metric: host=algo-2, completed 62 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2232, \"sum\": 2232.0, \"min\": 2232}, \"Total Records Seen\": {\"count\": 1, \"max\": 280674, \"sum\": 280674.0, \"min\": 280674}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}}, \"EndTime\": 1593379118.304279, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 61}, \"StartTime\": 1593379117.089837}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3726.92299394 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:38 INFO 140435207169856] # Starting training for epoch 63\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:39.523] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 1218, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] # Finished training epoch 63 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] Loss (name: value) total: 6.55960908863\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] Loss (name: value) kld: 0.170799381824\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] Loss (name: value) recons: 6.38880973392\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] Loss (name: value) logppx: 6.55960908863\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] #quality_metric: host=algo-2, epoch=63, train total_loss <loss>=6.55960908863\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:39.563] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1216, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] # Finished training epoch 65 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] Loss (name: value) total: 6.55849908458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] Loss (name: value) kld: 0.171075645731\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] Loss (name: value) recons: 6.38742342922\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] Loss (name: value) logppx: 6.55849908458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] #quality_metric: host=algo-1, epoch=65, train total_loss <loss>=6.55849908458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] patience losses:[6.573101368215349, 6.570529017183516, 6.567586064338684, 6.563929080963135, 6.562751518355475] min patience loss:6.56275151836 current loss:6.55849908458 absolute loss difference:0.00425243377686\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2340, \"sum\": 2340.0, \"min\": 2340}, \"Total Records Seen\": {\"count\": 1, \"max\": 294060, \"sum\": 294060.0, \"min\": 294060}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 130, \"sum\": 130.0, \"min\": 130}}, \"EndTime\": 1593379119.568306, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 64}, \"StartTime\": 1593379118.346294}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3701.60080481 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:39 INFO 140483679958848] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] patience losses:[6.579199135303497, 6.580312828222911, 6.57674194044537, 6.5713035927878485, 6.565099133385552] min patience loss:6.56509913339 current loss:6.55960908863 absolute loss difference:0.00549004475276\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] #progress_metric: host=algo-2, completed 63 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2268, \"sum\": 2268.0, \"min\": 2268}, \"Total Records Seen\": {\"count\": 1, \"max\": 285201, \"sum\": 285201.0, \"min\": 285201}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 126, \"sum\": 126.0, \"min\": 126}}, \"EndTime\": 1593379119.528895, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 62}, \"StartTime\": 1593379118.304689}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3696.76240674 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:39 INFO 140435207169856] # Starting training for epoch 64\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:40.740] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1171, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] # Finished training epoch 66 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] Loss (name: value) total: 6.55311158631\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] Loss (name: value) kld: 0.176283214138\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] Loss (name: value) recons: 6.37682839235\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] Loss (name: value) logppx: 6.55311158631\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] #quality_metric: host=algo-1, epoch=66, train total_loss <loss>=6.55311158631\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] patience losses:[6.570529017183516, 6.567586064338684, 6.563929080963135, 6.562751518355475, 6.55849908457862] min patience loss:6.55849908458 current loss:6.55311158631 absolute loss difference:0.00538749827279\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] Timing: train: 1.17s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2376, \"sum\": 2376.0, \"min\": 2376}, \"Total Records Seen\": {\"count\": 1, \"max\": 298584, \"sum\": 298584.0, \"min\": 298584}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 132, \"sum\": 132.0, \"min\": 132}}, \"EndTime\": 1593379120.744425, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 65}, \"StartTime\": 1593379119.568607}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3847.07955869 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:40 INFO 140483679958848] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:40.736] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 191, \"duration\": 1207, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] # Finished training epoch 64 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Loss (name: value) total: 6.56524967485\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Loss (name: value) kld: 0.175218408927\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Loss (name: value) recons: 6.39003127813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Loss (name: value) logppx: 6.56524967485\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] #quality_metric: host=algo-2, epoch=64, train total_loss <loss>=6.56524967485\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] patience losses:[6.580312828222911, 6.57674194044537, 6.5713035927878485, 6.565099133385552, 6.559609088632795] min patience loss:6.55960908863 current loss:6.56524967485 absolute loss difference:0.00564058621725\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] #progress_metric: host=algo-2, completed 64 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2304, \"sum\": 2304.0, \"min\": 2304}, \"Total Records Seen\": {\"count\": 1, \"max\": 289728, \"sum\": 289728.0, \"min\": 289728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 128, \"sum\": 128.0, \"min\": 128}}, \"EndTime\": 1593379120.738329, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 63}, \"StartTime\": 1593379119.52942}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3744.14407715 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:40 INFO 140435207169856] # Starting training for epoch 65\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:41.950] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1205, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] # Finished training epoch 67 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] Loss (name: value) total: 6.55075249407\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] Loss (name: value) kld: 0.176073074962\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] Loss (name: value) recons: 6.37467937999\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] Loss (name: value) logppx: 6.55075249407\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] #quality_metric: host=algo-1, epoch=67, train total_loss <loss>=6.55075249407\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] patience losses:[6.567586064338684, 6.563929080963135, 6.562751518355475, 6.55849908457862, 6.55311158630583] min patience loss:6.55311158631 current loss:6.55075249407 absolute loss difference:0.00235909223557\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2412, \"sum\": 2412.0, \"min\": 2412}, \"Total Records Seen\": {\"count\": 1, \"max\": 303108, \"sum\": 303108.0, \"min\": 303108}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 134, \"sum\": 134.0, \"min\": 134}}, \"EndTime\": 1593379121.954472, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 66}, \"StartTime\": 1593379120.744661}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3739.07198574 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:41 INFO 140483679958848] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:41.968] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 1229, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] # Finished training epoch 65 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] Loss (name: value) total: 6.54881564776\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] Loss (name: value) kld: 0.173644177409\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] Loss (name: value) recons: 6.37517146269\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] Loss (name: value) logppx: 6.54881564776\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] #quality_metric: host=algo-2, epoch=65, train total_loss <loss>=6.54881564776\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] patience losses:[6.57674194044537, 6.5713035927878485, 6.565099133385552, 6.559609088632795, 6.56524967485004] min patience loss:6.55960908863 current loss:6.54881564776 absolute loss difference:0.0107934408718\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] #progress_metric: host=algo-2, completed 65 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2340, \"sum\": 2340.0, \"min\": 2340}, \"Total Records Seen\": {\"count\": 1, \"max\": 294255, \"sum\": 294255.0, \"min\": 294255}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 130, \"sum\": 130.0, \"min\": 130}}, \"EndTime\": 1593379121.972687, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 64}, \"StartTime\": 1593379120.738647}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3667.96871925 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:41 INFO 140435207169856] # Starting training for epoch 66\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:43.132] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1177, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] # Finished training epoch 68 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] Loss (name: value) total: 6.54536405537\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] Loss (name: value) kld: 0.177455870021\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] Loss (name: value) recons: 6.36790825261\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] Loss (name: value) logppx: 6.54536405537\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] #quality_metric: host=algo-1, epoch=68, train total_loss <loss>=6.54536405537\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] patience losses:[6.563929080963135, 6.562751518355475, 6.55849908457862, 6.55311158630583, 6.550752494070265] min patience loss:6.55075249407 current loss:6.54536405537 absolute loss difference:0.00538843870163\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2448, \"sum\": 2448.0, \"min\": 2448}, \"Total Records Seen\": {\"count\": 1, \"max\": 307632, \"sum\": 307632.0, \"min\": 307632}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 136, \"sum\": 136.0, \"min\": 136}}, \"EndTime\": 1593379123.136989, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 67}, \"StartTime\": 1593379121.954728}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3826.10946829 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:43 INFO 140483679958848] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:43.203] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 197, \"duration\": 1230, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] # Finished training epoch 66 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Loss (name: value) total: 6.55364845196\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Loss (name: value) kld: 0.178897876706\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Loss (name: value) recons: 6.37475054794\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Loss (name: value) logppx: 6.55364845196\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] #quality_metric: host=algo-2, epoch=66, train total_loss <loss>=6.55364845196\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] patience losses:[6.5713035927878485, 6.565099133385552, 6.559609088632795, 6.56524967485004, 6.548815647761027] min patience loss:6.54881564776 current loss:6.55364845196 absolute loss difference:0.00483280420303\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] #progress_metric: host=algo-2, completed 66 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2376, \"sum\": 2376.0, \"min\": 2376}, \"Total Records Seen\": {\"count\": 1, \"max\": 298782, \"sum\": 298782.0, \"min\": 298782}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 132, \"sum\": 132.0, \"min\": 132}}, \"EndTime\": 1593379123.205052, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 65}, \"StartTime\": 1593379121.973018}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3673.93106576 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:43 INFO 140435207169856] # Starting training for epoch 67\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:44.358] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] # Finished training epoch 69 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Loss (name: value) total: 6.54817603694\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Loss (name: value) kld: 0.180201095425\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Loss (name: value) recons: 6.36797495021\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Loss (name: value) logppx: 6.54817603694\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] #quality_metric: host=algo-1, epoch=69, train total_loss <loss>=6.54817603694\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] patience losses:[6.562751518355475, 6.55849908457862, 6.55311158630583, 6.550752494070265, 6.545364055368635] min patience loss:6.54536405537 current loss:6.54817603694 absolute loss difference:0.00281198157205\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2484, \"sum\": 2484.0, \"min\": 2484}, \"Total Records Seen\": {\"count\": 1, \"max\": 312156, \"sum\": 312156.0, \"min\": 312156}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 138, \"sum\": 138.0, \"min\": 138}}, \"EndTime\": 1593379124.359911, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 68}, \"StartTime\": 1593379123.137257}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3699.68751019 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:44 INFO 140483679958848] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:44.424] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 1219, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] # Finished training epoch 67 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Loss (name: value) total: 6.55511966017\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Loss (name: value) kld: 0.17929425877\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Loss (name: value) recons: 6.37582543161\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Loss (name: value) logppx: 6.55511966017\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] #quality_metric: host=algo-2, epoch=67, train total_loss <loss>=6.55511966017\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] patience losses:[6.565099133385552, 6.559609088632795, 6.56524967485004, 6.548815647761027, 6.553648451964061] min patience loss:6.54881564776 current loss:6.55511966017 absolute loss difference:0.00630401240455\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] #progress_metric: host=algo-2, completed 67 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2412, \"sum\": 2412.0, \"min\": 2412}, \"Total Records Seen\": {\"count\": 1, \"max\": 303309, \"sum\": 303309.0, \"min\": 303309}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 134, \"sum\": 134.0, \"min\": 134}}, \"EndTime\": 1593379124.426373, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 66}, \"StartTime\": 1593379123.205282}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3706.90604067 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:44 INFO 140435207169856] # Starting training for epoch 68\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:45.498] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1137, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] # Finished training epoch 70 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] Loss (name: value) total: 6.54433588849\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] Loss (name: value) kld: 0.184044568489\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] Loss (name: value) recons: 6.36029131545\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] Loss (name: value) logppx: 6.54433588849\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] #quality_metric: host=algo-1, epoch=70, train total_loss <loss>=6.54433588849\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] patience losses:[6.55849908457862, 6.55311158630583, 6.550752494070265, 6.545364055368635, 6.5481760369406805] min patience loss:6.54536405537 current loss:6.54433588849 absolute loss difference:0.0010281668769\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2520, \"sum\": 2520.0, \"min\": 2520}, \"Total Records Seen\": {\"count\": 1, \"max\": 316680, \"sum\": 316680.0, \"min\": 316680}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 140, \"sum\": 140.0, \"min\": 140}}, \"EndTime\": 1593379125.50275, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 69}, \"StartTime\": 1593379124.360196}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3959.08912432 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:45 INFO 140483679958848] # Starting training for epoch 71\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m[2020-06-28 21:18:45.648] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 203, \"duration\": 1221, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] # Finished training epoch 68 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Loss (name: value) total: 6.55095266634\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Loss (name: value) kld: 0.183260036425\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Loss (name: value) recons: 6.36769267586\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Loss (name: value) logppx: 6.55095266634\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] #quality_metric: host=algo-2, epoch=68, train total_loss <loss>=6.55095266634\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] patience losses:[6.559609088632795, 6.56524967485004, 6.548815647761027, 6.553648451964061, 6.555119660165575] min patience loss:6.54881564776 current loss:6.55095266634 absolute loss difference:0.00213701857461\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] #progress_metric: host=algo-2, completed 68 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2448, \"sum\": 2448.0, \"min\": 2448}, \"Total Records Seen\": {\"count\": 1, \"max\": 307836, \"sum\": 307836.0, \"min\": 307836}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 136, \"sum\": 136.0, \"min\": 136}}, \"EndTime\": 1593379125.649748, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 67}, \"StartTime\": 1593379124.42666}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3700.90624304 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:45 INFO 140435207169856] # Starting training for epoch 69\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:46.732] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1229, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] # Finished training epoch 71 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] Loss (name: value) total: 6.54137049781\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] Loss (name: value) kld: 0.186982587394\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] Loss (name: value) recons: 6.35438793235\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] Loss (name: value) logppx: 6.54137049781\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] #quality_metric: host=algo-1, epoch=71, train total_loss <loss>=6.54137049781\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] patience losses:[6.55311158630583, 6.550752494070265, 6.545364055368635, 6.5481760369406805, 6.544335888491736] min patience loss:6.54433588849 current loss:6.54137049781 absolute loss difference:0.00296539068222\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2556, \"sum\": 2556.0, \"min\": 2556}, \"Total Records Seen\": {\"count\": 1, \"max\": 321204, \"sum\": 321204.0, \"min\": 321204}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}}, \"EndTime\": 1593379126.737826, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 70}, \"StartTime\": 1593379125.503015}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3663.29116022 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:46 INFO 140483679958848] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:46.942] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 206, \"duration\": 1292, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] # Finished training epoch 69 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Loss (name: value) total: 6.54891622729\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Loss (name: value) kld: 0.183343034445\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Loss (name: value) recons: 6.36557320091\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Loss (name: value) logppx: 6.54891622729\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] #quality_metric: host=algo-2, epoch=69, train total_loss <loss>=6.54891622729\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] patience losses:[6.56524967485004, 6.548815647761027, 6.553648451964061, 6.555119660165575, 6.550952666335636] min patience loss:6.54881564776 current loss:6.54891622729 absolute loss difference:0.000100579526689\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] #progress_metric: host=algo-2, completed 69 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2484, \"sum\": 2484.0, \"min\": 2484}, \"Total Records Seen\": {\"count\": 1, \"max\": 312363, \"sum\": 312363.0, \"min\": 312363}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 138, \"sum\": 138.0, \"min\": 138}}, \"EndTime\": 1593379126.944282, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 68}, \"StartTime\": 1593379125.649975}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3497.27609334 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:46 INFO 140435207169856] # Starting training for epoch 70\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:47.928] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1189, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] # Finished training epoch 72 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] Loss (name: value) total: 6.53528923459\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] Loss (name: value) kld: 0.187393496227\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] Loss (name: value) recons: 6.34789574809\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] Loss (name: value) logppx: 6.53528923459\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] #quality_metric: host=algo-1, epoch=72, train total_loss <loss>=6.53528923459\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] patience losses:[6.550752494070265, 6.545364055368635, 6.5481760369406805, 6.544335888491736, 6.541370497809516] min patience loss:6.54137049781 current loss:6.53528923459 absolute loss difference:0.00608126322428\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2592, \"sum\": 2592.0, \"min\": 2592}, \"Total Records Seen\": {\"count\": 1, \"max\": 325728, \"sum\": 325728.0, \"min\": 325728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}}, \"EndTime\": 1593379127.932251, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 71}, \"StartTime\": 1593379126.73811}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3787.27264845 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:47 INFO 140483679958848] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:48.167] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 209, \"duration\": 1222, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] # Finished training epoch 70 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] Loss (name: value) total: 6.54263265265\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] Loss (name: value) kld: 0.187846592938\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] Loss (name: value) recons: 6.35478605827\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] Loss (name: value) logppx: 6.54263265265\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] #quality_metric: host=algo-2, epoch=70, train total_loss <loss>=6.54263265265\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] patience losses:[6.548815647761027, 6.553648451964061, 6.555119660165575, 6.550952666335636, 6.548916227287716] min patience loss:6.54881564776 current loss:6.54263265265 absolute loss difference:0.00618299510744\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] #progress_metric: host=algo-2, completed 70 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2520, \"sum\": 2520.0, \"min\": 2520}, \"Total Records Seen\": {\"count\": 1, \"max\": 316890, \"sum\": 316890.0, \"min\": 316890}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 140, \"sum\": 140.0, \"min\": 140}}, \"EndTime\": 1593379128.172312, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 69}, \"StartTime\": 1593379126.944515}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3685.93914311 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:48 INFO 140435207169856] # Starting training for epoch 71\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:49.141] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1208, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] # Finished training epoch 73 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] Loss (name: value) total: 6.53275901079\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] Loss (name: value) kld: 0.192217198511\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] Loss (name: value) recons: 6.34054183298\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] Loss (name: value) logppx: 6.53275901079\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] #quality_metric: host=algo-1, epoch=73, train total_loss <loss>=6.53275901079\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] patience losses:[6.545364055368635, 6.5481760369406805, 6.544335888491736, 6.541370497809516, 6.535289234585232] min patience loss:6.53528923459 current loss:6.53275901079 absolute loss difference:0.00253022379345\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2628, \"sum\": 2628.0, \"min\": 2628}, \"Total Records Seen\": {\"count\": 1, \"max\": 330252, \"sum\": 330252.0, \"min\": 330252}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 146, \"sum\": 146.0, \"min\": 146}}, \"EndTime\": 1593379129.145709, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 72}, \"StartTime\": 1593379127.93292}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3729.67869898 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:49 INFO 140483679958848] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:49.428] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 212, \"duration\": 1255, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] # Finished training epoch 71 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] Loss (name: value) total: 6.53617072768\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] Loss (name: value) kld: 0.188466622391\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] Loss (name: value) recons: 6.34770415889\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] Loss (name: value) logppx: 6.53617072768\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] #quality_metric: host=algo-2, epoch=71, train total_loss <loss>=6.53617072768\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] patience losses:[6.553648451964061, 6.555119660165575, 6.550952666335636, 6.548916227287716, 6.542632652653588] min patience loss:6.54263265265 current loss:6.53617072768 absolute loss difference:0.00646192497677\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] #progress_metric: host=algo-2, completed 71 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2556, \"sum\": 2556.0, \"min\": 2556}, \"Total Records Seen\": {\"count\": 1, \"max\": 321417, \"sum\": 321417.0, \"min\": 321417}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 142, \"sum\": 142.0, \"min\": 142}}, \"EndTime\": 1593379129.435854, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 70}, \"StartTime\": 1593379128.17302}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3584.23234331 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:49 INFO 140435207169856] # Starting training for epoch 72\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:50.340] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1193, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] # Finished training epoch 74 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] Loss (name: value) total: 6.52976212237\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] Loss (name: value) kld: 0.191471354622\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] Loss (name: value) recons: 6.33829077747\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] Loss (name: value) logppx: 6.52976212237\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] #quality_metric: host=algo-1, epoch=74, train total_loss <loss>=6.52976212237\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] patience losses:[6.5481760369406805, 6.544335888491736, 6.541370497809516, 6.535289234585232, 6.532759010791779] min patience loss:6.53275901079 current loss:6.52976212237 absolute loss difference:0.00299688842562\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2664, \"sum\": 2664.0, \"min\": 2664}, \"Total Records Seen\": {\"count\": 1, \"max\": 334776, \"sum\": 334776.0, \"min\": 334776}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 148, \"sum\": 148.0, \"min\": 148}}, \"EndTime\": 1593379130.344881, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 73}, \"StartTime\": 1593379129.146009}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3773.14165792 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:50 INFO 140483679958848] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:50.677] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 215, \"duration\": 1238, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] # Finished training epoch 72 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Loss (name: value) total: 6.53855433067\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Loss (name: value) kld: 0.189631321364\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Loss (name: value) recons: 6.34892301427\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Loss (name: value) logppx: 6.53855433067\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] #quality_metric: host=algo-2, epoch=72, train total_loss <loss>=6.53855433067\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] patience losses:[6.555119660165575, 6.550952666335636, 6.548916227287716, 6.542632652653588, 6.536170727676815] min patience loss:6.53617072768 current loss:6.53855433067 absolute loss difference:0.00238360299004\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] #progress_metric: host=algo-2, completed 72 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2592, \"sum\": 2592.0, \"min\": 2592}, \"Total Records Seen\": {\"count\": 1, \"max\": 325944, \"sum\": 325944.0, \"min\": 325944}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 144, \"sum\": 144.0, \"min\": 144}}, \"EndTime\": 1593379130.679282, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 71}, \"StartTime\": 1593379129.436191}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3640.99082064 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:50 INFO 140435207169856] # Starting training for epoch 73\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:51.555] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1209, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] # Finished training epoch 75 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Loss (name: value) total: 6.53435803784\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Loss (name: value) kld: 0.196340866387\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Loss (name: value) recons: 6.33801719877\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Loss (name: value) logppx: 6.53435803784\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] #quality_metric: host=algo-1, epoch=75, train total_loss <loss>=6.53435803784\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] patience losses:[6.544335888491736, 6.541370497809516, 6.535289234585232, 6.532759010791779, 6.529762122366163] min patience loss:6.52976212237 current loss:6.53435803784 absolute loss difference:0.00459591547648\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2700, \"sum\": 2700.0, \"min\": 2700}, \"Total Records Seen\": {\"count\": 1, \"max\": 339300, \"sum\": 339300.0, \"min\": 339300}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}}, \"EndTime\": 1593379131.557232, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 74}, \"StartTime\": 1593379130.345119}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3731.9130445 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:51 INFO 140483679958848] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:51.930] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 218, \"duration\": 1250, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] # Finished training epoch 73 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] Loss (name: value) total: 6.53166878886\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] Loss (name: value) kld: 0.193234042265\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] Loss (name: value) recons: 6.33843478892\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] Loss (name: value) logppx: 6.53166878886\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] #quality_metric: host=algo-2, epoch=73, train total_loss <loss>=6.53166878886\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] patience losses:[6.550952666335636, 6.548916227287716, 6.542632652653588, 6.536170727676815, 6.53855433066686] min patience loss:6.53617072768 current loss:6.53166878886 absolute loss difference:0.00450193881989\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] #progress_metric: host=algo-2, completed 73 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2628, \"sum\": 2628.0, \"min\": 2628}, \"Total Records Seen\": {\"count\": 1, \"max\": 330471, \"sum\": 330471.0, \"min\": 330471}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 146, \"sum\": 146.0, \"min\": 146}}, \"EndTime\": 1593379131.935047, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 72}, \"StartTime\": 1593379130.679659}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3605.56884625 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:51 INFO 140435207169856] # Starting training for epoch 74\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:52.762] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] # Finished training epoch 76 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] Loss (name: value) total: 6.52311878072\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] Loss (name: value) kld: 0.1939866816\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] Loss (name: value) recons: 6.32913209332\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] Loss (name: value) logppx: 6.52311878072\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] #quality_metric: host=algo-1, epoch=76, train total_loss <loss>=6.52311878072\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] patience losses:[6.541370497809516, 6.535289234585232, 6.532759010791779, 6.529762122366163, 6.534358037842645] min patience loss:6.52976212237 current loss:6.52311878072 absolute loss difference:0.00664334164725\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2736, \"sum\": 2736.0, \"min\": 2736}, \"Total Records Seen\": {\"count\": 1, \"max\": 343824, \"sum\": 343824.0, \"min\": 343824}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 152, \"sum\": 152.0, \"min\": 152}}, \"EndTime\": 1593379132.767751, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 75}, \"StartTime\": 1593379131.557709}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3738.25948029 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:52 INFO 140483679958848] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:53.176] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 221, \"duration\": 1240, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] # Finished training epoch 74 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] Loss (name: value) total: 6.52826223771\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] Loss (name: value) kld: 0.193168177166\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] Loss (name: value) recons: 6.33509403467\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] Loss (name: value) logppx: 6.52826223771\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] #quality_metric: host=algo-2, epoch=74, train total_loss <loss>=6.52826223771\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] patience losses:[6.548916227287716, 6.542632652653588, 6.536170727676815, 6.53855433066686, 6.53166878885693] min patience loss:6.53166878886 current loss:6.52826223771 absolute loss difference:0.00340655114916\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] Timing: train: 1.24s, val: 0.00s, epoch: 1.24s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] #progress_metric: host=algo-2, completed 74 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2664, \"sum\": 2664.0, \"min\": 2664}, \"Total Records Seen\": {\"count\": 1, \"max\": 334998, \"sum\": 334998.0, \"min\": 334998}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 148, \"sum\": 148.0, \"min\": 148}}, \"EndTime\": 1593379133.180111, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 73}, \"StartTime\": 1593379131.935332}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3636.33432281 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:53 INFO 140435207169856] # Starting training for epoch 75\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:53.975] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1207, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] # Finished training epoch 77 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Loss (name: value) total: 6.53122138315\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Loss (name: value) kld: 0.200213751445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Loss (name: value) recons: 6.33100768593\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Loss (name: value) logppx: 6.53122138315\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] #quality_metric: host=algo-1, epoch=77, train total_loss <loss>=6.53122138315\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] patience losses:[6.535289234585232, 6.532759010791779, 6.529762122366163, 6.534358037842645, 6.523118780718909] min patience loss:6.52311878072 current loss:6.53122138315 absolute loss difference:0.00810260242886\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2772, \"sum\": 2772.0, \"min\": 2772}, \"Total Records Seen\": {\"count\": 1, \"max\": 348348, \"sum\": 348348.0, \"min\": 348348}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 154, \"sum\": 154.0, \"min\": 154}}, \"EndTime\": 1593379133.976837, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 76}, \"StartTime\": 1593379132.768045}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3742.15505493 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:53 INFO 140483679958848] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:54.350] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 224, \"duration\": 1169, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] # Finished training epoch 75 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Loss (name: value) total: 6.53698727157\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Loss (name: value) kld: 0.195203792097\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Loss (name: value) recons: 6.34178350369\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Loss (name: value) logppx: 6.53698727157\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] #quality_metric: host=algo-2, epoch=75, train total_loss <loss>=6.53698727157\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] patience losses:[6.542632652653588, 6.536170727676815, 6.53855433066686, 6.53166878885693, 6.528262237707774] min patience loss:6.52826223771 current loss:6.53698727157 absolute loss difference:0.00872503386603\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] #progress_metric: host=algo-2, completed 75 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2700, \"sum\": 2700.0, \"min\": 2700}, \"Total Records Seen\": {\"count\": 1, \"max\": 339525, \"sum\": 339525.0, \"min\": 339525}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 150, \"sum\": 150.0, \"min\": 150}}, \"EndTime\": 1593379134.352127, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 74}, \"StartTime\": 1593379133.18042}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3863.07929727 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:54 INFO 140435207169856] # Starting training for epoch 76\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:55.082] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1104, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] # Finished training epoch 78 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] Loss (name: value) total: 6.51340106461\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] Loss (name: value) kld: 0.197112217132\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] Loss (name: value) recons: 6.31628883547\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] Loss (name: value) logppx: 6.51340106461\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] #quality_metric: host=algo-1, epoch=78, train total_loss <loss>=6.51340106461\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] patience losses:[6.532759010791779, 6.529762122366163, 6.534358037842645, 6.523118780718909, 6.53122138314777] min patience loss:6.52311878072 current loss:6.51340106461 absolute loss difference:0.00971771611108\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] Timing: train: 1.11s, val: 0.00s, epoch: 1.11s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2808, \"sum\": 2808.0, \"min\": 2808}, \"Total Records Seen\": {\"count\": 1, \"max\": 352872, \"sum\": 352872.0, \"min\": 352872}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 156, \"sum\": 156.0, \"min\": 156}}, \"EndTime\": 1593379135.087877, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 77}, \"StartTime\": 1593379133.977102}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4072.28575273 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:55 INFO 140483679958848] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:55.560] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 227, \"duration\": 1207, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] # Finished training epoch 76 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Loss (name: value) total: 6.52796909544\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Loss (name: value) kld: 0.195864464674\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Loss (name: value) recons: 6.33210461007\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Loss (name: value) logppx: 6.52796909544\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] #quality_metric: host=algo-2, epoch=76, train total_loss <loss>=6.52796909544\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:56.318] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1230, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] # Finished training epoch 79 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Loss (name: value) total: 6.516524699\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Loss (name: value) kld: 0.202038005408\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Loss (name: value) recons: 6.31448670228\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Loss (name: value) logppx: 6.516524699\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] #quality_metric: host=algo-1, epoch=79, train total_loss <loss>=6.516524699\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] patience losses:[6.529762122366163, 6.534358037842645, 6.523118780718909, 6.53122138314777, 6.513401064607832] min patience loss:6.51340106461 current loss:6.516524699 absolute loss difference:0.00312363439136\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] Timing: train: 1.23s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2844, \"sum\": 2844.0, \"min\": 2844}, \"Total Records Seen\": {\"count\": 1, \"max\": 357396, \"sum\": 357396.0, \"min\": 357396}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 158, \"sum\": 158.0, \"min\": 158}}, \"EndTime\": 1593379136.319992, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 78}, \"StartTime\": 1593379135.088171}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3672.20266113 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:56 INFO 140483679958848] # Starting training for epoch 80\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] patience losses:[6.536170727676815, 6.53855433066686, 6.53166878885693, 6.528262237707774, 6.536987271573809] min patience loss:6.52826223771 current loss:6.52796909544 absolute loss difference:0.000293142265744\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] #progress_metric: host=algo-2, completed 76 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2736, \"sum\": 2736.0, \"min\": 2736}, \"Total Records Seen\": {\"count\": 1, \"max\": 344052, \"sum\": 344052.0, \"min\": 344052}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 152, \"sum\": 152.0, \"min\": 152}}, \"EndTime\": 1593379135.566349, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 75}, \"StartTime\": 1593379134.352389}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3727.54270045 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:55 INFO 140435207169856] # Starting training for epoch 77\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:56.822] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 230, \"duration\": 1254, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] # Finished training epoch 77 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] Loss (name: value) total: 6.52278696166\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] Loss (name: value) kld: 0.198864749736\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] Loss (name: value) recons: 6.32392221027\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] Loss (name: value) logppx: 6.52278696166\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] #quality_metric: host=algo-2, epoch=77, train total_loss <loss>=6.52278696166\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] patience losses:[6.53855433066686, 6.53166878885693, 6.528262237707774, 6.536987271573809, 6.52796909544203] min patience loss:6.52796909544 current loss:6.52278696166 absolute loss difference:0.00518213378059\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] Timing: train: 1.26s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] #progress_metric: host=algo-2, completed 77 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2772, \"sum\": 2772.0, \"min\": 2772}, \"Total Records Seen\": {\"count\": 1, \"max\": 348579, \"sum\": 348579.0, \"min\": 348579}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 154, \"sum\": 154.0, \"min\": 154}}, \"EndTime\": 1593379136.827063, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 76}, \"StartTime\": 1593379135.567128}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3592.07619866 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:56 INFO 140435207169856] # Starting training for epoch 78\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:57.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1134, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] # Finished training epoch 80 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] Loss (name: value) total: 6.51097149981\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] Loss (name: value) kld: 0.202375907451\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] Loss (name: value) recons: 6.30859561099\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] Loss (name: value) logppx: 6.51097149981\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] #quality_metric: host=algo-1, epoch=80, train total_loss <loss>=6.51097149981\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] patience losses:[6.534358037842645, 6.523118780718909, 6.53122138314777, 6.513401064607832, 6.516524698999193] min patience loss:6.51340106461 current loss:6.51097149981 absolute loss difference:0.0024295647939\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2880, \"sum\": 2880.0, \"min\": 2880}, \"Total Records Seen\": {\"count\": 1, \"max\": 361920, \"sum\": 361920.0, \"min\": 361920}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 160, \"sum\": 160.0, \"min\": 160}}, \"EndTime\": 1593379137.459383, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 79}, \"StartTime\": 1593379136.32023}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3970.97443178 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:57 INFO 140483679958848] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:58.026] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 233, \"duration\": 1196, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] # Finished training epoch 78 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Loss (name: value) total: 6.52575240533\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Loss (name: value) kld: 0.199967375956\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Loss (name: value) recons: 6.3257850541\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Loss (name: value) logppx: 6.52575240533\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] #quality_metric: host=algo-2, epoch=78, train total_loss <loss>=6.52575240533\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] patience losses:[6.53166878885693, 6.528262237707774, 6.536987271573809, 6.52796909544203, 6.522786961661445] min patience loss:6.52278696166 current loss:6.52575240533 absolute loss difference:0.00296544366413\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] #progress_metric: host=algo-2, completed 78 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2808, \"sum\": 2808.0, \"min\": 2808}, \"Total Records Seen\": {\"count\": 1, \"max\": 353106, \"sum\": 353106.0, \"min\": 353106}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 156, \"sum\": 156.0, \"min\": 156}}, \"EndTime\": 1593379138.027494, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 77}, \"StartTime\": 1593379136.827629}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3772.5185884 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:58 INFO 140435207169856] # Starting training for epoch 79\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:58.649] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1189, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] # Finished training epoch 81 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Loss (name: value) total: 6.51616189215\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Loss (name: value) kld: 0.203750411669\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Loss (name: value) recons: 6.31241152684\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Loss (name: value) logppx: 6.51616189215\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] #quality_metric: host=algo-1, epoch=81, train total_loss <loss>=6.51616189215\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] patience losses:[6.523118780718909, 6.53122138314777, 6.513401064607832, 6.516524698999193, 6.510971499813928] min patience loss:6.51097149981 current loss:6.51616189215 absolute loss difference:0.00519039233526\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2916, \"sum\": 2916.0, \"min\": 2916}, \"Total Records Seen\": {\"count\": 1, \"max\": 366444, \"sum\": 366444.0, \"min\": 366444}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 162, \"sum\": 162.0, \"min\": 162}}, \"EndTime\": 1593379138.652619, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 80}, \"StartTime\": 1593379137.459618}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3791.65367902 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:58 INFO 140483679958848] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:18:59.231] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 236, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] # Finished training epoch 79 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Loss (name: value) total: 6.52559194962\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Loss (name: value) kld: 0.204601212922\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Loss (name: value) recons: 6.32099070814\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Loss (name: value) logppx: 6.52559194962\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] #quality_metric: host=algo-2, epoch=79, train total_loss <loss>=6.52559194962\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] patience losses:[6.528262237707774, 6.536987271573809, 6.52796909544203, 6.522786961661445, 6.525752405325572] min patience loss:6.52278696166 current loss:6.52559194962 absolute loss difference:0.00280498796039\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] #progress_metric: host=algo-2, completed 79 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2844, \"sum\": 2844.0, \"min\": 2844}, \"Total Records Seen\": {\"count\": 1, \"max\": 357633, \"sum\": 357633.0, \"min\": 357633}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 158, \"sum\": 158.0, \"min\": 158}}, \"EndTime\": 1593379139.233448, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 78}, \"StartTime\": 1593379138.027738}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3754.15484483 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:18:59 INFO 140435207169856] # Starting training for epoch 80\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:18:59.821] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1167, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] # Finished training epoch 82 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] Loss (name: value) total: 6.50808742974\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] Loss (name: value) kld: 0.205168552697\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] Loss (name: value) recons: 6.30291888449\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] Loss (name: value) logppx: 6.50808742974\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] #quality_metric: host=algo-1, epoch=82, train total_loss <loss>=6.50808742974\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] patience losses:[6.53122138314777, 6.513401064607832, 6.516524698999193, 6.510971499813928, 6.516161892149183] min patience loss:6.51097149981 current loss:6.50808742974 absolute loss difference:0.00288407007853\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2952, \"sum\": 2952.0, \"min\": 2952}, \"Total Records Seen\": {\"count\": 1, \"max\": 370968, \"sum\": 370968.0, \"min\": 370968}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 164, \"sum\": 164.0, \"min\": 164}}, \"EndTime\": 1593379139.827041, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 81}, \"StartTime\": 1593379138.65306}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3852.45483621 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:18:59 INFO 140483679958848] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:00.412] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 239, \"duration\": 1178, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] # Finished training epoch 80 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] Loss (name: value) total: 6.51526667012\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] Loss (name: value) kld: 0.202621933901\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] Loss (name: value) recons: 6.31264474657\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] Loss (name: value) logppx: 6.51526667012\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] #quality_metric: host=algo-2, epoch=80, train total_loss <loss>=6.51526667012\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] patience losses:[6.536987271573809, 6.52796909544203, 6.522786961661445, 6.525752405325572, 6.525591949621837] min patience loss:6.52278696166 current loss:6.51526667012 absolute loss difference:0.00752029154036\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] #progress_metric: host=algo-2, completed 80 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2880, \"sum\": 2880.0, \"min\": 2880}, \"Total Records Seen\": {\"count\": 1, \"max\": 362160, \"sum\": 362160.0, \"min\": 362160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 160, \"sum\": 160.0, \"min\": 160}}, \"EndTime\": 1593379140.417543, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 79}, \"StartTime\": 1593379139.233744}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3823.4411847 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:00 INFO 140435207169856] # Starting training for epoch 81\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:00.974] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1146, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] # Finished training epoch 83 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Loss (name: value) total: 6.51091588206\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Loss (name: value) kld: 0.206500417242\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Loss (name: value) recons: 6.30441542466\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Loss (name: value) logppx: 6.51091588206\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] #quality_metric: host=algo-1, epoch=83, train total_loss <loss>=6.51091588206\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] patience losses:[6.513401064607832, 6.516524698999193, 6.510971499813928, 6.516161892149183, 6.5080874297353954] min patience loss:6.50808742974 current loss:6.51091588206 absolute loss difference:0.00282845232222\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2988, \"sum\": 2988.0, \"min\": 2988}, \"Total Records Seen\": {\"count\": 1, \"max\": 375492, \"sum\": 375492.0, \"min\": 375492}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 166, \"sum\": 166.0, \"min\": 166}}, \"EndTime\": 1593379140.976188, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 82}, \"StartTime\": 1593379139.827607}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3938.3209717 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:00 INFO 140483679958848] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:02.102] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1125, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] # Finished training epoch 84 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] Loss (name: value) total: 6.50681879123\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] Loss (name: value) kld: 0.208956775566\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] Loss (name: value) recons: 6.29786200656\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] Loss (name: value) logppx: 6.50681879123\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] #quality_metric: host=algo-1, epoch=84, train total_loss <loss>=6.50681879123\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] patience losses:[6.516524698999193, 6.510971499813928, 6.516161892149183, 6.5080874297353954, 6.510915882057613] min patience loss:6.50808742974 current loss:6.50681879123 absolute loss difference:0.00126863850488\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] Timing: train: 1.13s, val: 0.00s, epoch: 1.13s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3024, \"sum\": 3024.0, \"min\": 3024}, \"Total Records Seen\": {\"count\": 1, \"max\": 380016, \"sum\": 380016.0, \"min\": 380016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 168, \"sum\": 168.0, \"min\": 168}}, \"EndTime\": 1593379142.108084, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 83}, \"StartTime\": 1593379140.976421}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3997.1518199 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:02 INFO 140483679958848] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:01.587] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 242, \"duration\": 1167, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] # Finished training epoch 81 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Loss (name: value) total: 6.52274559604\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Loss (name: value) kld: 0.206206555789\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Loss (name: value) recons: 6.31653906239\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Loss (name: value) logppx: 6.52274559604\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] #quality_metric: host=algo-2, epoch=81, train total_loss <loss>=6.52274559604\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] patience losses:[6.52796909544203, 6.522786961661445, 6.525752405325572, 6.525591949621837, 6.515266670121087] min patience loss:6.51526667012 current loss:6.52274559604 absolute loss difference:0.00747892591688\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] #progress_metric: host=algo-2, completed 81 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2916, \"sum\": 2916.0, \"min\": 2916}, \"Total Records Seen\": {\"count\": 1, \"max\": 366687, \"sum\": 366687.0, \"min\": 366687}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 162, \"sum\": 162.0, \"min\": 162}}, \"EndTime\": 1593379141.588971, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 80}, \"StartTime\": 1593379140.418015}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3865.40476077 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:01 INFO 140435207169856] # Starting training for epoch 82\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:03.301] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] # Finished training epoch 85 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] Loss (name: value) total: 6.50457190805\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] Loss (name: value) kld: 0.210216441295\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] Loss (name: value) recons: 6.29435547193\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] Loss (name: value) logppx: 6.50457190805\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] #quality_metric: host=algo-1, epoch=85, train total_loss <loss>=6.50457190805\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] patience losses:[6.510971499813928, 6.516161892149183, 6.5080874297353954, 6.510915882057613, 6.506818791230519] min patience loss:6.50681879123 current loss:6.50457190805 absolute loss difference:0.00224688318041\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3060, \"sum\": 3060.0, \"min\": 3060}, \"Total Records Seen\": {\"count\": 1, \"max\": 384540, \"sum\": 384540.0, \"min\": 384540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 170, \"sum\": 170.0, \"min\": 170}}, \"EndTime\": 1593379143.305587, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 84}, \"StartTime\": 1593379142.108379}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3778.33172563 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:03 INFO 140483679958848] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:02.863] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 245, \"duration\": 1273, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] # Finished training epoch 82 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] Loss (name: value) total: 6.50780881113\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] Loss (name: value) kld: 0.206874040059\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] Loss (name: value) recons: 6.3009347717\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] Loss (name: value) logppx: 6.50780881113\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] #quality_metric: host=algo-2, epoch=82, train total_loss <loss>=6.50780881113\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] patience losses:[6.522786961661445, 6.525752405325572, 6.525591949621837, 6.515266670121087, 6.5227455960379706] min patience loss:6.51526667012 current loss:6.50780881113 absolute loss difference:0.00745785898633\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] Timing: train: 1.27s, val: 0.00s, epoch: 1.28s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] #progress_metric: host=algo-2, completed 82 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2952, \"sum\": 2952.0, \"min\": 2952}, \"Total Records Seen\": {\"count\": 1, \"max\": 371214, \"sum\": 371214.0, \"min\": 371214}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 164, \"sum\": 164.0, \"min\": 164}}, \"EndTime\": 1593379142.868457, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 81}, \"StartTime\": 1593379141.589294}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3538.62896177 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:02 INFO 140435207169856] # Starting training for epoch 83\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:04.411] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1105, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] # Finished training epoch 86 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Loss (name: value) total: 6.5076753762\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Loss (name: value) kld: 0.215064627429\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Loss (name: value) recons: 6.29261081749\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Loss (name: value) logppx: 6.5076753762\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] #quality_metric: host=algo-1, epoch=86, train total_loss <loss>=6.5076753762\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] patience losses:[6.516161892149183, 6.5080874297353954, 6.510915882057613, 6.506818791230519, 6.504571908050114] min patience loss:6.50457190805 current loss:6.5076753762 absolute loss difference:0.00310346815321\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] Timing: train: 1.11s, val: 0.00s, epoch: 1.11s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3096, \"sum\": 3096.0, \"min\": 3096}, \"Total Records Seen\": {\"count\": 1, \"max\": 389064, \"sum\": 389064.0, \"min\": 389064}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 172, \"sum\": 172.0, \"min\": 172}}, \"EndTime\": 1593379144.413041, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 85}, \"StartTime\": 1593379143.305868}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4085.52112991 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:04 INFO 140483679958848] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:04.048] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 248, \"duration\": 1178, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] # Finished training epoch 83 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Loss (name: value) total: 6.5144282447\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Loss (name: value) kld: 0.209148953772\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Loss (name: value) recons: 6.30527927478\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Loss (name: value) logppx: 6.5144282447\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] #quality_metric: host=algo-2, epoch=83, train total_loss <loss>=6.5144282447\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] patience losses:[6.525752405325572, 6.525591949621837, 6.515266670121087, 6.5227455960379706, 6.507808811134762] min patience loss:6.50780881113 current loss:6.5144282447 absolute loss difference:0.00661943356196\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] #progress_metric: host=algo-2, completed 83 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2988, \"sum\": 2988.0, \"min\": 2988}, \"Total Records Seen\": {\"count\": 1, \"max\": 375741, \"sum\": 375741.0, \"min\": 375741}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 166, \"sum\": 166.0, \"min\": 166}}, \"EndTime\": 1593379144.052105, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 82}, \"StartTime\": 1593379142.868739}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3823.92551773 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:04 INFO 140435207169856] # Starting training for epoch 84\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:05.256] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 251, \"duration\": 1203, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] # Finished training epoch 84 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Loss (name: value) total: 6.5146627095\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Loss (name: value) kld: 0.211354863726\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Loss (name: value) recons: 6.30330787102\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Loss (name: value) logppx: 6.5146627095\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] #quality_metric: host=algo-2, epoch=84, train total_loss <loss>=6.5146627095\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] patience losses:[6.525591949621837, 6.515266670121087, 6.5227455960379706, 6.507808811134762, 6.514428244696723] min patience loss:6.50780881113 current loss:6.5146627095 absolute loss difference:0.00685389836629\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] Timing: train: 1.21s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] #progress_metric: host=algo-2, completed 84 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3024, \"sum\": 3024.0, \"min\": 3024}, \"Total Records Seen\": {\"count\": 1, \"max\": 380268, \"sum\": 380268.0, \"min\": 380268}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 168, \"sum\": 168.0, \"min\": 168}}, \"EndTime\": 1593379145.25895, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 83}, \"StartTime\": 1593379144.052837}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3751.92049909 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:05 INFO 140435207169856] # Starting training for epoch 85\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:05.609] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1195, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] # Finished training epoch 87 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Loss (name: value) total: 6.50723509656\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Loss (name: value) kld: 0.217246358386\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Loss (name: value) recons: 6.28998874956\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Loss (name: value) logppx: 6.50723509656\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] #quality_metric: host=algo-1, epoch=87, train total_loss <loss>=6.50723509656\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] patience losses:[6.5080874297353954, 6.510915882057613, 6.506818791230519, 6.504571908050114, 6.507675376203325] min patience loss:6.50457190805 current loss:6.50723509656 absolute loss difference:0.00266318851047\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3132, \"sum\": 3132.0, \"min\": 3132}, \"Total Records Seen\": {\"count\": 1, \"max\": 393588, \"sum\": 393588.0, \"min\": 393588}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 174, \"sum\": 174.0, \"min\": 174}}, \"EndTime\": 1593379145.611871, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 86}, \"StartTime\": 1593379144.413337}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3773.36075258 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:05 INFO 140483679958848] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:06.446] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 254, \"duration\": 1186, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] # Finished training epoch 85 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Loss (name: value) total: 6.50916992293\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Loss (name: value) kld: 0.211843878238\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Loss (name: value) recons: 6.29732602835\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Loss (name: value) logppx: 6.50916992293\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] #quality_metric: host=algo-2, epoch=85, train total_loss <loss>=6.50916992293\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] patience losses:[6.515266670121087, 6.5227455960379706, 6.507808811134762, 6.514428244696723, 6.514662709501055] min patience loss:6.50780881113 current loss:6.50916992293 absolute loss difference:0.00136111179988\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] #progress_metric: host=algo-2, completed 85 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3060, \"sum\": 3060.0, \"min\": 3060}, \"Total Records Seen\": {\"count\": 1, \"max\": 384795, \"sum\": 384795.0, \"min\": 384795}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 170, \"sum\": 170.0, \"min\": 170}}, \"EndTime\": 1593379146.448415, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 84}, \"StartTime\": 1593379145.259811}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3808.1605573 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:06 INFO 140435207169856] # Starting training for epoch 86\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:06.866] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1253, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] # Finished training epoch 88 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] Loss (name: value) total: 6.50166372458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] Loss (name: value) kld: 0.214148001538\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] Loss (name: value) recons: 6.28751578596\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] Loss (name: value) logppx: 6.50166372458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] #quality_metric: host=algo-1, epoch=88, train total_loss <loss>=6.50166372458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] patience losses:[6.510915882057613, 6.506818791230519, 6.504571908050114, 6.507675376203325, 6.507235096560584] min patience loss:6.50457190805 current loss:6.50166372458 absolute loss difference:0.00290818346871\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] Timing: train: 1.25s, val: 0.00s, epoch: 1.26s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3168, \"sum\": 3168.0, \"min\": 3168}, \"Total Records Seen\": {\"count\": 1, \"max\": 398112, \"sum\": 398112.0, \"min\": 398112}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 176, \"sum\": 176.0, \"min\": 176}}, \"EndTime\": 1593379146.870554, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 87}, \"StartTime\": 1593379145.612473}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3595.54689097 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:06 INFO 140483679958848] # Starting training for epoch 89\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-28 21:19:08.085] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1214, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] # Finished training epoch 89 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] Loss (name: value) total: 6.50061796771\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] Loss (name: value) kld: 0.217592530159\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] Loss (name: value) recons: 6.28302543031\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] Loss (name: value) logppx: 6.50061796771\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] #quality_metric: host=algo-1, epoch=89, train total_loss <loss>=6.50061796771\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] patience losses:[6.506818791230519, 6.504571908050114, 6.507675376203325, 6.507235096560584, 6.501663724581401] min patience loss:6.50166372458 current loss:6.50061796771 absolute loss difference:0.00104575686985\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3204, \"sum\": 3204.0, \"min\": 3204}, \"Total Records Seen\": {\"count\": 1, \"max\": 402636, \"sum\": 402636.0, \"min\": 402636}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 178, \"sum\": 178.0, \"min\": 178}}, \"EndTime\": 1593379148.091016, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 88}, \"StartTime\": 1593379146.870831}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3706.98102908 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:08 INFO 140483679958848] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:07.736] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 257, \"duration\": 1287, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] # Finished training epoch 86 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Loss (name: value) total: 6.50981665982\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Loss (name: value) kld: 0.214140717354\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Loss (name: value) recons: 6.29567589362\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Loss (name: value) logppx: 6.50981665982\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] #quality_metric: host=algo-2, epoch=86, train total_loss <loss>=6.50981665982\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] patience losses:[6.5227455960379706, 6.507808811134762, 6.514428244696723, 6.514662709501055, 6.509169922934638] min patience loss:6.50780881113 current loss:6.50981665982 absolute loss difference:0.00200784868664\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] Timing: train: 1.29s, val: 0.00s, epoch: 1.29s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] #progress_metric: host=algo-2, completed 86 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3096, \"sum\": 3096.0, \"min\": 3096}, \"Total Records Seen\": {\"count\": 1, \"max\": 389322, \"sum\": 389322.0, \"min\": 389322}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 172, \"sum\": 172.0, \"min\": 172}}, \"EndTime\": 1593379147.737768, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 85}, \"StartTime\": 1593379146.448719}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3511.50448092 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:07 INFO 140435207169856] # Starting training for epoch 87\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:09.267] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1175, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] # Finished training epoch 90 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Loss (name: value) total: 6.50071131521\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Loss (name: value) kld: 0.221160118158\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Loss (name: value) recons: 6.27955123451\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Loss (name: value) logppx: 6.50071131521\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] #quality_metric: host=algo-1, epoch=90, train total_loss <loss>=6.50071131521\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] patience losses:[6.504571908050114, 6.507675376203325, 6.507235096560584, 6.501663724581401, 6.5006179677115545] min patience loss:6.50061796771 current loss:6.50071131521 absolute loss difference:9.33474964571e-05\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3240, \"sum\": 3240.0, \"min\": 3240}, \"Total Records Seen\": {\"count\": 1, \"max\": 407160, \"sum\": 407160.0, \"min\": 407160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}}, \"EndTime\": 1593379149.270507, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 89}, \"StartTime\": 1593379148.091461}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3836.35991729 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:09 INFO 140483679958848] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:08.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 260, \"duration\": 1220, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] # Finished training epoch 87 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Loss (name: value) total: 6.50812009308\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Loss (name: value) kld: 0.21591483367\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Loss (name: value) recons: 6.29220525424\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Loss (name: value) logppx: 6.50812009308\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] #quality_metric: host=algo-2, epoch=87, train total_loss <loss>=6.50812009308\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] patience losses:[6.507808811134762, 6.514428244696723, 6.514662709501055, 6.509169922934638, 6.5098166598214044] min patience loss:6.50780881113 current loss:6.50812009308 absolute loss difference:0.000311281945971\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] #progress_metric: host=algo-2, completed 87 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3132, \"sum\": 3132.0, \"min\": 3132}, \"Total Records Seen\": {\"count\": 1, \"max\": 393849, \"sum\": 393849.0, \"min\": 393849}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 174, \"sum\": 174.0, \"min\": 174}}, \"EndTime\": 1593379148.960674, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 86}, \"StartTime\": 1593379147.738005}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3702.15892973 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:08 INFO 140435207169856] # Starting training for epoch 88\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:10.164] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 263, \"duration\": 1202, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] # Finished training epoch 88 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] Loss (name: value) total: 6.5067764322\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] Loss (name: value) kld: 0.216227068255\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] Loss (name: value) recons: 6.29054935111\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] Loss (name: value) logppx: 6.5067764322\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] #quality_metric: host=algo-2, epoch=88, train total_loss <loss>=6.5067764322\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] patience losses:[6.514428244696723, 6.514662709501055, 6.509169922934638, 6.5098166598214044, 6.508120093080732] min patience loss:6.50812009308 current loss:6.5067764322 absolute loss difference:0.00134366088443\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] Timing: train: 1.20s, val: 0.00s, epoch: 1.21s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] #progress_metric: host=algo-2, completed 88 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3168, \"sum\": 3168.0, \"min\": 3168}, \"Total Records Seen\": {\"count\": 1, \"max\": 398376, \"sum\": 398376.0, \"min\": 398376}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 176, \"sum\": 176.0, \"min\": 176}}, \"EndTime\": 1593379150.169364, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 87}, \"StartTime\": 1593379148.960913}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3745.64638825 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:10 INFO 140435207169856] # Starting training for epoch 89\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:10.455] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1184, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] # Finished training epoch 91 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] Loss (name: value) total: 6.49596445428\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] Loss (name: value) kld: 0.219246227708\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] Loss (name: value) recons: 6.27671819925\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] Loss (name: value) logppx: 6.49596445428\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] #quality_metric: host=algo-1, epoch=91, train total_loss <loss>=6.49596445428\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] patience losses:[6.507675376203325, 6.507235096560584, 6.501663724581401, 6.5006179677115545, 6.500711315208012] min patience loss:6.50061796771 current loss:6.49596445428 absolute loss difference:0.00465351343155\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3276, \"sum\": 3276.0, \"min\": 3276}, \"Total Records Seen\": {\"count\": 1, \"max\": 411684, \"sum\": 411684.0, \"min\": 411684}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 182, \"sum\": 182.0, \"min\": 182}}, \"EndTime\": 1593379150.462499, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 90}, \"StartTime\": 1593379149.271136}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3796.64944908 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:10 INFO 140483679958848] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:11.387] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 266, \"duration\": 1217, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] # Finished training epoch 89 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] Loss (name: value) total: 6.49988795651\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] Loss (name: value) kld: 0.217719876518\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] Loss (name: value) recons: 6.2821680771\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] Loss (name: value) logppx: 6.49988795651\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] #quality_metric: host=algo-2, epoch=89, train total_loss <loss>=6.49988795651\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] patience losses:[6.514662709501055, 6.509169922934638, 6.5098166598214044, 6.508120093080732, 6.5067764321962995] min patience loss:6.5067764322 current loss:6.49988795651 absolute loss difference:0.006888475683\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] #progress_metric: host=algo-2, completed 89 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3204, \"sum\": 3204.0, \"min\": 3204}, \"Total Records Seen\": {\"count\": 1, \"max\": 402903, \"sum\": 402903.0, \"min\": 402903}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 178, \"sum\": 178.0, \"min\": 178}}, \"EndTime\": 1593379151.392358, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 88}, \"StartTime\": 1593379150.169708}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3702.20151866 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:11 INFO 140435207169856] # Starting training for epoch 90\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:11.558] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1093, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] # Finished training epoch 92 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Loss (name: value) total: 6.49512878391\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Loss (name: value) kld: 0.219926203498\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Loss (name: value) recons: 6.27520256572\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Loss (name: value) logppx: 6.49512878391\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] #quality_metric: host=algo-1, epoch=92, train total_loss <loss>=6.49512878391\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] patience losses:[6.507235096560584, 6.501663724581401, 6.5006179677115545, 6.500711315208012, 6.4959644542800055] min patience loss:6.49596445428 current loss:6.49512878391 absolute loss difference:0.000835670365228\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] Timing: train: 1.10s, val: 0.00s, epoch: 1.10s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3312, \"sum\": 3312.0, \"min\": 3312}, \"Total Records Seen\": {\"count\": 1, \"max\": 416208, \"sum\": 416208.0, \"min\": 416208}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 184, \"sum\": 184.0, \"min\": 184}}, \"EndTime\": 1593379151.562582, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 91}, \"StartTime\": 1593379150.463127}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=4114.19362935 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:11 INFO 140483679958848] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:12.721] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1158, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] # Finished training epoch 93 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] Loss (name: value) total: 6.48964877923\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] Loss (name: value) kld: 0.221011590213\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] Loss (name: value) recons: 6.26863720682\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] Loss (name: value) logppx: 6.48964877923\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] #quality_metric: host=algo-1, epoch=93, train total_loss <loss>=6.48964877923\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] patience losses:[6.501663724581401, 6.5006179677115545, 6.500711315208012, 6.4959644542800055, 6.495128783914778] min patience loss:6.49512878391 current loss:6.48964877923 absolute loss difference:0.00548000468148\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3348, \"sum\": 3348.0, \"min\": 3348}, \"Total Records Seen\": {\"count\": 1, \"max\": 420732, \"sum\": 420732.0, \"min\": 420732}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 186, \"sum\": 186.0, \"min\": 186}}, \"EndTime\": 1593379152.727513, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 92}, \"StartTime\": 1593379151.562856}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3883.84851726 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:12 INFO 140483679958848] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:12.612] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 269, \"duration\": 1216, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] # Finished training epoch 90 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Loss (name: value) total: 6.50236437718\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Loss (name: value) kld: 0.219416036581\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Loss (name: value) recons: 6.28294831514\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Loss (name: value) logppx: 6.50236437718\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] #quality_metric: host=algo-2, epoch=90, train total_loss <loss>=6.50236437718\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] patience losses:[6.509169922934638, 6.5098166598214044, 6.508120093080732, 6.5067764321962995, 6.499887956513299] min patience loss:6.49988795651 current loss:6.50236437718 absolute loss difference:0.00247642066744\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] #progress_metric: host=algo-2, completed 90 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3240, \"sum\": 3240.0, \"min\": 3240}, \"Total Records Seen\": {\"count\": 1, \"max\": 407430, \"sum\": 407430.0, \"min\": 407430}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}}, \"EndTime\": 1593379152.613618, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 89}, \"StartTime\": 1593379151.392588}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3707.13329336 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:12 INFO 140435207169856] # Starting training for epoch 91\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:13.898] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1169, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] # Finished training epoch 94 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Loss (name: value) total: 6.49957116445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Loss (name: value) kld: 0.224983421258\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Loss (name: value) recons: 6.27458775043\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Loss (name: value) logppx: 6.49957116445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] #quality_metric: host=algo-1, epoch=94, train total_loss <loss>=6.49957116445\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] patience losses:[6.5006179677115545, 6.500711315208012, 6.4959644542800055, 6.495128783914778, 6.489648779233296] min patience loss:6.48964877923 current loss:6.49957116445 absolute loss difference:0.00992238521576\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] Timing: train: 1.17s, val: 0.00s, epoch: 1.17s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3384, \"sum\": 3384.0, \"min\": 3384}, \"Total Records Seen\": {\"count\": 1, \"max\": 425256, \"sum\": 425256.0, \"min\": 425256}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 188, \"sum\": 188.0, \"min\": 188}}, \"EndTime\": 1593379153.899634, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 93}, \"StartTime\": 1593379152.727822}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3859.58718778 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:13 INFO 140483679958848] # Starting training for epoch 95\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:13.833] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 272, \"duration\": 1217, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] # Finished training epoch 91 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] Loss (name: value) total: 6.49877595901\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] Loss (name: value) kld: 0.220061659813\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] Loss (name: value) recons: 6.27871433894\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] Loss (name: value) logppx: 6.49877595901\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] #quality_metric: host=algo-2, epoch=91, train total_loss <loss>=6.49877595901\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] patience losses:[6.5098166598214044, 6.508120093080732, 6.5067764321962995, 6.499887956513299, 6.502364377180736] min patience loss:6.49988795651 current loss:6.49877595901 absolute loss difference:0.00111199749841\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] #progress_metric: host=algo-2, completed 91 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3276, \"sum\": 3276.0, \"min\": 3276}, \"Total Records Seen\": {\"count\": 1, \"max\": 411957, \"sum\": 411957.0, \"min\": 411957}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 182, \"sum\": 182.0, \"min\": 182}}, \"EndTime\": 1593379153.837661, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 90}, \"StartTime\": 1593379152.613853}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3698.71968247 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:13 INFO 140435207169856] # Starting training for epoch 92\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:15.062] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1161, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] # Finished training epoch 95 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Loss (name: value) total: 6.49034166336\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Loss (name: value) kld: 0.222150610553\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Loss (name: value) recons: 6.2681909932\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Loss (name: value) logppx: 6.49034166336\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] #quality_metric: host=algo-1, epoch=95, train total_loss <loss>=6.49034166336\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] patience losses:[6.500711315208012, 6.4959644542800055, 6.495128783914778, 6.489648779233296, 6.499571164449056] min patience loss:6.48964877923 current loss:6.49034166336 absolute loss difference:0.000692884127299\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] Timing: train: 1.16s, val: 0.00s, epoch: 1.16s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3420, \"sum\": 3420.0, \"min\": 3420}, \"Total Records Seen\": {\"count\": 1, \"max\": 429780, \"sum\": 429780.0, \"min\": 429780}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 190, \"sum\": 190.0, \"min\": 190}}, \"EndTime\": 1593379155.064087, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 94}, \"StartTime\": 1593379153.90022}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3886.48398741 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:15 INFO 140483679958848] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:15.061] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 275, \"duration\": 1222, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] # Finished training epoch 92 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] Loss (name: value) total: 6.49625845088\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] Loss (name: value) kld: 0.218493629661\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] Loss (name: value) recons: 6.27776483695\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] Loss (name: value) logppx: 6.49625845088\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] #quality_metric: host=algo-2, epoch=92, train total_loss <loss>=6.49625845088\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] patience losses:[6.508120093080732, 6.5067764321962995, 6.499887956513299, 6.502364377180736, 6.498775959014893] min patience loss:6.49877595901 current loss:6.49625845088 absolute loss difference:0.0025175081359\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.23s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] #progress_metric: host=algo-2, completed 92 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3312, \"sum\": 3312.0, \"min\": 3312}, \"Total Records Seen\": {\"count\": 1, \"max\": 416484, \"sum\": 416484.0, \"min\": 416484}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 184, \"sum\": 184.0, \"min\": 184}}, \"EndTime\": 1593379155.06483, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 91}, \"StartTime\": 1593379153.837894}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3689.27580034 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:15 INFO 140435207169856] # Starting training for epoch 93\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:16.215] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1150, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] # Finished training epoch 96 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] Loss (name: value) total: 6.48548474577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] Loss (name: value) kld: 0.226193192104\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] Loss (name: value) recons: 6.25929158264\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] Loss (name: value) logppx: 6.48548474577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] #quality_metric: host=algo-1, epoch=96, train total_loss <loss>=6.48548474577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] patience losses:[6.4959644542800055, 6.495128783914778, 6.489648779233296, 6.499571164449056, 6.490341663360596] min patience loss:6.48964877923 current loss:6.48548474577 absolute loss difference:0.00416403346591\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] Timing: train: 1.15s, val: 0.00s, epoch: 1.15s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3456, \"sum\": 3456.0, \"min\": 3456}, \"Total Records Seen\": {\"count\": 1, \"max\": 434304, \"sum\": 434304.0, \"min\": 434304}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 192, \"sum\": 192.0, \"min\": 192}}, \"EndTime\": 1593379156.21941, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 95}, \"StartTime\": 1593379155.064349}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3916.00857455 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:16 INFO 140483679958848] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:16.202] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 278, \"duration\": 1136, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] # Finished training epoch 93 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Loss (name: value) total: 6.4962935249\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Loss (name: value) kld: 0.222767164103\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Loss (name: value) recons: 6.27352637053\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Loss (name: value) logppx: 6.4962935249\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] #quality_metric: host=algo-2, epoch=93, train total_loss <loss>=6.4962935249\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] patience losses:[6.5067764321962995, 6.499887956513299, 6.502364377180736, 6.498775959014893, 6.496258450878991] min patience loss:6.49625845088 current loss:6.4962935249 absolute loss difference:3.50740220814e-05\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] Timing: train: 1.14s, val: 0.00s, epoch: 1.14s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] #progress_metric: host=algo-2, completed 93 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3348, \"sum\": 3348.0, \"min\": 3348}, \"Total Records Seen\": {\"count\": 1, \"max\": 421011, \"sum\": 421011.0, \"min\": 421011}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 186, \"sum\": 186.0, \"min\": 186}}, \"EndTime\": 1593379156.203368, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 92}, \"StartTime\": 1593379155.065092}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3976.52200305 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:16 INFO 140435207169856] # Starting training for epoch 94\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:17.412] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1192, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] # Finished training epoch 97 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Loss (name: value) total: 6.49628894197\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Loss (name: value) kld: 0.227742495636\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Loss (name: value) recons: 6.26854640245\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Loss (name: value) logppx: 6.49628894197\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] #quality_metric: host=algo-1, epoch=97, train total_loss <loss>=6.49628894197\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] patience losses:[6.495128783914778, 6.489648779233296, 6.499571164449056, 6.490341663360596, 6.485484745767382] min patience loss:6.48548474577 current loss:6.49628894197 absolute loss difference:0.0108041961988\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3492, \"sum\": 3492.0, \"min\": 3492}, \"Total Records Seen\": {\"count\": 1, \"max\": 438828, \"sum\": 438828.0, \"min\": 438828}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 194, \"sum\": 194.0, \"min\": 194}}, \"EndTime\": 1593379157.414484, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 96}, \"StartTime\": 1593379156.2198}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3786.36652806 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:17 INFO 140483679958848] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:17.393] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 281, \"duration\": 1187, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] # Finished training epoch 94 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Loss (name: value) total: 6.50076970789\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Loss (name: value) kld: 0.221602083908\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Loss (name: value) recons: 6.27916761239\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Loss (name: value) logppx: 6.50076970789\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] #quality_metric: host=algo-2, epoch=94, train total_loss <loss>=6.50076970789\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] patience losses:[6.499887956513299, 6.502364377180736, 6.498775959014893, 6.496258450878991, 6.4962935249010725] min patience loss:6.49625845088 current loss:6.50076970789 absolute loss difference:0.00451125701268\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] #progress_metric: host=algo-2, completed 94 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3384, \"sum\": 3384.0, \"min\": 3384}, \"Total Records Seen\": {\"count\": 1, \"max\": 425538, \"sum\": 425538.0, \"min\": 425538}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 188, \"sum\": 188.0, \"min\": 188}}, \"EndTime\": 1593379157.394887, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 93}, \"StartTime\": 1593379156.203628}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3799.70235087 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:17 INFO 140435207169856] # Starting training for epoch 95\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-06-28 21:19:27 Uploading - Uploading generated training model\u001b[35m[2020-06-28 21:19:18.577] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 284, \"duration\": 1182, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] # Finished training epoch 95 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Loss (name: value) total: 6.49992576573\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Loss (name: value) kld: 0.224718533663\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Loss (name: value) recons: 6.27520721489\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Loss (name: value) logppx: 6.49992576573\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] #quality_metric: host=algo-2, epoch=95, train total_loss <loss>=6.49992576573\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] patience losses:[6.502364377180736, 6.498775959014893, 6.496258450878991, 6.4962935249010725, 6.500769707891676] min patience loss:6.49625845088 current loss:6.49992576573 absolute loss difference:0.00366731484731\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] #progress_metric: host=algo-2, completed 95 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3420, \"sum\": 3420.0, \"min\": 3420}, \"Total Records Seen\": {\"count\": 1, \"max\": 430065, \"sum\": 430065.0, \"min\": 430065}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 190, \"sum\": 190.0, \"min\": 190}}, \"EndTime\": 1593379158.579484, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 94}, \"StartTime\": 1593379157.395184}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3822.00815464 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:18 INFO 140435207169856] # Starting training for epoch 96\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:18.596] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 1181, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] # Finished training epoch 98 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Loss (name: value) total: 6.49064960745\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Loss (name: value) kld: 0.229441184757\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Loss (name: value) recons: 6.26120844815\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Loss (name: value) logppx: 6.49064960745\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] #quality_metric: host=algo-1, epoch=98, train total_loss <loss>=6.49064960745\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] patience losses:[6.489648779233296, 6.499571164449056, 6.490341663360596, 6.485484745767382, 6.496288941966163] min patience loss:6.48548474577 current loss:6.49064960745 absolute loss difference:0.00516486167908\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3528, \"sum\": 3528.0, \"min\": 3528}, \"Total Records Seen\": {\"count\": 1, \"max\": 443352, \"sum\": 443352.0, \"min\": 443352}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 196, \"sum\": 196.0, \"min\": 196}}, \"EndTime\": 1593379158.598193, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 97}, \"StartTime\": 1593379157.414834}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3822.57312757 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:18 INFO 140483679958848] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:19.794] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 1195, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] # Finished training epoch 99 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Loss (name: value) total: 6.48717700773\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Loss (name: value) kld: 0.230635635348\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Loss (name: value) recons: 6.25654141108\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Loss (name: value) logppx: 6.48717700773\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] #quality_metric: host=algo-1, epoch=99, train total_loss <loss>=6.48717700773\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] patience losses:[6.499571164449056, 6.490341663360596, 6.485484745767382, 6.496288941966163, 6.490649607446459] min patience loss:6.48548474577 current loss:6.48717700773 absolute loss difference:0.00169226196077\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] Timing: train: 1.20s, val: 0.00s, epoch: 1.20s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3564, \"sum\": 3564.0, \"min\": 3564}, \"Total Records Seen\": {\"count\": 1, \"max\": 447876, \"sum\": 447876.0, \"min\": 447876}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 198, \"sum\": 198.0, \"min\": 198}}, \"EndTime\": 1593379159.795675, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 98}, \"StartTime\": 1593379158.598449}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3778.33473502 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] \u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:19 INFO 140483679958848] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:19.795] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 287, \"duration\": 1215, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] # Finished training epoch 96 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] Loss (name: value) total: 6.49297910266\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] Loss (name: value) kld: 0.226275069846\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] Loss (name: value) recons: 6.26670406262\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] Loss (name: value) logppx: 6.49297910266\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] #quality_metric: host=algo-2, epoch=96, train total_loss <loss>=6.49297910266\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] patience losses:[6.498775959014893, 6.496258450878991, 6.4962935249010725, 6.500769707891676, 6.499925765726301] min patience loss:6.49625845088 current loss:6.49297910266 absolute loss difference:0.00327934821447\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] Timing: train: 1.22s, val: 0.00s, epoch: 1.22s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] #progress_metric: host=algo-2, completed 96 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3456, \"sum\": 3456.0, \"min\": 3456}, \"Total Records Seen\": {\"count\": 1, \"max\": 434592, \"sum\": 434592.0, \"min\": 434592}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 192, \"sum\": 192.0, \"min\": 192}}, \"EndTime\": 1593379159.80247, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 95}, \"StartTime\": 1593379158.579798}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3702.06509317 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:19 INFO 140435207169856] # Starting training for epoch 97\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:20.981] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 1185, \"num_examples\": 36, \"num_bytes\": 1045376}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] # Finished training epoch 100 on 4524 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Loss (name: value) total: 6.48725899723\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Loss (name: value) kld: 0.227591831651\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Loss (name: value) recons: 6.25966714488\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Loss (name: value) logppx: 6.48725899723\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] #quality_metric: host=algo-1, epoch=100, train total_loss <loss>=6.48725899723\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] patience losses:[6.490341663360596, 6.485484745767382, 6.496288941966163, 6.490649607446459, 6.487177007728153] min patience loss:6.48548474577 current loss:6.48725899723 absolute loss difference:0.00177425146103\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] Timing: train: 1.19s, val: 0.00s, epoch: 1.19s\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3600, \"sum\": 3600.0, \"min\": 3600}, \"Total Records Seen\": {\"count\": 1, \"max\": 452400, \"sum\": 452400.0, \"min\": 452400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4524, \"sum\": 4524.0, \"min\": 4524}, \"Reset Count\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}}, \"EndTime\": 1593379160.982896, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 99}, \"StartTime\": 1593379159.795904}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:20 INFO 140483679958848] #throughput_metric: host=algo-1, train throughput=3810.89693725 records/second\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:20.980] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 290, \"duration\": 1174, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] # Finished training epoch 97 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] Loss (name: value) total: 6.48963614305\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] Loss (name: value) kld: 0.22384341889\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] Loss (name: value) recons: 6.26579274072\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] Loss (name: value) logppx: 6.48963614305\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] #quality_metric: host=algo-2, epoch=97, train total_loss <loss>=6.48963614305\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] patience losses:[6.496258450878991, 6.4962935249010725, 6.500769707891676, 6.499925765726301, 6.492979102664524] min patience loss:6.49297910266 current loss:6.48963614305 absolute loss difference:0.00334295961592\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] Timing: train: 1.18s, val: 0.00s, epoch: 1.18s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] #progress_metric: host=algo-2, completed 97 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3492, \"sum\": 3492.0, \"min\": 3492}, \"Total Records Seen\": {\"count\": 1, \"max\": 439119, \"sum\": 439119.0, \"min\": 439119}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 194, \"sum\": 194.0, \"min\": 194}}, \"EndTime\": 1593379160.986386, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 96}, \"StartTime\": 1593379159.802922}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=3823.90626527 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:20 INFO 140435207169856] # Starting training for epoch 98\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:21.921] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 293, \"duration\": 933, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] # Finished training epoch 98 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] Loss (name: value) total: 6.47681639592\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] Loss (name: value) kld: 0.228778441747\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] Loss (name: value) recons: 6.24803798066\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] Loss (name: value) logppx: 6.47681639592\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] #quality_metric: host=algo-2, epoch=98, train total_loss <loss>=6.47681639592\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] patience losses:[6.4962935249010725, 6.500769707891676, 6.499925765726301, 6.492979102664524, 6.489636143048604] min patience loss:6.48963614305 current loss:6.47681639592 absolute loss difference:0.0128197471301\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] Timing: train: 0.93s, val: 0.00s, epoch: 0.94s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] #progress_metric: host=algo-2, completed 98 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3528, \"sum\": 3528.0, \"min\": 3528}, \"Total Records Seen\": {\"count\": 1, \"max\": 443646, \"sum\": 443646.0, \"min\": 443646}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 196, \"sum\": 196.0, \"min\": 196}}, \"EndTime\": 1593379161.925257, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 97}, \"StartTime\": 1593379160.987042}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=4824.2041878 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:21 INFO 140435207169856] # Starting training for epoch 99\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:22.856] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 296, \"duration\": 930, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] # Finished training epoch 99 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] Loss (name: value) total: 6.46565338638\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] Loss (name: value) kld: 0.232721109978\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] Loss (name: value) recons: 6.23293231593\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] Loss (name: value) logppx: 6.46565338638\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] #quality_metric: host=algo-2, epoch=99, train total_loss <loss>=6.46565338638\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] patience losses:[6.500769707891676, 6.499925765726301, 6.492979102664524, 6.489636143048604, 6.4768163959185285] min patience loss:6.47681639592 current loss:6.46565338638 absolute loss difference:0.0111630095376\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] Timing: train: 0.93s, val: 0.00s, epoch: 0.94s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] #progress_metric: host=algo-2, completed 99 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3564, \"sum\": 3564.0, \"min\": 3564}, \"Total Records Seen\": {\"count\": 1, \"max\": 448173, \"sum\": 448173.0, \"min\": 448173}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 198, \"sum\": 198.0, \"min\": 198}}, \"EndTime\": 1593379162.861924, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 98}, \"StartTime\": 1593379161.925548}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=4833.8684712 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] \u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:22 INFO 140435207169856] # Starting training for epoch 100\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] Best model based on early stopping at epoch 96. Best loss: 6.48548474577\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] Topics from epoch:final (num_topics:30) [, tu 0.55]:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 283 1651 27 634 1031 1574 783 674 232 481 378 1909 918 1983 1541 797 895 907 556 1075\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1651 528 1909 439 246 1213 1751 1774 527 1274 1826 1117 319 783 1708 661 1529 1210 939 280\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1909 1724 282 1969 1128 895 1928 1739 1031 1117 246 225 103 1780 298 1563 1524 1169 672 21\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1765 741 145 1679 1260 1141 499 1294 1242 682 1840 507 806 826 1070 142 830 174 1258 201\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 741 1765 145 1260 142 1141 1679 507 1242 1070 499 1942 682 1258 201 400 540 806 1131 1526\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 939 1993 1776 528 996 642 1676 1210 527 245 1293 1092 952 142 1751 1661 1722 1729 784 68\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1231 1327 891 832 681 1366 1906 177 1992 1955 1339 1111 1907 689 657 1989 1296 1990 1618 1109\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1137 1022 731 1024 170 1146 421 1023 135 1893 730 976 1831 1418 1147 1298 134 137 1964 1047\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 134 137 1831 1418 1893 976 1724 1751 1075 511 730 1729 731 1237 1216 1510 985 1820 1630 1862\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 578 558 1547 557 409 1592 924 1369 295 1960 1186 48 309 410 1588 1283 1568 1531 1196 287\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1651 791 994 528 1213 1993 1909 1349 1654 1117 939 1210 1906 1016 1420 762 1616 637 245 1087\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1724 134 1030 137 1656 183 1447 1318 1128 797 511 1831 1239 1028 1563 1514 730 1928 1780 1969\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1765 741 1131 145 1294 197 499 513 142 1260 927 174 1242 952 527 661 1679 806 1751 507\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1651 10 1075 1909 527 1226 797 1993 634 1819 939 1128 1420 1724 146 991 1765 1855 762 1210\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 668 1765 1526 513 1141 1445 1437 669 174 1528 146 1260 1840 969 1811 1942 499 791 197 1128\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1031 1318 730 31 1060 516 1170 136 989 82 751 1030 1172 556 0 635 1298 27 1605 225\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1765 1031 741 1007 31 556 1801 71 1128 1447 1724 876 1030 134 1744 952 1752 27 1170 137\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1192 1825 1727 472 232 1574 283 476 1292 1826 1193 1406 212 1575 1312 713 231 1295 832 1115\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1765 741 145 1131 513 1751 642 1742 1242 1416 806 201 939 527 1679 1703 142 1528 1141 1042\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1765 741 1128 507 145 174 530 1641 1724 1996 1070 1656 225 1170 1789 962 762 197 194 1062\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1128 1031 225 1780 1170 386 547 31 1812 726 989 1752 1318 1497 1969 102 27 751 1801 183\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1128 1031 516 1800 1170 635 1338 530 556 27 1752 989 803 225 1990 861 1765 1801 1503 1780\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 0 136 730 1298 1418 1831 1893 1964 1419 731 137 134 1060 1024 421 976 1022 1146 1023 1137\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1765 741 145 1679 830 507 499 1445 1242 142 1141 1260 793 856 806 530 452 1751 826 682\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1578 904 171 1654 736 508 1522 1904 291 584 292 113 629 172 112 1782 1541 886 293 742\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1556 1092 350 104 1815 1636 595 200 1602 1205 198 1360 1243 1454 59 1691 1601 1855 245 439\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1170 1031 1128 183 27 876 1030 782 1969 1800 174 1780 1880 1724 635 34 384 1702 1366 82\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 97 133 896 96 91 88 1858 131 1094 721 1793 1856 300 1156 906 1673 364 132 1232 1613\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1651 1724 183 458 313 895 211 955 1751 1957 676 190 593 355 654 1774 1996 1920 837 1125\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] 1128 1780 1447 1969 912 1031 100 1514 1448 511 71 27 556 647 876 1605 1170 1263 1030 458\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:23 INFO 140483679958848] Saved checkpoint to \"/tmp/tmpnUDKZ7/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:23.950] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 124537, \"num_examples\": 1, \"num_bytes\": 27088}\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:19:24.076] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 126, \"num_examples\": 18, \"num_bytes\": 519656}\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] Loss (name: value) total: 6.71234941483\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] Loss (name: value) kld: 0.232443798991\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] Loss (name: value) recons: 6.47990554922\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] Loss (name: value) logppx: 6.71234941483\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Total Records Seen\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593379164.077301, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1593379163.950354}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:19:24 INFO 140483679958848] #test_score (algo-1) : ('log_perplexity', 6.7123494148254395)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 129271.31915092468, \"sum\": 129271.31915092468, \"min\": 129271.31915092468}, \"finalize.time\": {\"count\": 1, \"max\": 126.72209739685059, \"sum\": 126.72209739685059, \"min\": 126.72209739685059}, \"initialize.time\": {\"count\": 1, \"max\": 3321.8419551849365, \"sum\": 3321.8419551849365, \"min\": 3321.8419551849365}, \"model.serialize.time\": {\"count\": 1, \"max\": 4.289150238037109, \"sum\": 4.289150238037109, \"min\": 4.289150238037109}, \"setuptime\": {\"count\": 1, \"max\": 4565.401077270508, \"sum\": 4565.401077270508, \"min\": 4565.401077270508}, \"early_stop.time\": {\"count\": 100, \"max\": 4.935026168823242, \"sum\": 277.3773670196533, \"min\": 0.15282630920410156}, \"update.time\": {\"count\": 100, \"max\": 1385.1890563964844, \"sum\": 118193.29166412354, \"min\": 1090.453863143921}, \"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"model.score.time\": {\"count\": 1, \"max\": 126.88302993774414, \"sum\": 126.88302993774414, \"min\": 126.88302993774414}}, \"EndTime\": 1593379164.078531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1593379039.404208}\n",
      "\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:23.806] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 299, \"duration\": 943, \"num_examples\": 36, \"num_bytes\": 1057504}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] # Finished training epoch 100 on 4527 examples from 36 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Loss (name: value) total: 6.45752392875\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Loss (name: value) kld: 0.231171006544\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Loss (name: value) recons: 6.22635291682\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Loss (name: value) logppx: 6.45752392875\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] #quality_metric: host=algo-2, epoch=100, train total_loss <loss>=6.45752392875\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] patience losses:[6.499925765726301, 6.492979102664524, 6.489636143048604, 6.4768163959185285, 6.465653386380938] min patience loss:6.46565338638 current loss:6.45752392875 absolute loss difference:0.0081294576327\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Timing: train: 0.94s, val: 0.00s, epoch: 0.95s\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] #progress_metric: host=algo-2, completed 100 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3600, \"sum\": 3600.0, \"min\": 3600}, \"Total Records Seen\": {\"count\": 1, \"max\": 452700, \"sum\": 452700.0, \"min\": 452700}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 4527, \"sum\": 4527.0, \"min\": 4527}, \"Reset Count\": {\"count\": 1, \"max\": 200, \"sum\": 200.0, \"min\": 200}}, \"EndTime\": 1593379163.81093, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 99}, \"StartTime\": 1593379162.862361}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] #throughput_metric: host=algo-2, train throughput=4770.53079841 records/second\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Best model based on early stopping at epoch 100. Best loss: 6.45752392875\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Topics from epoch:final (num_topics:30) [, tu 0.55]:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 27 1031 1969 907 1651 1909 1026 1285 1177 722 1724 397 593 797 783 1983 1125 674 1440 246\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1651 194 439 939 528 1117 1909 1125 1751 527 1420 793 807 165 88 1969 762 246 929 1213\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1969 1724 876 527 1128 439 795 282 1088 1031 1993 210 896 511 791 933 1819 1075 325 1335\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1765 741 145 1242 1679 1260 499 1141 682 1294 142 507 806 830 1071 530 793 240 550 452\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 741 1765 145 1260 1141 1679 814 499 1242 826 530 197 1070 806 682 142 1979 1131 1841 540\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1293 939 527 642 1742 1563 1751 1969 1075 791 528 1210 245 68 1042 1651 1676 148 1092 1761\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1231 1327 891 832 681 1366 1906 177 1955 1339 1992 1907 1111 689 657 1296 1614 1990 1109 1075\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 170 731 1137 1022 1024 730 976 135 421 1146 1831 134 137 1023 1298 1047 1418 1893 1419 1060\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 134 137 1831 1751 1724 1117 1420 154 976 1923 1123 1052 68 1893 100 699 694 1132 1418 1135\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 578 1547 558 409 1592 557 924 1369 295 1960 1186 1588 1283 309 48 1568 410 1196 1531 1122\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1651 1993 1075 939 1210 1213 791 1819 528 1909 955 1420 1654 994 928 245 1751 1563 1131 1226\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1724 912 1780 183 704 225 134 137 1656 730 380 31 1486 741 1831 885 1497 0 1446 1513\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1765 741 145 1679 1131 240 1516 927 1935 848 1294 1242 174 642 513 142 1979 793 661 1751\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1651 1909 1102 1128 246 1294 1075 791 165 1993 762 278 1210 991 1855 1764 224 1313 511 1906\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1765 1128 507 741 499 145 1661 540 1724 1260 1729 1577 1437 197 452 513 562 1416 1859 1679\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1031 1318 635 136 0 730 1170 989 1060 1172 1990 31 82 803 1030 27 1419 751 1418 1893\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1031 1128 1765 1171 732 1103 741 1338 1679 1724 145 798 1170 34 556 137 225 134 635 876\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1825 232 1192 1574 472 476 1727 283 1292 1406 1575 1826 1193 1295 212 231 1312 713 1813 832\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1765 513 741 145 1416 1751 806 1942 1742 201 1679 1294 682 1242 642 248 540 1141 1526 146\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1765 741 145 530 1007 1336 1679 1934 1724 632 174 704 803 1031 965 1128 71 751 798 793\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1128 1031 635 225 1318 1170 1780 974 27 1447 511 1752 183 113 1724 1969 751 861 1497 989\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1128 1031 386 989 516 31 1170 1887 1780 27 798 1990 803 530 1070 1752 1171 556 1239 83\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 136 0 1298 730 1418 1831 137 134 1893 1419 731 1060 1964 421 1024 1146 976 138 1022 1023\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 741 1765 145 1260 1242 1141 1679 385 856 682 1258 1445 793 826 806 452 142 814 499 1577\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1578 904 171 736 1654 1904 508 1522 584 172 291 113 1782 629 292 886 112 293 1541 788\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1556 1092 350 595 1636 1815 104 200 1205 1602 198 1243 1855 1454 59 1360 1691 245 1601 439\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1128 1031 1497 1969 782 1724 308 1752 1366 876 1103 1263 1318 1170 949 798 82 1880 100 183\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 97 133 96 896 88 1094 1858 91 1793 131 1856 721 1232 906 300 1156 364 1673 405 893\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 955 1651 1075 1969 1724 1663 1920 1509 458 1881 1128 184 1016 1907 246 1751 837 1563 1436 707\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] 1128 1780 876 1447 308 1031 1969 1765 1007 27 242 1263 709 515 128 912 246 989 1030 1909\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Serializing model to /opt/ml/model/model_algo-2\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:23 INFO 140435207169856] Saved checkpoint to \"/tmp/tmp_6L2N6/state-0001.params\"\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:23.945] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 121641, \"num_examples\": 1, \"num_bytes\": 27088}\u001b[0m\n",
      "\u001b[35m[2020-06-28 21:19:24.067] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 121, \"num_examples\": 18, \"num_bytes\": 519656}\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] Finished scoring on 2176 examples from 17 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] Loss (name: value) total: 6.7034256318\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] Loss (name: value) kld: 0.221369923914\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] Loss (name: value) recons: 6.48205572016\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] Loss (name: value) logppx: 6.7034256318\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}, \"Total Records Seen\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2263, \"sum\": 2263.0, \"min\": 2263}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593379164.068287, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1593379163.945697}\n",
      "\u001b[0m\n",
      "\u001b[35m[06/28/2020 21:19:24 INFO 140435207169856] #test_score (algo-2) : ('log_perplexity', 6.703425631803625)\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 121868.62087249756, \"sum\": 121868.62087249756, \"min\": 121868.62087249756}, \"finalize.time\": {\"count\": 1, \"max\": 129.19902801513672, \"sum\": 129.19902801513672, \"min\": 129.19902801513672}, \"initialize.time\": {\"count\": 1, \"max\": 423.9809513092041, \"sum\": 423.9809513092041, \"min\": 423.9809513092041}, \"model.serialize.time\": {\"count\": 1, \"max\": 4.139900207519531, \"sum\": 4.139900207519531, \"min\": 4.139900207519531}, \"setuptime\": {\"count\": 1, \"max\": 48.73085021972656, \"sum\": 48.73085021972656, \"min\": 48.73085021972656}, \"early_stop.time\": {\"count\": 100, \"max\": 5.0029754638671875, \"sum\": 267.8053379058838, \"min\": 0.1590251922607422}, \"update.time\": {\"count\": 100, \"max\": 1580.085039138794, \"sum\": 121012.94136047363, \"min\": 936.1319541931152}, \"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"model.score.time\": {\"count\": 1, \"max\": 122.5290298461914, \"sum\": 122.5290298461914, \"min\": 122.5290298461914}}, \"EndTime\": 1593379164.0693, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1593379042.290529}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-06-28 21:19:34 Completed - Training job completed\n",
      "Training seconds: 376\n",
      "Billable seconds: 376\n"
     ]
    }
   ],
   "source": [
    "# Model Training, the topic model is trained with the NTM algorithms\n",
    "from sagemaker.session import s3_input\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key') \n",
    "ntm.fit({'train': s3_train, 'test': s3_val_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deploy the topic model\n",
    "\n",
    "two ways to deploy the model:\n",
    "1. Create one inference at a time, set up a persistent endpoint using SageMaker hosting services\n",
    "2. To get inferences for an entire dataset, use SageMaker batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-content-recommender/20newsgroups/batch/train/trainvectors.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: ntm-2020-06-28-21-11-57-701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\n",
      "\u001b[34mDocker entrypoint called with argument(s): serve\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded entry point class algorithm.serve.server_config:config_api\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loading entry points\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/serve.py:195: DeprecationWarning: entrypoint algorithm.request_iterators is deprecated in favor of algorithm.io.data_handlers.serve\n",
      "  \"in favor of algorithm.io.data_handlers.serve\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded request iterator text/csv\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded request iterator application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded request iterator application/json\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded request iterator application/jsonlines\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded response encoder application/x-recordio-protobuf\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded response encoder application/json\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded response encoder application/jsonlines\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loaded entry point class algorithm:model\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] Number of server workers: 1\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:36:18 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:36:18 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:36:18 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2020-06-28 21:36:18 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] loading model...\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] Deserialized model from /opt/ml/model\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 WARNING 140092981241664] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] nvidia-smi took: 0.025267124176 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 WARNING 140092981241664] Requesting context without setting the requested num of gpus. Using 'auto'\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/28/2020 21:36:18 INFO 140092981241664] ...model loaded.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"execution_parameters.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380178.765094, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380178.704222}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 4.099130630493164, \"sum\": 4.099130630493164, \"min\": 4.099130630493164}, \"json.encoder.time\": {\"count\": 1, \"max\": 34.651994705200195, \"sum\": 34.651994705200195, \"min\": 34.651994705200195}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380179.552736, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380178.765159}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.7798671722412109, \"sum\": 0.7798671722412109, \"min\": 0.7798671722412109}, \"json.encoder.time\": {\"count\": 1, \"max\": 22.89605140686035, \"sum\": 22.89605140686035, \"min\": 22.89605140686035}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380180.271405, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380179.552817}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.6940364837646484, \"sum\": 0.6940364837646484, \"min\": 0.6940364837646484}, \"json.encoder.time\": {\"count\": 1, \"max\": 29.221057891845703, \"sum\": 29.221057891845703, \"min\": 29.221057891845703}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380180.906657, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380180.271485}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.6527900695800781, \"sum\": 0.6527900695800781, \"min\": 0.6527900695800781}, \"json.encoder.time\": {\"count\": 1, \"max\": 28.822898864746094, \"sum\": 28.822898864746094, \"min\": 28.822898864746094}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380181.548686, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380180.906738}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8089542388916016, \"sum\": 0.8089542388916016, \"min\": 0.8089542388916016}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.200895309448242, \"sum\": 21.200895309448242, \"min\": 21.200895309448242}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380182.178279, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380181.548768}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.8959770202636719, \"sum\": 0.8959770202636719, \"min\": 0.8959770202636719}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.25692367553711, \"sum\": 21.25692367553711, \"min\": 21.25692367553711}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380182.808351, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380182.178361}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 0.7929801940917969, \"sum\": 0.7929801940917969, \"min\": 0.7929801940917969}, \"json.encoder.time\": {\"count\": 1, \"max\": 21.049976348876953, \"sum\": 21.049976348876953, \"min\": 21.049976348876953}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380183.452287, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380182.808436}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.evaluate.time\": {\"count\": 1, \"max\": 2.1610260009765625, \"sum\": 2.1610260009765625, \"min\": 2.1610260009765625}, \"json.encoder.time\": {\"count\": 1, \"max\": 5.748987197875977, \"sum\": 5.748987197875977, \"min\": 5.748987197875977}, \"invocations.count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593380183.576075, \"Dimensions\": {\"Host\": \"UNKNOWN\", \"Operation\": \"scoring\", \"Algorithm\": \"NVDMModel\"}, \"StartTime\": 1593380183.452382}\n",
      "\u001b[0m\n",
      "\u001b[32m2020-06-28T21:36:18.769:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# deploy the topic model with batch transform\n",
    "np.savetxt('trainvectors.csv',\n",
    "           vectors.todense(),\n",
    "           delimiter=',',\n",
    "           fmt='%i')\n",
    "batch_prefix = '20newsgroups/batch'\n",
    "\n",
    "train_s3 = sess.upload_data('trainvectors.csv', \n",
    "                            bucket=bucket, \n",
    "                            key_prefix='{}/train'.format(batch_prefix))\n",
    "print(train_s3)\n",
    "batch_output_path = 's3://{}/{}/test'.format(bucket, batch_prefix)\n",
    "\n",
    "ntm_transformer = ntm.transformer(instance_count=1,\n",
    "                                  instance_type ='ml.m4.xlarge',\n",
    "                                  output_path=batch_output_path\n",
    "                                 )\n",
    "ntm_transformer.transform(train_s3, content_type='text/csv', split_type='Line')\n",
    "ntm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Use SageMaker hosting service\n",
    "\n",
    "# ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "\n",
    "# Ensure that the input payload is seralized in a machine readable format and the output is deserialized into a \n",
    "# human readbale format\n",
    "# from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "# ntm_predictor.content_type = 'text/csv'\n",
    "# ntm_predictor.serializer = csv_serializer\n",
    "# ntm_predictor.deserializer = json_deserializer\n",
    "\n",
    "# Extract the topic vectors for the training data that we use in the K-NN model\n",
    "# predictions = []\n",
    "# for item in np.array(vectors.todense()):\n",
    "#     np.shape(item)\n",
    "#     results = ntm_predictor.predict(item)\n",
    "#     predictions.append(np.array([prediction['topic_weights'] for prediction in results['predictions']]))\n",
    "    \n",
    "# predictions = np.array([np.ndarray.flatten(x) for x in predictions])\n",
    "# topicvec = train_labels[newidx]\n",
    "# topicnames = [categories[x] for x in topicvec]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the topic model\n",
    "\n",
    "Use the T-SNE(Distributed Stocahstic Neighbour Embedding) plot to visualize the topic vectors\n",
    "\n",
    "The T-SNE is a non-linear technique for dimensionality reduction which aims to ensure that the distance between nearest neighbors in the original high dimensional space is preserved in the resulting lower dimensional space.\n",
    "\n",
    "Setting the number of dimensions to 2 so that we could visualize the topic in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 151 nearest neighbors...\n",
      "[t-SNE] Indexed 11314 samples in 0.038s...\n",
      "[t-SNE] Computed neighbors for 11314 samples in 3.619s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 11314\n",
      "[t-SNE] Computed conditional probabilities for sample 11314 / 11314\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 79.635117\n",
      "[t-SNE] KL divergence after 5000 iterations: 1.379687\n",
      "t-SNE done! Time elapsed: 374.675151348114 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'topicnames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a674ca5cf749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtsnedf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tsne-2d-one'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtsnedf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tsne-2d-two'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtsnedf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopicnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m sns.lmplot(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topicnames' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=5000)\n",
    "tsne_results = tsne.fit_transform(predictions)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "tsnedf = pd.DataFrame()\n",
    "tsnedf['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsnedf['tsne-2d-two'] = tsne_results[:,1]\n",
    "tsnedf['Topic']=topicnames\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue='Topic',\n",
    "    palette=sns.color_palette(\"hls\", NUM_TOPICS),\n",
    "    data=tsnedf,\n",
    "    legend=\"full\",\n",
    "    fit_reg=False\n",
    ")\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP3 train and deploy the content recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary which links the shuffled labels to the original labels in the trainin data\n",
    "labels = newidx \n",
    "labeldict = dict(zip(newidx,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (11314, 30)\n",
      "train_labels shape =  (11314,)\n",
      "20newsgroups/knn/train\n",
      "uploaded training data location: s3://sagemaker-content-recommender/20newsgroups/knn/train\n"
     ]
    }
   ],
   "source": [
    "# store the training data in S3 bucket\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "\n",
    "print('train_features shape = ', predictions.shape)\n",
    "print('train_labels shape = ', labels.shape)\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, predictions, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "key = 'knn/train'\n",
    "fname = os.path.join(prefix, key)\n",
    "print(fname)\n",
    "boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a K-NN estimator\n",
    "\n",
    "The model will be trained to return the nearest neighbors given a particular test topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-28 21:57:14 Starting - Starting the training job...\n",
      "2020-06-28 21:57:16 Starting - Launching requested ML instances......\n",
      "2020-06-28 21:58:43 Starting - Preparing the instances for training......\n",
      "2020-06-28 21:59:29 Downloading - Downloading input data......\n",
      "2020-06-28 22:00:36 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:39 INFO 139742945994560] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'index_metric': u'L2', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'faiss_index_ivf_nlists': u'auto', u'epochs': u'1', u'index_type': u'faiss.Flat', u'_faiss_index_nprobe': u'5', u'_kvstore': u'dist_async', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:39 INFO 139742945994560] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {u'sample_size': u'11314', u'feature_dim': u'30', u'index_metric': u'COSINE', u'predictor_type': u'classifier', u'k': u'10'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:39 INFO 139742945994560] Final configuration: {u'index_metric': u'COSINE', u'predictor_type': u'classifier', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'_log_level': u'info', u'feature_dim': u'30', u'faiss_index_ivf_nlists': u'auto', u'sample_size': u'11314', u'epochs': u'1', u'index_type': u'faiss.Flat', u'_faiss_index_nprobe': u'5', u'_kvstore': u'dist_async', u'_num_kv_servers': u'1', u'mini_batch_size': u'5000', u'k': u'10'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:39 WARNING 139742945994560] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-06-28-21-57-14-199', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-118-248.ca-central-1.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/51e0e22b-1474-47d9-9f99-2d014ce13c7f', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/knn-2020-06-28-21-57-14-199', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.118.248', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-06-28-21-57-14-199', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-118-248.ca-central-1.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/51e0e22b-1474-47d9-9f99-2d014ce13c7f', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/knn-2020-06-28-21-57-14-199', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-06-28-21-57-14-199', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-118-248.ca-central-1.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/51e0e22b-1474-47d9-9f99-2d014ce13c7f', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/knn-2020-06-28-21-57-14-199', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '1', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.118.248', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-06-28-21-57-14-199', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-118-248.ca-central-1.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/51e0e22b-1474-47d9-9f99-2d014ce13c7f', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/knn-2020-06-28-21-57-14-199', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/73a2ddaf-8f07-42fa-b6c2-1c865b532363', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '1', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.118.248', 'AWS_REGION': 'ca-central-1', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'knn-2020-06-28-21-57-14-199', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-118-248.ca-central-1.compute.internal', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/51e0e22b-1474-47d9-9f99-2d014ce13c7f', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '1', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:ca-central-1:030453898923:training-job/knn-2020-06-28-21-57-14-199', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 62 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 71 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:40 INFO 139742945994560] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] nvidia-smi took: 0.0252039432526 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 ERROR 139742945994560] nvidia-smi: failed to run (127): /bin/sh: nvidia-smi: command not found\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] Using per-worker sample size = 11314 (Available virtual memory = 6244757504 bytes, GPU free memory = 0 bytes, number of workers = 1). If an out-of-memory error occurs, choose a larger instance type, use dimension reduction, decrease sample_size, and/or decrease mini_batch_size.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1593381641.890176, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1593381641.890118}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-06-28 22:00:41.890] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 903, \"num_examples\": 1, \"num_bytes\": 1440000}\u001b[0m\n",
      "\u001b[34m[2020-06-28 22:00:41.922] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 31, \"num_examples\": 3, \"num_bytes\": 3258432}\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] push reservoir to kv... 1 num_workers 0 rank\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] ...done (11314)\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 11314, \"sum\": 11314.0, \"min\": 11314}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}, \"Total Records Seen\": {\"count\": 1, \"max\": 11314, \"sum\": 11314.0, \"min\": 11314}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 11314, \"sum\": 11314.0, \"min\": 11314}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1593381641.927768, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\", \"epoch\": 0}, \"StartTime\": 1593381641.890804}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] #throughput_metric: host=algo-1, train throughput=304876.650044 records/second\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] pulled row count... worker 0 rows 11314\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] pulled... worker 0 data (11314, 30) labels (11314, 1) nans 0\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] calling index.train...\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] ...done calling index.train\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] calling index.add...\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] ...done calling index.add\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.777099609375, \"sum\": 2.777099609375, \"min\": 2.777099609375}, \"finalize.time\": {\"count\": 1, \"max\": 8.461952209472656, \"sum\": 8.461952209472656, \"min\": 8.461952209472656}, \"initialize.time\": {\"count\": 1, \"max\": 880.8858394622803, \"sum\": 880.8858394622803, \"min\": 880.8858394622803}, \"update.time\": {\"count\": 1, \"max\": 36.71908378601074, \"sum\": 36.71908378601074, \"min\": 36.71908378601074}}, \"EndTime\": 1593381641.939331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1593381640.986342}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/28/2020 22:00:41 INFO 139742945994560] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2451.4248371124268, \"sum\": 2451.4248371124268, \"min\": 2451.4248371124268}, \"setuptime\": {\"count\": 1, \"max\": 1028.7659168243408, \"sum\": 1028.7659168243408, \"min\": 1028.7659168243408}}, \"EndTime\": 1593381641.940422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/KNN\"}, \"StartTime\": 1593381641.939416}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-06-28 22:00:53 Uploading - Uploading generated training model\n",
      "2020-06-28 22:00:53 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 84\n"
     ]
    }
   ],
   "source": [
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.c4.xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': predictions.shape[1],\n",
    "    'k': NUM_NEIGHBORS,\n",
    "    'sample_size': predictions.shape[0],\n",
    "    'predictor_type': 'classifier' ,\n",
    "    'index_metric':'COSINE'\n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/knn/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the content recommendation model\n",
    "The k_NN model will return all the cosine distances \n",
    "To build the engine, we need to get top-k suggestions from the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up the endpoint..\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name,\n",
    "                                        accept=\"application/jsonlines; verbose=true\")\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor\n",
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "print('setting up the endpoint..')\n",
    "knn_predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the test data so that you can run inferences\n",
    "def preprocess_input(text):\n",
    "    text = strip_newsgroup_header(text)\n",
    "    text = strip_newsgroup_quoting(text)\n",
    "    text = strip_newsgroup_footer(text)\n",
    "    return text    \n",
    "    \n",
    "test_data_prep = []\n",
    "for i in range(len(newsgroups_test)):\n",
    "    test_data_prep.append(preprocess_input(newsgroups_test[i]))\n",
    "test_vectors = vectorizer.fit_transform(test_data_prep)\n",
    "\n",
    "test_vectors = np.array(test_vectors.todense())\n",
    "test_topics = []\n",
    "for vec in test_vectors:\n",
    "    test_result = ntm_predictor.predict(vec)\n",
    "    test_topics.append(test_result['predictions'][0]['topic_weights'])\n",
    "\n",
    "topic_predictions = []\n",
    "for topic in test_topics:\n",
    "    result = knn_predictor.predict(topic)\n",
    "    cur_predictions = np.array([int(result['labels'][i]) for i in range(len(result['labels']))])\n",
    "    topic_predictions.append(cur_predictions[::-1][:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore content recommendation model\n",
    "\n",
    "Now that the predictions are obtained. Plot the topics distributions of the \n",
    "test topics, compared to the closest K topics recommedned by the k-NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your own k.\n",
    "def plot_topic_distribution(topic_num, k = 5):\n",
    "    \n",
    "    closest_topics = [predictions[labeldict[x]] for x in topic_predictions[topic_num][:k]]\n",
    "    closest_topics.append(np.array(test_topics[topic_num]))\n",
    "    closest_topics = np.array(closest_topics)\n",
    "    df = pd.DataFrame(closest_topics.T)\n",
    "    df.rename(columns ={k:\"Test Document Distribution\"}, inplace=True)\n",
    "    fs = 12\n",
    "    df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "    plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "    plt.xlabel('Topic ID', fontsize=fs+2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_topic_distribution(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_topic_distribution(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_topic_distribution(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_endpoint()\n",
    "knn_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
